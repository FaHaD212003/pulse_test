{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb51df1b",
   "metadata": {},
   "source": [
    "# Initializing MinIO and PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce9dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "import List as mapping_list\n",
    "import psycopg2\n",
    "from io import BytesIO\n",
    "from minio import Minio\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "    \"localhost:9100\",\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"minioadmin\",\n",
    "    secure=False,\n",
    ")\n",
    "\n",
    "bucket_name = 'pulse-bucket-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e3d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"NormalizeData\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9100\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"inferSchema\", \"true\") \\\n",
    "    .config(\"mergeSchema\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686bfb4",
   "metadata": {},
   "source": [
    "#  Function To load all files from minio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "62a3db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_table = {\n",
    "    \"customers_df\": \"customers\",\n",
    "    \"addresses_df\": \"addresses\",\n",
    "    \"products_df\": \"products\",\n",
    "    \"inventories_df\": \"inventory\",\n",
    "    \"orders_df\": \"orders\",\n",
    "    \"reviews_df\": \"reviews\",\n",
    "    \"categories_df\": \"categories\",\n",
    "    \"wishlists_df\": \"wishlist\",\n",
    "    \"payments_df\": \"payments\",\n",
    "    \"order_items_df\": \"order_items\",\n",
    "    \"shopping_carts_df\": \"shopping_cart\",\n",
    "    \"customer_sessions_df\": \"customer_sessions\",\n",
    "    \"marketing_campaigns_df\": \"marketing_campaigns\",\n",
    "    \"suppliers_df\": \"suppliers\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8410962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from difflib import get_close_matches\n",
    "def normalize_name(name):\n",
    "    name = name.lower().replace(\"_df\", \"\")\n",
    "    name = os.path.splitext(name)[0]\n",
    "\n",
    "    possible_keys = list(df_to_table.keys())\n",
    "    close = get_close_matches(\n",
    "        name, possible_keys, n=1, cutoff=0.6\n",
    "    )\n",
    "    if close:\n",
    "        return close[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_table_name(df_name):\n",
    "    norm = normalize_name(df_name)\n",
    "    return df_to_table.get(norm, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "34b21ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "def load_all_files_from_minio(minio_client, bucket_name, spark):\n",
    "    dataframes = {}\n",
    "\n",
    "    objects = minio_client.list_objects(bucket_name, recursive=True)\n",
    "    print(\"Listing available files in the bucket...\")\n",
    "\n",
    "    for obj in objects:\n",
    "        file_name = obj.object_name\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "\n",
    "        if not (file_name.endswith('.csv') or file_name.endswith('.xlsx') or \n",
    "                file_name.endswith('.parquet') or file_name.endswith('.json')):\n",
    "            print(f\"Skipping {file_name} - unsupported format\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            base_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            clean_name = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", base_name)\n",
    "            norm_name = normalize_name(clean_name)\n",
    "            if norm_name is None:\n",
    "                print(f\"No match found for {base_name}, skipping.\")\n",
    "                continue\n",
    "            df = load_file_from_minio(minio_client, bucket_name, file_name)\n",
    "            df_name = f\"{norm_name}\"\n",
    "            dataframes[df_name] = df\n",
    "            print(f\"✅ Successfully loaded {file_name} as {df_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(dataframes)} dataframes: {', '.join(dataframes.keys())}\")\n",
    "    return dataframes\n",
    "\n",
    "import uuid\n",
    "\n",
    "def load_file_from_minio(minio_client, bucket_name, file_name):\n",
    "\n",
    "    obj = minio_client.get_object(bucket_name, file_name)\n",
    "    data = obj.read()\n",
    "    obj.close()\n",
    "    obj.release_conn()\n",
    "\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        pdf = pd.read_csv(BytesIO(data))\n",
    "    elif file_name.endswith(\".xlsx\"):\n",
    "        pdf = pd.read_excel(BytesIO(data))\n",
    "    elif file_name.endswith(\".parquet\"):\n",
    "        pdf = pd.read_parquet(BytesIO(data))\n",
    "    elif file_name.endswith(\".json\"):\n",
    "        pdf = pd.read_json(BytesIO(data))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_name}\")\n",
    "\n",
    "    temp_csv = os.path.join(\n",
    "        tempfile.gettempdir(),\n",
    "        f\"temp_{uuid.uuid4().hex}_{os.path.basename(file_name)}.csv\",\n",
    "    )\n",
    "    pdf.to_csv(temp_csv, index=False)\n",
    "\n",
    "    spark_df = spark.read.csv(temp_csv, header=True, inferSchema=True).cache()\n",
    "    _ = spark_df.count()\n",
    "\n",
    "    try:\n",
    "        os.remove(temp_csv)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eebb510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available files in the bucket...\n",
      "Processing file: addresses.xlsx\n",
      "✅ Successfully loaded addresses.xlsx as addresses_df\n",
      "Processing file: categories.xlsx\n",
      "✅ Successfully loaded categories.xlsx as categories_df\n",
      "Processing file: customer_sessions.xlsx\n",
      "✅ Successfully loaded customer_sessions.xlsx as customer_sessions_df\n",
      "Processing file: customers.xlsx\n",
      "✅ Successfully loaded customers.xlsx as customers_df\n",
      "Processing file: inventory.xlsx\n",
      "✅ Successfully loaded inventory.xlsx as inventories_df\n",
      "Processing file: marketing_campaigns.xlsx\n",
      "✅ Successfully loaded marketing_campaigns.xlsx as marketing_campaigns_df\n",
      "Processing file: order_items.xlsx\n",
      "✅ Successfully loaded order_items.xlsx as order_items_df\n",
      "Processing file: orders.xlsx\n",
      "✅ Successfully loaded orders.xlsx as orders_df\n",
      "Processing file: payments.xlsx\n",
      "✅ Successfully loaded payments.xlsx as payments_df\n",
      "Processing file: products.xlsx\n",
      "✅ Successfully loaded products.xlsx as products_df\n",
      "Processing file: reviews.xlsx\n",
      "✅ Successfully loaded reviews.xlsx as reviews_df\n",
      "Processing file: shopping_carts.xlsx\n",
      "✅ Successfully loaded shopping_carts.xlsx as shopping_carts_df\n",
      "Processing file: suppliers.xlsx\n",
      "✅ Successfully loaded suppliers.xlsx as suppliers_df\n",
      "Processing file: wishlists.xlsx\n",
      "✅ Successfully loaded wishlists.xlsx as wishlists_df\n",
      "Loaded 14 dataframes: addresses_df, categories_df, customer_sessions_df, customers_df, inventories_df, marketing_campaigns_df, order_items_df, orders_df, payments_df, products_df, reviews_df, shopping_carts_df, suppliers_df, wishlists_df\n"
     ]
    }
   ],
   "source": [
    "all_dataframes = load_all_files_from_minio(minio_client, bucket_name, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a2f8e084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'addresses_df': DataFrame[addr_id: string, customer_ref: string, address_category: string, street_line1: string, city_name: string, state_region: string, zip_postal: string, country_name: string, default_flag: string, record_created: string, latitude: string, longitude: string, street_line2: string, address_verified: string, last_modified: string],\n",
       " 'categories_df': DataFrame[cat_id: string, cat_name: string, parent_cat_id: string, active_flag: string, date_created: string, category_desc: string, product_count: string, breadcrumb_path: string, display_sequence: string, hierarchy_level: string, url_slug: string],\n",
       " 'customer_sessions_df': DataFrame[session_ref: string, user_id: string, start_timestamp: string, end_timestamp: string, device_category: string, traffic_source: string, page_views: string, products_browsed: string, purchase_made: string, cart_abandoned: string, bounce_session: string, entry_page: string, exit_page: string, visitor_type: string, session_duration_sec: string, cart_items_count: string, browser_name: string, geo_country: string, session_revenue: string],\n",
       " 'customers_df': DataFrame[cust_id: string, full_name: string, customer_category: string, sex: string, birth_date: string, account_created: string, status: string, acquisition_source: string, customer_tier: string, created_timestamp: string, email_address: string, phone_number: string, last_purchase_date: string, total_purchases: string, loyalty_points: string, age: string, lifetime_value: string, country: string],\n",
       " 'inventories_df': DataFrame[inv_id: string, product_ref: string, vendor_id: string, current_stock: string, reserved_stock: string, min_stock_level: string, last_restock_date: string, last_sale_date: string, monthly_storage_cost: string, created_date: string, available_qty: string, days_since_last_sale: string, stock_status: string, warehouse_location: string, total_stock_value: string, restock_lead_time_days: string, expiry_date: string],\n",
       " 'marketing_campaigns_df': DataFrame[campaign_ref: string, campaign_title: string, channel_type: string, launch_date: string, completion_date: string, allocated_budget: string, current_spend: string, total_impressions: string, total_clicks: string, conversion_count: string, target_segment: string, campaign_status: string, created_timestamp: string, ctr_rate: string, avg_cpc: string, campaign_manager: string, test_variant: string, roi_percentage: string, conversion_rate: string, ad_platform: string],\n",
       " 'order_items_df': DataFrame[line_item_id: string, order_ref: string, product_ref: string, qty_ordered: string, unit_selling_price: string, item_discount: string, line_total: string, unit_cost: string, created_timestamp: string, product_sku: string, total_weight_kg: string, is_gift: string, tax_amount: string, profit_margin: string, fulfillment_location: string, return_status: string],\n",
       " 'orders_df': DataFrame[order_ref: string, customer_ref: string, session_ref: string, purchase_date: string, order_status: string, order_subtotal: string, tax_total: string, shipping_fee: string, discount_total: string, grand_total: string, currency_code: string, marketing_channel: string, device_category: string, shipment_date: string, delivery_date: string, record_created: string, coupon_code: string, payment_method: string, shipping_method: string, item_count: string, customer_ip: string, refund_amount: string],\n",
       " 'payments_df': DataFrame[payment_ref: string, order_ref: string, payment_type: string, gateway_provider: string, payment_status: string, transaction_date: string, payment_amount: string, gateway_transaction_id: string, transaction_fee: string, refund_total: string, refund_processed_date: string, record_created: string, risk_score: string, billing_country: string, authorization_code: string, customer_ip: string, currency_code: string, card_last_four: string, card_brand: string, retry_attempt: string],\n",
       " 'products_df': DataFrame[prod_id: string, product_description: string, stock_code: string, category_ref: string, manufacturer: string, vendor_id: string, unit_cost: string, retail_price: string, color: string, size: string, weight: string, dimensions: string, release_date: string, availability_status: string, digital_product: string, avg_customer_rating: string, review_count: string, record_created_at: string, inventory_qty: string, discount_pct: string],\n",
       " 'reviews_df': DataFrame[review_ref: string, product_ref: string, customer_ref: string, star_rating: string, review_headline: string, review_content: string, submitted_date: string, verified_purchase: string, moderation_status: string, submission_device: string, helpful_count: string, total_votes: string, image_count: string, review_language: string, reviewer_name: string, seller_response: string, review_date_only: date],\n",
       " 'shopping_carts_df': DataFrame[cart_item_id: string, customer_ref: string, session_identifier: string, product_ref: string, item_quantity: string, price_per_unit: string, date_added_to_cart: string, status: string, order_reference: string, device_type: string, discount_amount: string, promo_code: string, ip_address: string, tax_amount: string, last_updated: string, saved_for_later: string],\n",
       " 'suppliers_df': DataFrame[supplier_id: string, supplier_name: string, contact_email: string, phone_number: string, country: string, created_at: string, status: string, supplier_category: string, tax_id: string],\n",
       " 'wishlists_df': DataFrame[wish_id: string, user_id: string, item_id: string, date_added: string, priority: string, purchase_date: string, removal_date: string, user_notes: string, price_alert_enabled: string, notification_sent: string, current_price: string, desired_quantity: string, list_name: string, price_at_addition: string, add_source: string]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0ea03ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=os.getenv(\"POSTGRES_DATABASE_NAME\"),\n",
    "    user=os.getenv(\"POSTGRES_USER\"),\n",
    "    password=os.getenv(\"POSTGRES_PASSWORD\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "02ee1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    SELECT table_name, column_name, data_type\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'public'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ea80af70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('products', 'cost_price', 'numeric'),\n",
       " ('products', 'selling_price', 'numeric'),\n",
       " ('products', 'launch_date', 'date'),\n",
       " ('orders', 'order_date', 'timestamp without time zone'),\n",
       " ('products', 'weight', 'numeric'),\n",
       " ('orders', 'subtotal', 'numeric'),\n",
       " ('orders', 'tax_amount', 'numeric'),\n",
       " ('orders', 'shipping_cost', 'numeric'),\n",
       " ('orders', 'discount_amount', 'numeric'),\n",
       " ('orders', 'total_amount', 'numeric'),\n",
       " ('customer_sessions', 'session_end', 'timestamp without time zone'),\n",
       " ('customers', 'date_of_birth', 'date'),\n",
       " ('customers', 'customers_created_at', 'timestamp without time zone'),\n",
       " ('orders', 'shipped_date', 'timestamp without time zone'),\n",
       " ('orders', 'delivered_date', 'timestamp without time zone'),\n",
       " ('orders', 'order_created_at', 'timestamp without time zone'),\n",
       " ('customer_sessions', 'pages_viewed', 'integer'),\n",
       " ('customer_sessions', 'products_viewed', 'integer'),\n",
       " ('products', 'is_digital', 'boolean'),\n",
       " ('order_items', 'quantity', 'integer'),\n",
       " ('order_items', 'unit_price', 'numeric'),\n",
       " ('order_items', 'total_price', 'numeric'),\n",
       " ('order_items', 'discount_amount', 'numeric'),\n",
       " ('order_items', 'product_cost', 'numeric'),\n",
       " ('order_items', 'order_item_created_at', 'timestamp without time zone'),\n",
       " ('products', 'average_rating', 'numeric'),\n",
       " ('products', 'total_reviews', 'integer'),\n",
       " ('products', 'product_created_at', 'timestamp without time zone'),\n",
       " ('reviews', 'rating', 'integer'),\n",
       " ('customer_sessions', 'conversion_flag', 'boolean'),\n",
       " ('customer_sessions', 'cart_abandonment_flag', 'boolean'),\n",
       " ('reviews', 'review_date', 'timestamp without time zone'),\n",
       " ('reviews', 'is_verified_purchase', 'boolean'),\n",
       " ('customers', 'registration_date', 'timestamp without time zone'),\n",
       " ('inventory', 'stock_quantity', 'integer'),\n",
       " ('inventory', 'reserved_quantity', 'integer'),\n",
       " ('inventory', 'reorder_level', 'integer'),\n",
       " ('shopping_cart', 'quantity', 'integer'),\n",
       " ('shopping_cart', 'unit_price', 'numeric'),\n",
       " ('shopping_cart', 'added_date', 'timestamp without time zone'),\n",
       " ('inventory', 'last_restocked_date', 'timestamp without time zone'),\n",
       " ('inventory', 'last_sold_date', 'timestamp without time zone'),\n",
       " ('inventory', 'storage_cost', 'numeric'),\n",
       " ('inventory', 'inventory_created_at', 'timestamp without time zone'),\n",
       " ('categories', 'is_active', 'boolean'),\n",
       " ('wishlist', 'added_date', 'timestamp without time zone'),\n",
       " ('categories', 'category_created_at', 'timestamp without time zone'),\n",
       " ('wishlist', 'purchased_date', 'timestamp without time zone'),\n",
       " ('wishlist', 'removed_date', 'timestamp without time zone'),\n",
       " ('addresses', 'is_default', 'boolean'),\n",
       " ('marketing_campaigns', 'start_date', 'date'),\n",
       " ('marketing_campaigns', 'end_date', 'date'),\n",
       " ('marketing_campaigns', 'budget', 'numeric'),\n",
       " ('marketing_campaigns', 'spent_amount', 'numeric'),\n",
       " ('payments', 'payment_date', 'timestamp without time zone'),\n",
       " ('payments', 'amount', 'numeric'),\n",
       " ('marketing_campaigns', 'impressions', 'integer'),\n",
       " ('payments', 'processing_fee', 'numeric'),\n",
       " ('payments', 'refund_amount', 'numeric'),\n",
       " ('payments', 'refund_date', 'timestamp without time zone'),\n",
       " ('payments', 'payment_created_at', 'timestamp without time zone'),\n",
       " ('marketing_campaigns', 'clicks', 'integer'),\n",
       " ('marketing_campaigns', 'conversions', 'integer'),\n",
       " ('addresses', 'address_created_at', 'timestamp without time zone'),\n",
       " ('customer_sessions', 'session_start', 'timestamp without time zone'),\n",
       " ('marketing_campaigns', 'campaign_created_at', 'timestamp without time zone'),\n",
       " ('suppliers', 'supplier_created_at', 'timestamp without time zone'),\n",
       " ('payments', 'payment_method', 'character varying'),\n",
       " ('payments', 'payment_provider', 'character varying'),\n",
       " ('payments', 'payment_status', 'character varying'),\n",
       " ('payments', 'transaction_id', 'character varying'),\n",
       " ('suppliers', 'supplier_id', 'character varying'),\n",
       " ('suppliers', 'supplier_name', 'character varying'),\n",
       " ('suppliers', 'contact_email', 'character varying'),\n",
       " ('suppliers', 'phone_number', 'character varying'),\n",
       " ('suppliers', 'country', 'character varying'),\n",
       " ('orders', 'device_type', 'character varying'),\n",
       " ('customers', 'customer_name', 'character varying'),\n",
       " ('customers', 'customer_type', 'character varying'),\n",
       " ('customers', 'gender', 'character varying'),\n",
       " ('customers', 'customer_status', 'character varying'),\n",
       " ('customers', 'acquisition_channel', 'character varying'),\n",
       " ('customers', 'customer_segment', 'character varying'),\n",
       " ('addresses', 'address_id', 'character varying'),\n",
       " ('addresses', 'customer_id', 'character varying'),\n",
       " ('addresses', 'address_type', 'character varying'),\n",
       " ('addresses', 'city', 'character varying'),\n",
       " ('addresses', 'state_province', 'character varying'),\n",
       " ('addresses', 'postal_code', 'character varying'),\n",
       " ('addresses', 'country', 'character varying'),\n",
       " ('categories', 'category_id', 'character varying'),\n",
       " ('categories', 'category_name', 'character varying'),\n",
       " ('categories', 'parent_category_id', 'character varying'),\n",
       " ('customer_sessions', 'session_id', 'character varying'),\n",
       " ('customer_sessions', 'customer_id', 'character varying'),\n",
       " ('customer_sessions', 'device_type', 'character varying'),\n",
       " ('customer_sessions', 'referrer_source', 'character varying'),\n",
       " ('products', 'product_id', 'character varying'),\n",
       " ('products', 'product_name', 'character varying'),\n",
       " ('products', 'sku', 'character varying'),\n",
       " ('products', 'category_id', 'character varying'),\n",
       " ('products', 'brand', 'character varying'),\n",
       " ('products', 'supplier_id', 'character varying'),\n",
       " ('products', 'dimensions', 'character varying'),\n",
       " ('products', 'color', 'character varying'),\n",
       " ('products', 'size', 'character varying'),\n",
       " ('products', 'material', 'character varying'),\n",
       " ('products', 'product_status', 'character varying'),\n",
       " ('inventory', 'inventory_id', 'character varying'),\n",
       " ('inventory', 'product_id', 'character varying'),\n",
       " ('inventory', 'supplier_id', 'character varying'),\n",
       " ('marketing_campaigns', 'campaign_id', 'character varying'),\n",
       " ('marketing_campaigns', 'campaign_name', 'character varying'),\n",
       " ('marketing_campaigns', 'campaign_type', 'character varying'),\n",
       " ('marketing_campaigns', 'target_audience', 'text'),\n",
       " ('marketing_campaigns', 'campaign_status', 'character varying'),\n",
       " ('orders', 'order_id', 'character varying'),\n",
       " ('orders', 'customer_id', 'character varying'),\n",
       " ('orders', 'session_id', 'character varying'),\n",
       " ('orders', 'order_status', 'character varying'),\n",
       " ('orders', 'currency', 'character varying'),\n",
       " ('orders', 'acquisition_channel', 'character varying'),\n",
       " ('customers', 'customer_id', 'character varying'),\n",
       " ('order_items', 'order_item_id', 'character varying'),\n",
       " ('order_items', 'order_id', 'character varying'),\n",
       " ('order_items', 'product_id', 'character varying'),\n",
       " ('reviews', 'review_id', 'character varying'),\n",
       " ('reviews', 'product_id', 'character varying'),\n",
       " ('reviews', 'customer_id', 'character varying'),\n",
       " ('reviews', 'review_title', 'character varying'),\n",
       " ('reviews', 'review_text', 'text'),\n",
       " ('shopping_cart', 'cart_id', 'character varying'),\n",
       " ('shopping_cart', 'customer_id', 'character varying'),\n",
       " ('shopping_cart', 'session_id', 'character varying'),\n",
       " ('shopping_cart', 'product_id', 'character varying'),\n",
       " ('shopping_cart', 'cart_status', 'character varying'),\n",
       " ('shopping_cart', 'converted_order_id', 'character varying'),\n",
       " ('wishlist', 'wishlist_id', 'character varying'),\n",
       " ('wishlist', 'customer_id', 'character varying'),\n",
       " ('wishlist', 'product_id', 'character varying'),\n",
       " ('wishlist', 'priority_level', 'character varying'),\n",
       " ('payments', 'payment_id', 'character varying'),\n",
       " ('payments', 'order_id', 'character varying')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_info = cur.fetchall()\n",
    "columns_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fec5b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae321761",
   "metadata": {},
   "source": [
    "# Mapping with Predefined Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "efb20333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'List' from '/mnt/01DBA8B8279979A0/Pulse - E-Commerce Data Analytics Engine/mapping/List.py'>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0cf250d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df, column_variants, mapped_cols):\n",
    "    variant_to_standard = {\n",
    "        v.lower(): std_col\n",
    "        for std_col, variants in column_variants.items()\n",
    "        for v in variants\n",
    "    }\n",
    "\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if col_lower in variant_to_standard:\n",
    "            std_col = variant_to_standard[col_lower]\n",
    "            new_columns.append(std_col)\n",
    "            mapped_cols[std_col] = col\n",
    "        else:\n",
    "            new_columns.append(col)\n",
    "\n",
    "    for old_col, new_col in zip(df.columns, new_columns):\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    missing_cols = []\n",
    "    for std_col in column_variants.keys():\n",
    "        if std_col not in df.columns:\n",
    "            df = df.withColumn(std_col, lit(None))\n",
    "            missing_cols.append(std_col)\n",
    "\n",
    "    schema_cols = list(column_variants.keys())\n",
    "    extra_cols = [c for c in df.columns if c not in schema_cols]\n",
    "\n",
    "    new_df = df.select(schema_cols)\n",
    "    df_extra = df.select(schema_cols + extra_cols)\n",
    "    \n",
    "\n",
    "    return new_df, df_extra, extra_cols, missing_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ea80f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir = os.getcwd()\n",
    "# file_path_excel = os.path.join(base_dir, \"./../faker/messy_inventory_data.xlsx\")\n",
    "# file_path_csv = os.path.join(base_dir, \"./../faker/messy_inventory_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1658a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel_df = pd.read_excel(file_path_excel, engine=\"openpyxl\")\n",
    "# excel_df.to_csv(file_path_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ee5496f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(file_path_csv, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "088732aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374dcf8",
   "metadata": {},
   "source": [
    "# Implementing RapidFuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0cf46606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "\n",
    "def rapidfuzz_column_mapping(df, missing_cols, extra_cols, mapped_cols, threshold=85):\n",
    "    \"\"\"\n",
    "    Map columns using RapidFuzz's string matching algorithms\n",
    "    Returns normalized dataframe, remaining missing columns, extra columns, and mapped columns\n",
    "    \"\"\"\n",
    "    for missing_col in missing_cols[:]:\n",
    "        match = process.extractOne(\n",
    "            missing_col,\n",
    "            extra_cols,\n",
    "            scorer=fuzz.ratio,\n",
    "            score_cutoff=threshold,\n",
    "        )\n",
    "        if match:\n",
    "            best_match, score = match[0], match[1]\n",
    "            print(f\"RapidFuzz Mapping: {best_match} -> {missing_col}: {score:.2f}\")\n",
    "            mapped_cols[missing_col] = best_match\n",
    "            missing_cols.remove(missing_col)\n",
    "            extra_cols.remove(best_match)\n",
    "    for new_col, old_col in mapped_cols.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "    df = df.drop(*extra_cols)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af250a",
   "metadata": {},
   "source": [
    "# Implementing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "de5e8907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/khalid_ah_1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/khalid_ah_1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/khalid_ah_1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/khalid_ah_1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/khalid_ah_1/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "183e24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_column_name(column):\n",
    "    words = \"\".join(c if c.isalnum() else \" \" for c in column).split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        result.extend(filter(None, re.split(\"([A-Z][a-z]*)\", word)))\n",
    "    return [w.lower() for w in result if w]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b15ea",
   "metadata": {},
   "source": [
    "#### Implementing Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e66dca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(source_column, target_column):\n",
    "    source_col = preprocess_column_name(source_column)\n",
    "    target_col = preprocess_column_name(target_column)\n",
    "    intersection = len(set(source_col).intersection(set(target_col)))\n",
    "    union = len(set(source_col).union(set(target_col)))\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255bc872",
   "metadata": {},
   "source": [
    "#### Implementing Sequence Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "15a97ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_matching(source_column, target_column):\n",
    "    source_col = preprocess_column_name(source_column)\n",
    "    target_col = preprocess_column_name(target_column)\n",
    "    return SequenceMatcher(None, source_col, target_col).ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b371b",
   "metadata": {},
   "source": [
    "#### Implementing Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f33e4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def editing_distance(source_column, target_column):\n",
    "    source_col = preprocess_column_name(source_column)\n",
    "    target_col = preprocess_column_name(target_column)\n",
    "    max_len = max(len(source_col), len(target_col))\n",
    "    return 1 - (edit_distance(source_col, target_col) / max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9002fd43",
   "metadata": {},
   "source": [
    "#### Combining them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f3d76824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_with_combination(df, missing_cols, extra_cols, mapped_cols, threshold=0.87):\n",
    "    for missing_col in missing_cols[:]:\n",
    "        best_match = None\n",
    "        best_score = threshold\n",
    "        for extra_col in extra_cols[:]:\n",
    "            final = (\n",
    "                0.4 * jaccard_similarity(missing_col, extra_col)\n",
    "                + 0.3 * sequence_matching(missing_col, extra_col)\n",
    "                + 0.3 * editing_distance(missing_col, extra_col)\n",
    "            )\n",
    "            if final > best_score:\n",
    "                best_score = final\n",
    "                best_match = extra_col\n",
    "\n",
    "        if best_match and best_score > threshold:\n",
    "            print(f\"NLTK Mapping: {best_match} -> {missing_col}: {best_score:.2f}\")\n",
    "            mapped_cols[missing_col] = best_match\n",
    "            missing_cols.remove(missing_col)\n",
    "            extra_cols.remove(best_match)\n",
    "\n",
    "    for new_col, old_col in mapped_cols.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    df = df.drop(*extra_cols)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d660",
   "metadata": {},
   "source": [
    "#### Wordnet Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "627d9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_column_name(column):\n",
    "#     name = re.sub(\"([A-Z][a-z]+)\", r\" \\1\", column)\n",
    "#     name = re.sub(\"_\", \" \", name)\n",
    "#     tokens = word_tokenize(name.lower())\n",
    "#     return [token for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c4081f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_synsets(word):\n",
    "    return wordnet.synsets(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e89da300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_semantic_similarity(missing_col, extra_col):\n",
    "    missing_tokens = preprocess_column_name(missing_col)\n",
    "    extra_tokens = preprocess_column_name(extra_col)\n",
    "    if not missing_tokens or not extra_tokens:\n",
    "        return 0.0\n",
    "    max_similarities = []\n",
    "    for token1 in missing_tokens:\n",
    "        synsets1 = get_wordnet_synsets(token1)\n",
    "        if not synsets1:\n",
    "            continue\n",
    "        token_similarities = []\n",
    "        for token2 in extra_tokens:\n",
    "            synsets2 = get_wordnet_synsets(token2)\n",
    "            if not synsets2:\n",
    "                continue\n",
    "            similarities = [\n",
    "                s1.path_similarity(s2)\n",
    "                for s1 in synsets1\n",
    "                for s2 in synsets2\n",
    "                if s1.path_similarity(s2) is not None\n",
    "            ]\n",
    "            if similarities:\n",
    "                token_similarities.append(max(similarities))\n",
    "        if token_similarities:\n",
    "            max_similarities.append(max(token_similarities))\n",
    "    return sum(max_similarities) / len(max_similarities) if max_similarities else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a6e94f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_column_mapping(df, missing_cols, extra_cols, mapped_cols, threshold=0.6):\n",
    "    for missing_col in missing_cols[:]:\n",
    "        best_match = None\n",
    "        best_score = threshold\n",
    "        for extra_col in extra_cols[:]:\n",
    "            similarity = calculate_semantic_similarity(missing_col, extra_col)\n",
    "            if similarity > best_score:\n",
    "                best_score = similarity\n",
    "                best_match = extra_col\n",
    "        if best_match:\n",
    "            print(f\"Wordnet Mapping: {best_match} -> {missing_col}: {best_score:.2f}\")\n",
    "            mapped_cols[missing_col] = best_match\n",
    "            missing_cols.remove(missing_col)\n",
    "            extra_cols.remove(best_match)\n",
    "\n",
    "    for new_col, old_col in mapped_cols.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    df = df.drop(*extra_cols)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6db7ff",
   "metadata": {},
   "source": [
    "# Implementing spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e6a60c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def spacy_column_mapping(\n",
    "    df,\n",
    "    missing_cols,\n",
    "    extra_cols,\n",
    "    mapped_cols,\n",
    "    threshold=0.87\n",
    "):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    for missing_col in missing_cols[:]:\n",
    "        best_match = None\n",
    "        best_score = threshold\n",
    "        missing_doc = nlp(\" \".join(preprocess_column_name(missing_col)))\n",
    "\n",
    "        for extra_col in extra_cols[:]:\n",
    "            extra_doc = nlp(\" \".join(preprocess_column_name(extra_col)))\n",
    "            similarity = missing_doc.similarity(extra_doc)\n",
    "\n",
    "            if similarity > best_score:\n",
    "                best_score = similarity\n",
    "                best_match = extra_col\n",
    "\n",
    "        if best_match:\n",
    "            print(f\"spaCy Mapping: {best_match} -> {missing_col}: {best_score:.2f}\")\n",
    "            mapped_cols[missing_col] = best_match\n",
    "            missing_cols.remove(missing_col)\n",
    "            extra_cols.remove(best_match)\n",
    "\n",
    "    for new_col, old_col in mapped_cols.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    df = df.drop(*extra_cols)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47635ac7",
   "metadata": {},
   "source": [
    "# Implementing Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "92e8503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "\n",
    "def load_word2vec_model(df, extra_df):\n",
    "    all_columns = list(set(df.columns + extra_df.columns))\n",
    "    \n",
    "    try:\n",
    "        model = KeyedVectors.load_word2vec_format(\n",
    "            \"GoogleNews-vectors-negative300.bin\", binary=True\n",
    "        )\n",
    "    except:\n",
    "        sentences = [preprocess_column_name(col) for col in all_columns]\n",
    "        model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "        model = model.wv\n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_word2vec_similarity(col1, col2, model):\n",
    "    words1 = preprocess_column_name(col1)\n",
    "    words2 = preprocess_column_name(col2)\n",
    "\n",
    "    if not words1 or not words2:\n",
    "        return 0.0\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for word in words1:\n",
    "        try:\n",
    "            vec1.append(model[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    for word in words2:\n",
    "        try:\n",
    "            vec2.append(model[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    if not vec1 or not vec2:\n",
    "        return 0.0\n",
    "\n",
    "    vec1_avg = np.mean(vec1, axis=0)\n",
    "    vec2_avg = np.mean(vec2, axis=0)\n",
    "\n",
    "    similarity = np.dot(vec1_avg, vec2_avg) / (\n",
    "        np.linalg.norm(vec1_avg) * np.linalg.norm(vec2_avg)\n",
    "    )\n",
    "    return float(similarity)\n",
    "\n",
    "\n",
    "def word2vec_column_mapping(df, extra_df, missing_cols, extra_cols, mapped_cols, threshold=0.87):\n",
    "    model = load_word2vec_model(df, extra_df)\n",
    "\n",
    "    for missing_col in missing_cols[:]:\n",
    "        best_match = None\n",
    "        best_score = threshold\n",
    "\n",
    "        for extra_col in extra_cols[:]:\n",
    "            similarity = calculate_word2vec_similarity(missing_col, extra_col, model)\n",
    "            if similarity > best_score:\n",
    "                best_score = similarity\n",
    "                best_match = extra_col\n",
    "\n",
    "        if best_match:\n",
    "            print(f\"Word2Vec Mapping: {best_match} -> {missing_col}: {best_score:.2f}\")\n",
    "            mapped_cols[missing_col] = best_match\n",
    "            missing_cols.remove(missing_col)\n",
    "            extra_cols.remove(best_match)\n",
    "\n",
    "    for new_col, old_col in mapped_cols.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    df = df.drop(*extra_cols)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0984ca",
   "metadata": {},
   "source": [
    "# Implementing Hugging Face RoBERTa Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "070fc806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "roberta = SentenceTransformer(\"all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0f77945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_similarity(df, missing_cols, extra_cols, mapped_cols, threshold=0.87):\n",
    "    missing_embeddings = roberta.encode(missing_cols, convert_to_tensor=True)\n",
    "    extra_embeddings = roberta.encode(extra_cols, convert_to_tensor=True)\n",
    "\n",
    "    for i, missing_col in enumerate(missing_cols):\n",
    "        sims = util.cos_sim(missing_embeddings[i], extra_embeddings)[0]\n",
    "        best_score, best_idx = torch.max(sims, dim=0)\n",
    "        best_score = best_score.item()\n",
    "        best_match = extra_cols[best_idx]\n",
    "\n",
    "        if best_score >= threshold:\n",
    "            print(\n",
    "                f\"BERT Mapping: {best_match} -> {missing_col} (score={best_score:.2f})\"\n",
    "            )\n",
    "            mapped_cols[missing_col] = best_match\n",
    "            missing_cols.remove(missing_col)\n",
    "            extra_cols.remove(best_match)\n",
    "    for schema_col, df_col in mapped_cols.items():\n",
    "        df = df.withColumnRenamed(df_col, schema_col)\n",
    "    df = df.drop(*extra_cols)\n",
    "    for col in missing_cols:\n",
    "        df = df.withColumn(col, lit(None))\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9db28a",
   "metadata": {},
   "source": [
    "# Implementing GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ab9e1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from openai import OpenAI\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url=\"https://router.huggingface.co/v1\",\n",
    "#     api_key=os.getenv(\"HF_TOKEN\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e89f7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_safe(data):\n",
    "    \"\"\"Recursively convert all values in a dict or list to JSON-safe types.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {k: make_json_safe(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [make_json_safe(v) for v in data]\n",
    "    else:\n",
    "        return safe_serialize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0edf6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "FREEGPT4_API_URL = (\n",
    "    \"http://localhost:5500/v1/chat/completions\"  # adjust if hosted elsewhere\n",
    ")\n",
    "\n",
    "\n",
    "def gpt_schema_mapping(df, missing_cols, extra_cols, mapped_cols):\n",
    "    pdf = df.limit(5).toPandas()\n",
    "\n",
    "    schema_info = {\n",
    "        \"missing_cols\": missing_cols,\n",
    "        \"extra_cols\": extra_cols,\n",
    "        \"already_mapped\": mapped_cols,\n",
    "        \"sample_data\": pdf.to_dict(orient=\"list\"),\n",
    "    }\n",
    "\n",
    "    # ensure schema_info is JSON safe\n",
    "    def make_json_safe(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {str(k): make_json_safe(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [make_json_safe(v) for v in obj]\n",
    "        elif isinstance(obj, (int, float, str)) or obj is None:\n",
    "            return obj\n",
    "        return str(obj)\n",
    "\n",
    "    schema_info = make_json_safe(schema_info)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are a data engineer helping with schema alignment.\n",
    "\n",
    "        I have a dataset with extra columns and some missing schema columns.\n",
    "        Please map extra columns to missing schema columns based on semantics and sample data.\n",
    "\n",
    "        Input (JSON):\n",
    "        {json.dumps(schema_info, indent=2)}\n",
    "\n",
    "        Return ONLY valid JSON with this exact structure:\n",
    "        {{\n",
    "        \"mapped_cols\": {{\"schema_col\": \"df_col\", ...}},\n",
    "        \"remaining_missing_cols\": [],\n",
    "        \"remaining_extra_cols\": []\n",
    "        }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call FreeGPT4-WEB-API instead of OpenAI\n",
    "    response = requests.post(\n",
    "        FREEGPT4_API_URL,\n",
    "        json={\n",
    "            \"model\": \"gpt-4.0\",  # adjust model name based on what FreeGPT4 API supports\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "        # timeout=60,\n",
    "    )\n",
    "\n",
    "    print(\"Full response:\", response.json())\n",
    "\n",
    "    try:\n",
    "        result_text = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        model_mapping = json.loads(result_text)\n",
    "\n",
    "        mapped_cols.update(model_mapping[\"mapped_cols\"])\n",
    "        missing_cols = model_mapping[\"remaining_missing_cols\"]\n",
    "        extra_cols = model_mapping[\"remaining_extra_cols\"]\n",
    "\n",
    "        # rename + drop + add cols in df\n",
    "        for schema_col, df_col in model_mapping[\"mapped_cols\"].items():\n",
    "            df = df.withColumnRenamed(df_col, schema_col)\n",
    "        df = df.drop(*extra_cols)\n",
    "        for col in missing_cols:\n",
    "            df = df.withColumn(col, lit(None))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing model output:\", e)\n",
    "        model_mapping = {}\n",
    "\n",
    "    return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "007cd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gptoss_schema_mapping(df, missing_cols, extra_cols, mapped_cols):\n",
    "#     pdf = df.limit(5).toPandas()\n",
    "\n",
    "#     schema_info = {\n",
    "#         \"missing_cols\": missing_cols,\n",
    "#         \"extra_cols\": extra_cols,\n",
    "#         \"already_mapped\": mapped_cols,\n",
    "#         \"sample_data\": pdf.to_dict(orient=\"list\"),\n",
    "#     }\n",
    "    \n",
    "#     schema_info = make_json_safe(schema_info)\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#         You are a data engineer helping with schema alignment.\n",
    "\n",
    "#         I have a dataset with extra columns and some missing schema columns.\n",
    "#         Please map extra columns to missing schema columns based on semantics and sample data.\n",
    "\n",
    "#         Input (JSON):\n",
    "#         {json.dumps(schema_info, indent=2)}\n",
    "\n",
    "#         Return ONLY valid JSON with this exact structure:\n",
    "#         {{\n",
    "#         \"mapped_cols\": {{\"schema_col\": \"df_col\", ...}},\n",
    "#         \"remaining_missing_cols\": [],\n",
    "#         \"remaining_extra_cols\": []\n",
    "#         }}\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.responses.create(\n",
    "#         model=\"openai/gpt-oss-20b\",\n",
    "#         input=prompt,\n",
    "#     )\n",
    "#     print(\"Full response:\", response)\n",
    "\n",
    "#     result = response.output_text.strip()\n",
    "#     try:\n",
    "#         model_mapping = json.loads(result)\n",
    "\n",
    "#         mapped_cols.update(model_mapping[\"mapped_cols\"])\n",
    "#         missing_cols = model_mapping[\"remaining_missing_cols\"]\n",
    "#         extra_cols = model_mapping[\"remaining_extra_cols\"]\n",
    "\n",
    "#         for schema_col, df_col in model_mapping[\"mapped_cols\"].items():\n",
    "#             df = df.withColumnRenamed(df_col, schema_col)\n",
    "#         df = df.drop(*extra_cols)\n",
    "#         for col in missing_cols:\n",
    "#             df = df.withColumn(col, lit(None))\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"Error parsing model output:\", e)\n",
    "#         model_mapping = {}\n",
    "\n",
    "#     return df, missing_cols, extra_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd7823",
   "metadata": {},
   "source": [
    "# Mapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8607d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(df, column_variants , mapped):\n",
    "    new_df, extra_df, extra_cols, missing_cols, mapped_cols = normalize_dataframe(\n",
    "        df, column_variants, mapped\n",
    "    )\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter Initial Normalization:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(new_df.columns)\n",
    "        print(\"Implementing RapidFuzz Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = rapidfuzz_column_mapping(\n",
    "            df, missing_cols, extra_cols, mapped_cols, threshold=70\n",
    "        )\n",
    "\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter RapidFuzz Mapping:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(new_df.columns)\n",
    "        print(\"Implementing NLTK Combination Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = mapping_with_combination(\n",
    "            df, missing_cols, extra_cols, mapped_cols, threshold=0.7\n",
    "        )\n",
    "\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter NLTK Combination Mapping:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(new_df.columns)\n",
    "        print(\"Implementing WordNet Semantic Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = semantic_column_mapping(\n",
    "            df, missing_cols, extra_cols, mapped_cols, threshold=0.7\n",
    "        )\n",
    "\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter WordNet Semantic Mapping:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(new_df.columns)\n",
    "        print(\"Implementing spaCy Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = spacy_column_mapping(\n",
    "            df, missing_cols, extra_cols, mapped_cols, threshold=0.7\n",
    "        )\n",
    "\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter spaCy Mapping:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        print(\"Implementing Word2Vec Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = word2vec_column_mapping(\n",
    "            df, extra_df, missing_cols, extra_cols, mapped_cols\n",
    "        )\n",
    "\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter Word2Vec Mapping:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(new_df.columns)\n",
    "        print(\"Implementing BERT Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = roberta_similarity(\n",
    "            df, missing_cols, extra_cols, mapped_cols, threshold=0.7\n",
    "        )\n",
    "\n",
    "    if missing_cols:\n",
    "        print(\"\\nAfter roBERTa Mapping:\")\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(new_df.columns)\n",
    "        print(\"Implementing GPT Mapping...\")\n",
    "        new_df, missing_cols, extra_cols, mapped_cols = gpt_schema_mapping(\n",
    "            df, missing_cols, extra_cols, mapped_cols\n",
    "        )\n",
    "\n",
    "    return new_df, extra_df, extra_cols, missing_cols, mapped_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0347b",
   "metadata": {},
   "source": [
    "# Processing & Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5173b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def safe_serialize(obj):\n",
    "    if isinstance(obj, (datetime.date, datetime.datetime)):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "73c2132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_table(df, columns_info, threshold=0.5):\n",
    "    df_cols = set(df.columns)\n",
    "    table_scores = {}\n",
    "\n",
    "    for table, col, _ in columns_info:\n",
    "        if table not in table_scores:\n",
    "            table_scores[table] = 0\n",
    "        if col in df_cols:\n",
    "            table_scores[table] += 1\n",
    "\n",
    "    best_table = max(table_scores, key=table_scores.get)\n",
    "    overlap_ratio = table_scores[best_table] / max(1, len(df_cols))\n",
    "\n",
    "    if overlap_ratio >= threshold:\n",
    "        return best_table\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7a14f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_unified_dataframe(df, columns_info):\n",
    "    \"\"\"\n",
    "    Splits a unified dataframe into per-table sub-dataframes\n",
    "    based on the strict rule that columns are named `table__column`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build canonical column set per table for validation\n",
    "    canonical_schema = {\n",
    "        table: {col for t, col, _ in columns_info if t == table}\n",
    "        for table, _, _ in columns_info\n",
    "    }\n",
    "\n",
    "    table_to_cols = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if \"__\" not in col:\n",
    "            raise ValueError(\n",
    "                f\"❌ Invalid column '{col}'. All columns must follow 'table__column' format.\"\n",
    "            )\n",
    "\n",
    "        table, column = col.split(\"__\", 1)\n",
    "\n",
    "        if table not in canonical_schema:\n",
    "            raise ValueError(f\"❌ Unknown table '{table}' in column '{col}'.\")\n",
    "\n",
    "        if column not in canonical_schema[table]:\n",
    "            raise ValueError(\n",
    "                f\"❌ Unknown column '{column}' for table '{table}' (from '{col}').\"\n",
    "            )\n",
    "\n",
    "        table_to_cols.setdefault(table, []).append((col, column))\n",
    "\n",
    "    # Build sub-dataframes\n",
    "    split_dfs = {}\n",
    "    for table, col_pairs in table_to_cols.items():\n",
    "        renamed = [df[c].alias(new_c) for c, new_c in col_pairs]\n",
    "        split_dfs[table] = df.select(*renamed)\n",
    "\n",
    "    return split_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f043152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_dataframes(all_dataframes, columns_info, mapping_list, mode=\"batch\"):\n",
    "    \"\"\"\n",
    "    Unified processor for schema mapping.\n",
    "\n",
    "    Supports:\n",
    "    1. Single unified file (all tables in one DataFrame).\n",
    "    2. Multiple files (one or more tables per file).\n",
    "    3. Streaming (micro-batches, API, DB source).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_dataframes : dict\n",
    "        Dict of {df_name: dataframe} from files, unified table, or stream.\n",
    "    columns_info : list\n",
    "        List of (table_name, column_name, data_type) from PostgreSQL schema.\n",
    "    mapping_list : object\n",
    "        Object containing mapping_dict_<table> for each table.\n",
    "    mode : str\n",
    "        \"batch\" for files (single or multiple), \"stream\" for streaming.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dict with results per canonical table.\n",
    "    \"\"\"\n",
    "\n",
    "    df_to_mapping_dict = {\n",
    "        \"customer_df\": mapping_list.mapping_dict_customers,\n",
    "        \"product_df\": mapping_list.mapping_dict_products,\n",
    "        \"inventory_df\": mapping_list.mapping_dict_inventory,\n",
    "        \"orders_df\": mapping_list.mapping_dict_orders,\n",
    "        \"reviews_df\": mapping_list.mapping_dict_reviews,\n",
    "        \"wishlist_df\": mapping_list.mapping_dict_wishlist,\n",
    "        \"payments_df\": mapping_list.mapping_dict_payments,\n",
    "        \"order_items_df\": mapping_list.mapping_dict_order_items,\n",
    "        \"shopping_cart_df\": mapping_list.mapping_dict_shopping_cart,\n",
    "        \"customer_sessions_df\": mapping_list.mapping_dict_customer_sessions,\n",
    "        \"marketing_campaigns_df\": mapping_list.mapping_dict_marketing_campaigns,\n",
    "        \"suppliers_df\": mapping_list.mapping_dict_suppliers,\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 2. Iterate over incoming dataframes\n",
    "    for df_name, df in all_dataframes.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Incoming dataframe: {df_name}\")\n",
    "\n",
    "        if mode == \"stream\":\n",
    "            detected_table = detect_table(df, columns_info)\n",
    "            if detected_table:\n",
    "                split_dfs = {detected_table: df}\n",
    "            else:\n",
    "                print(f\"⚠️ Could not detect schema for streaming dataframe {df_name}\")\n",
    "                continue\n",
    "        else:\n",
    "            if df_name in df_to_table:\n",
    "                split_dfs = {df_to_table[df_name]: df}\n",
    "            else:\n",
    "                split_dfs = split_unified_dataframe(df, columns_info)\n",
    "\n",
    "        for table_name, sub_df in split_dfs.items():\n",
    "            mapping_dict = getattr(mapping_list, f\"mapping_dict_{table_name}\", None)\n",
    "            if not mapping_dict:\n",
    "                print(f\"⚠️ No mapping dict found for {table_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            mapped = {col: \"\" for t, col, _ in columns_info if t == table_name}\n",
    "            print(f\"Processing → {table_name} with {len(mapped)} canonical columns\")\n",
    "\n",
    "        # try:\n",
    "            final_df, extra_df, extra_cols, missing_cols, mapped_cols = mapping(\n",
    "                sub_df, mapping_dict, mapped\n",
    "            )\n",
    "\n",
    "            # ✅ Sanitize lists before putting them into results\n",
    "            results[f\"{table_name}\"] = {\n",
    "                \"table_name\": table_name,\n",
    "                \"final_df\": final_df,  # keep Spark DF\n",
    "                \"extra_df\": extra_df,  # keep Spark DF\n",
    "                \"extra_cols\": [safe_serialize(c) for c in extra_cols],\n",
    "                \"missing_cols\": [safe_serialize(c) for c in missing_cols],\n",
    "                \"mapped_cols\": [safe_serialize(c) for c in mapped_cols],\n",
    "            }\n",
    "\n",
    "            print(f\"✅ Completed {df_name} → {table_name}\")\n",
    "            print(f\"   Missing cols: {missing_cols}\")\n",
    "            print(f\"   Extra cols: {extra_cols}\")\n",
    "            print(\"   Preview:\")\n",
    "            final_df.show(3)\n",
    "\n",
    "        # except Exception as e:\n",
    "            # print(f\"❌ Error processing {df_name}/{table_name}: {str(e)}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "50294889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Incoming dataframe: addresses_df\n",
      "Processing → addresses with 9 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['address_type', 'state_province', 'postal_code']\n",
      "['address_id', 'customer_id', 'address_type', 'city', 'state_province', 'postal_code', 'country', 'is_default', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: address_category -> address_type: 71.43\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['state_province', 'postal_code']\n",
      "['address_id', 'customer_id', 'address_type', 'city', 'country', 'is_default', 'created_at', 'state_province', 'postal_code']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['state_province', 'postal_code']\n",
      "['address_id', 'customer_id', 'address_type', 'city', 'country', 'is_default', 'created_at', 'state_province', 'postal_code']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "Wordnet Mapping: state_region -> state_province: 1.00\n",
      "Wordnet Mapping: zip_postal -> postal_code: 0.75\n",
      "✅ Completed addresses_df → addresses\n",
      "   Missing cols: []\n",
      "   Extra cols: ['street_line1', 'latitude', 'longitude', 'street_line2', 'address_verified', 'last_modified']\n",
      "   Preview:\n",
      "+----------+-----------+------------+----------+--------------+-----------+--------------+----------+--------------------+\n",
      "|address_id|customer_id|address_type|      city|state_province|postal_code|       country|is_default|          created_at|\n",
      "+----------+-----------+------------+----------+--------------+-----------+--------------+----------+--------------------+\n",
      "|     50555| CUST_01343|        Both| West Sean| Massachusetts|       0942|       Moldova|      True|2025-06-01 18:41:...|\n",
      "|     50938| CUST_01357|    Shipping|Lake Jamie|    Queensland|      27893|American Samoa|     False|2025-07-25 09:03:...|\n",
      "|     50060| CUST_99999|        NULL|      NULL|          NULL|       NULL|            GB|       Yes|                NULL|\n",
      "+----------+-----------+------------+----------+--------------+-----------+--------------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: categories_df\n",
      "Processing → categories with 5 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['created_at']\n",
      "['category_id', 'category_name', 'parent_category_id', 'is_active', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['category_id', 'category_name', 'parent_category_id', 'is_active', 'created_at']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['category_id', 'category_name', 'parent_category_id', 'is_active', 'created_at']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['category_id', 'category_name', 'parent_category_id', 'is_active', 'created_at']\n",
      "Implementing spaCy Mapping...\n",
      "\n",
      "After spaCy Mapping:\n",
      "Missing columns: ['created_at']\n",
      "Implementing Word2Vec Mapping...\n",
      "\n",
      "After Word2Vec Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['category_id', 'category_name', 'parent_category_id', 'is_active', 'created_at']\n",
      "Implementing BERT Mapping...\n",
      "\n",
      "After roBERTa Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['category_id', 'category_name', 'parent_category_id', 'is_active', 'created_at']\n",
      "Implementing GPT Mapping...\n",
      "Full response: {'error': 'Not found'}\n",
      "Error parsing model output: 'choices'\n",
      "✅ Completed categories_df → categories\n",
      "   Missing cols: ['created_at']\n",
      "   Extra cols: ['date_created', 'category_desc', 'product_count', 'breadcrumb_path', 'display_sequence', 'hierarchy_level', 'url_slug']\n",
      "   Preview:\n",
      "+------+------------------+-------------+-----------+--------------------+--------------------+-------------+---------------+----------------+---------------+------------------+\n",
      "|cat_id|          cat_name|parent_cat_id|active_flag|        date_created|       category_desc|product_count|breadcrumb_path|display_sequence|hierarchy_level|          url_slug|\n",
      "+------+------------------+-------------+-----------+--------------------+--------------------+-------------+---------------+----------------+---------------+------------------+\n",
      "|   295|           Spoorts|          222|       True|2024-08-24 17:55:...|Fuga praesentium ...|          611|           NULL|              27|              0|              NULL|\n",
      "|   259|Premium Collection|          103|       True|2025-08-27 01:46:...|Explicabo archite...|          390|           NULL|              64|           NULL|premium-collection|\n",
      "|   171|     Boys Clothing|         NULL|       True|2024-11-02 18:52:...|                NULL|          811|           NULL|            NULL|           NULL|              NULL|\n",
      "+------+------------------+-------------+-----------+--------------------+--------------------+-------------+---------------+----------------+---------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: customer_sessions_df\n",
      "Processing → customer_sessions with 10 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['session_id']\n",
      "['session_id', 'customer_id', 'session_start', 'session_end', 'device_type', 'referrer_source', 'pages_viewed', 'products_viewed', 'conversion_flag', 'cart_abandonment_flag']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: session_ref -> session_id: 76.19\n",
      "✅ Completed customer_sessions_df → customer_sessions\n",
      "   Missing cols: []\n",
      "   Extra cols: ['bounce_session', 'entry_page', 'exit_page', 'visitor_type', 'session_duration_sec', 'cart_items_count', 'browser_name', 'geo_country', 'session_revenue']\n",
      "   Preview:\n",
      "+----------+-----------+--------------------+--------------------+-----------+---------------+------------+---------------+---------------+---------------------+\n",
      "|session_id|customer_id|       session_start|         session_end|device_type|referrer_source|pages_viewed|products_viewed|conversion_flag|cart_abandonment_flag|\n",
      "+----------+-----------+--------------------+--------------------+-----------+---------------+------------+---------------+---------------+---------------------+\n",
      "|  ec570e42| CUST_01003|2025-09-11 01:44:...|2025-09-11 01:45:...|     Mobile|    Display Ads|           1|              1|          False|                 True|\n",
      "|  92564A19| CUST_01173|2025-09-08 20:59:...|2025-09-08 21:02:...|     Mobile|   Facebook Ads|           2|            1  |        False  |                 True|\n",
      "|  c0dcdd86| CUST_01055|2025-09-05 02:58:...|2025-09-05 03:08:...|    Desktop|         Direct|           4|              1|          False|                False|\n",
      "+----------+-----------+--------------------+--------------------+-----------+---------------+------------+---------------+---------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: customers_df\n",
      "Processing → customers with 10 canonical columns\n",
      "✅ Completed customers_df → customers\n",
      "   Missing cols: []\n",
      "   Extra cols: ['email_address', 'phone_number', 'last_purchase_date', 'total_purchases', 'loyalty_points', 'age', 'lifetime_value', 'country']\n",
      "   Preview:\n",
      "+-----------+---------------+-------------+------+-------------------+--------------------+---------------+-------------------+----------------+--------------------+\n",
      "|customer_id|  customer_name|customer_type|gender|      date_of_birth|   registration_date|customer_status|acquisition_channel|customer_segment|          created_at|\n",
      "+-----------+---------------+-------------+------+-------------------+--------------------+---------------+-------------------+----------------+--------------------+\n",
      "|      10369|Cynthia Gregory|          B2C|  Male|1971-10-04 00:00:00|2022-11-11 11:16:...|       Inactive|           LinkedIn|      High Value|2024-12-21 08:06:...|\n",
      "|      10078|Ms Karen Turner|       Guest*|Female|1994-04-30 00:00:00|2025-05-16 17:36:...|       Inactive|            Twitter|Occasional Buyer|2025-07-26 11:31:...|\n",
      "|      10299|  Maureen Lewis|          B2C| Other|1996-07-11 00:00:00|2022-12-17 23:00:...|        Blocked|           Referral|         Churned|2025-05-27 04:16:...|\n",
      "+-----------+---------------+-------------+------+-------------------+--------------------+---------------+-------------------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: inventories_df\n",
      "Processing → inventory with 10 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at']\n",
      "['inventory_id', 'product_id', 'supplier_id', 'stock_quantity', 'reserved_quantity', 'reorder_level', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: last_restock_date -> last_restocked_date: 94.44\n",
      "RapidFuzz Mapping: last_sale_date -> last_sold_date: 85.71\n",
      "RapidFuzz Mapping: monthly_storage_cost -> storage_cost: 75.00\n",
      "RapidFuzz Mapping: created_date -> created_at: 90.91\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level']\n",
      "['inventory_id', 'product_id', 'supplier_id', 'stock_quantity', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at', 'reserved_quantity', 'reorder_level']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level']\n",
      "['inventory_id', 'product_id', 'supplier_id', 'stock_quantity', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at', 'reserved_quantity', 'reorder_level']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level']\n",
      "['inventory_id', 'product_id', 'supplier_id', 'stock_quantity', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at', 'reserved_quantity', 'reorder_level']\n",
      "Implementing spaCy Mapping...\n",
      "\n",
      "After spaCy Mapping:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level']\n",
      "Implementing Word2Vec Mapping...\n",
      "\n",
      "After Word2Vec Mapping:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level']\n",
      "['inventory_id', 'product_id', 'supplier_id', 'stock_quantity', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at', 'reserved_quantity', 'reorder_level']\n",
      "Implementing BERT Mapping...\n",
      "\n",
      "After roBERTa Mapping:\n",
      "Missing columns: ['reserved_quantity', 'reorder_level']\n",
      "['inventory_id', 'product_id', 'supplier_id', 'stock_quantity', 'last_restocked_date', 'last_sold_date', 'storage_cost', 'created_at', 'reserved_quantity', 'reorder_level']\n",
      "Implementing GPT Mapping...\n",
      "Full response: {'error': 'Not found'}\n",
      "Error parsing model output: 'choices'\n",
      "✅ Completed inventories_df → inventory\n",
      "   Missing cols: ['reserved_quantity', 'reorder_level']\n",
      "   Extra cols: ['reserved_stock', 'min_stock_level', 'available_qty', 'days_since_last_sale', 'stock_status', 'warehouse_location', 'total_stock_value', 'restock_lead_time_days', 'expiry_date']\n",
      "   Preview:\n",
      "+------+-----------+------------+-------------+--------------+---------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+------------+------------------+-----------------+----------------------+-----------+\n",
      "|inv_id|product_ref|   vendor_id|current_stock|reserved_stock|min_stock_level|   last_restock_date|      last_sale_date|monthly_storage_cost|        created_date|available_qty|days_since_last_sale|stock_status|warehouse_location|total_stock_value|restock_lead_time_days|expiry_date|\n",
      "+------+-----------+------------+-------------+--------------+---------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+------------+------------------+-----------------+----------------------+-----------+\n",
      "|100555|  PROD_0781|    SUPP_017|           72|            21|             69|2025-08-25 18:33:...|2025-09-07 03:54:...|                0.74|2024-05-31 17:01:...|         NULL|                NULL|    In Stock|        WH-EAST-01|             NULL|                  NULL|       NULL|\n",
      "|100938|  PROD_0373|    SUPP_003|            8|             4|             85|2025-08-13 17:25:...|2025-09-06 17:27:...|                3.25|2025-05-26 09:32:...|         NULL|                NULL|        NULL|              NULL|             NULL|                     7|       NULL|\n",
      "|100060|  PROD_9999|  SUPP_029  |       Many  |             0|           NULL|2025-08-25 20:31:...|                NULL|                3.14|                NULL|         NULL|                NULL|        NULL|     WH-CENTRAL-01|             NULL|                  NULL|       NULL|\n",
      "+------+-----------+------------+-------------+--------------+---------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+------------+------------------+-----------------+----------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: marketing_campaigns_df\n",
      "Processing → marketing_campaigns with 13 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['campaign_name', 'campaign_type', 'end_date', 'spent_amount', 'impressions', 'clicks', 'conversions', 'target_audience']\n",
      "['campaign_id', 'campaign_name', 'campaign_type', 'start_date', 'end_date', 'budget', 'spent_amount', 'impressions', 'clicks', 'conversions', 'target_audience', 'campaign_status', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: campaign_manager -> campaign_name: 82.76\n",
      "RapidFuzz Mapping: campaign_title -> campaign_type: 81.48\n",
      "RapidFuzz Mapping: total_impressions -> impressions: 78.57\n",
      "RapidFuzz Mapping: conversion_rate -> conversions: 76.92\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['end_date', 'spent_amount', 'clicks', 'target_audience']\n",
      "['campaign_id', 'campaign_type', 'start_date', 'budget', 'impressions', 'campaign_status', 'created_at', 'campaign_name', 'conversions', 'end_date', 'spent_amount', 'clicks', 'target_audience']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['end_date', 'spent_amount', 'clicks', 'target_audience']\n",
      "['campaign_id', 'campaign_type', 'start_date', 'budget', 'impressions', 'campaign_status', 'created_at', 'campaign_name', 'conversions', 'end_date', 'spent_amount', 'clicks', 'target_audience']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "Wordnet Mapping: total_clicks -> clicks: 1.00\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['end_date', 'spent_amount', 'target_audience']\n",
      "['campaign_id', 'campaign_type', 'start_date', 'budget', 'impressions', 'clicks', 'campaign_status', 'created_at', 'campaign_name', 'conversions', 'end_date', 'spent_amount', 'target_audience']\n",
      "Implementing spaCy Mapping...\n",
      "spaCy Mapping: current_spend -> end_date: 0.75\n",
      "\n",
      "After spaCy Mapping:\n",
      "Missing columns: ['spent_amount', 'target_audience']\n",
      "Implementing Word2Vec Mapping...\n",
      "\n",
      "After Word2Vec Mapping:\n",
      "Missing columns: ['spent_amount', 'target_audience']\n",
      "['campaign_id', 'campaign_type', 'start_date', 'budget', 'end_date', 'impressions', 'clicks', 'campaign_status', 'created_at', 'campaign_name', 'conversions', 'spent_amount', 'target_audience']\n",
      "Implementing BERT Mapping...\n",
      "\n",
      "After roBERTa Mapping:\n",
      "Missing columns: ['spent_amount', 'target_audience']\n",
      "['campaign_id', 'campaign_type', 'start_date', 'budget', 'end_date', 'impressions', 'clicks', 'campaign_status', 'created_at', 'campaign_name', 'conversions', 'spent_amount', 'target_audience']\n",
      "Implementing GPT Mapping...\n",
      "Full response: {'error': 'Not found'}\n",
      "Error parsing model output: 'choices'\n",
      "✅ Completed marketing_campaigns_df → marketing_campaigns\n",
      "   Missing cols: ['spent_amount', 'target_audience']\n",
      "   Extra cols: ['channel_type', 'completion_date', 'conversion_count', 'target_segment', 'ctr_rate', 'avg_cpc', 'test_variant', 'roi_percentage', 'ad_platform']\n",
      "   Preview:\n",
      "+------------+--------------------+------------+-------------------+-------------------+----------------+-------------+-----------------+------------+----------------+--------------------+---------------+--------------------+--------+-------+----------------+------------+--------------+---------------+-----------+\n",
      "|campaign_ref|      campaign_title|channel_type|        launch_date|    completion_date|allocated_budget|current_spend|total_impressions|total_clicks|conversion_count|      target_segment|campaign_status|   created_timestamp|ctr_rate|avg_cpc|campaign_manager|test_variant|roi_percentage|conversion_rate|ad_platform|\n",
      "+------------+--------------------+------------+-------------------+-------------------+----------------+-------------+-----------------+------------+----------------+--------------------+---------------+--------------------+--------+-------+----------------+------------+--------------+---------------+-----------+\n",
      "|    10390  !|  Summer Sale 2025  |       EMAIL|               NULL|         09/10/2025|            NULL|            0|             NULL|     1000.25|            NULL|Target: 18-65+, $...|           Live|                NULL|    NULL|   NULL|            NULL|        NULL|          NULL|           NULL|       NULL|\n",
      "|       10018|Customer Retentio...|         SEO|2025-09-10 00:00:00|2025-09-12 00:00:00|           15116|      7091.48|          1191114|       29346|            2892|     budget shoppers|         Paused|2025-08-22 01:36:...|    NULL|   NULL|  Malcolm Coates|        NULL|          NULL|           NULL|       NULL|\n",
      "|       10121|Lead Generation 2...|          TV|2025-09-10 00:00:00|2025-10-06 00:00:00|       205235.32|      6392.49|           419874|        9958|             961|    College students|         Active|2025-08-26 04:40:...|    NULL|   NULL|            NULL|        NULL|          NULL|           NULL|       NULL|\n",
      "+------------+--------------------+------------+-------------------+-------------------+----------------+-------------+-----------------+------------+----------------+--------------------+---------------+--------------------+--------+-------+----------------+------------+--------------+---------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: order_items_df\n",
      "Processing → order_items with 9 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['quantity', 'unit_price', 'created_at']\n",
      "['order_item_id', 'order_id', 'product_id', 'quantity', 'unit_price', 'total_price', 'discount_amount', 'product_cost', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: unit_selling_price -> unit_price: 71.43\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['quantity', 'created_at']\n",
      "['order_item_id', 'order_id', 'product_id', 'unit_price', 'discount_amount', 'total_price', 'product_cost', 'quantity', 'created_at']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['quantity', 'created_at']\n",
      "['order_item_id', 'order_id', 'product_id', 'unit_price', 'discount_amount', 'total_price', 'product_cost', 'quantity', 'created_at']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "Wordnet Mapping: tax_amount -> quantity: 1.00\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['order_item_id', 'order_id', 'product_id', 'unit_price', 'discount_amount', 'total_price', 'product_cost', 'quantity', 'created_at']\n",
      "Implementing spaCy Mapping...\n",
      "\n",
      "After spaCy Mapping:\n",
      "Missing columns: ['created_at']\n",
      "Implementing Word2Vec Mapping...\n",
      "\n",
      "After Word2Vec Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['order_item_id', 'order_id', 'product_id', 'unit_price', 'discount_amount', 'total_price', 'product_cost', 'quantity', 'created_at']\n",
      "Implementing BERT Mapping...\n",
      "\n",
      "After roBERTa Mapping:\n",
      "Missing columns: ['created_at']\n",
      "['order_item_id', 'order_id', 'product_id', 'unit_price', 'discount_amount', 'total_price', 'product_cost', 'quantity', 'created_at']\n",
      "Implementing GPT Mapping...\n",
      "Full response: {'error': 'Not found'}\n",
      "Error parsing model output: 'choices'\n",
      "✅ Completed order_items_df → order_items\n",
      "   Missing cols: ['created_at']\n",
      "   Extra cols: ['qty_ordered', 'created_timestamp', 'product_sku', 'total_weight_kg', 'is_gift', 'profit_margin', 'fulfillment_location', 'return_status']\n",
      "   Preview:\n",
      "+------------+----------+-----------+-----------+------------------+-------------+----------+----------+--------------------+------------+---------------+-------+----------+-------------+--------------------+-------------+\n",
      "|line_item_id| order_ref|product_ref|qty_ordered|unit_selling_price|item_discount|line_total| unit_cost|   created_timestamp| product_sku|total_weight_kg|is_gift|tax_amount|profit_margin|fulfillment_location|return_status|\n",
      "+------------+----------+-----------+-----------+------------------+-------------+----------+----------+--------------------+------------+---------------+-------+----------+-------------+--------------------+-------------+\n",
      "|      102777|ORD_100833|  PROD_0237|          3|             34.45|            0|    103.35|     24.21|2023-10-21 23:14:...|        NULL|           NULL|   NULL|      NULL|        30.72|                NULL|         NULL|\n",
      "|      101421|ORD_100001|  PROD_0053|        100|            400.55|       4005.5|   36049.5|  184.03  |  2024-08-16 04:1...|        NULL|         208.12|   NULL|      NULL|         NULL|       VENDOR-DIRECT|         NULL|\n",
      "|     101346#|ORD_100169|  PROD_0085|          1|            280.14|         34.6|    245.54|    161.41|2025-07-15 05:58:...|SKU-0085-RED|           NULL|   NULL|      NULL|         NULL|                NULL|         NULL|\n",
      "+------------+----------+-----------+-----------+------------------+-------------+----------+----------+--------------------+------------+---------------+-------+----------+-------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: orders_df\n",
      "Processing → orders with 16 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['customer_id', 'session_id', 'subtotal', 'device_type']\n",
      "['order_id', 'customer_id', 'session_id', 'order_date', 'order_status', 'subtotal', 'tax_amount', 'shipping_cost', 'discount_amount', 'total_amount', 'currency', 'acquisition_channel', 'device_type', 'shipped_date', 'delivered_date', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: customer_ip -> customer_id: 90.91\n",
      "RapidFuzz Mapping: session_ref -> session_id: 76.19\n",
      "RapidFuzz Mapping: order_subtotal -> subtotal: 72.73\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['device_type']\n",
      "['order_id', 'session_id', 'order_date', 'order_status', 'subtotal', 'tax_amount', 'shipping_cost', 'discount_amount', 'total_amount', 'currency', 'acquisition_channel', 'shipped_date', 'delivered_date', 'created_at', 'customer_id', 'device_type']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['device_type']\n",
      "['order_id', 'session_id', 'order_date', 'order_status', 'subtotal', 'tax_amount', 'shipping_cost', 'discount_amount', 'total_amount', 'currency', 'acquisition_channel', 'shipped_date', 'delivered_date', 'created_at', 'customer_id', 'device_type']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['device_type']\n",
      "['order_id', 'session_id', 'order_date', 'order_status', 'subtotal', 'tax_amount', 'shipping_cost', 'discount_amount', 'total_amount', 'currency', 'acquisition_channel', 'shipped_date', 'delivered_date', 'created_at', 'customer_id', 'device_type']\n",
      "Implementing spaCy Mapping...\n",
      "\n",
      "After spaCy Mapping:\n",
      "Missing columns: ['device_type']\n",
      "Implementing Word2Vec Mapping...\n",
      "\n",
      "After Word2Vec Mapping:\n",
      "Missing columns: ['device_type']\n",
      "['order_id', 'session_id', 'order_date', 'order_status', 'subtotal', 'tax_amount', 'shipping_cost', 'discount_amount', 'total_amount', 'currency', 'acquisition_channel', 'shipped_date', 'delivered_date', 'created_at', 'customer_id', 'device_type']\n",
      "Implementing BERT Mapping...\n",
      "BERT Mapping: device_category -> device_type (score=0.72)\n",
      "✅ Completed orders_df → orders\n",
      "   Missing cols: []\n",
      "   Extra cols: ['customer_ref', 'coupon_code', 'payment_method', 'shipping_method', 'item_count', 'refund_amount']\n",
      "   Preview:\n",
      "+--------+--------------------+--------------------+------------+--------+----------+-------------+---------------+------------+--------+-------------------+-----------+------------+--------------+--------------------+---------------+\n",
      "|order_id|          session_id|          order_date|order_status|subtotal|tax_amount|shipping_cost|discount_amount|total_amount|currency|acquisition_channel|device_type|shipped_date|delivered_date|          created_at|    customer_id|\n",
      "+--------+--------------------+--------------------+------------+--------+----------+-------------+---------------+------------+--------+-------------------+-----------+------------+--------------+--------------------+---------------+\n",
      "|    NULL|                NULL|                NULL|        NULL|    NULL|          |         NULL|           NULL|        NULL|  NULL  |               NULL|       NULL|        NULL|          NULL|                NULL|           NULL|\n",
      "| 1001213|7880adc6-2af2-471...|2024-07-17 01:16:...|  Processing|   804.3|     48.96|            0|              0|      853.26|     EUR|       Google Ads  |     mobile|        NULL|          NULL|2024-07-17 01:37:...|191.108.140.203|\n",
      "| 1001879|f46976af-10be-43e...|2024-11-01 11:23:...|     Pending| 1468.76|    134.58|         8.32|              0|     1611.66|     USD|          Instagram|      App  |        NULL|          NULL|2024-11-01 12:18:...|           NULL|\n",
      "+--------+--------------------+--------------------+------------+--------+----------+-------------+---------------+------------+--------+-------------------+-----------+------------+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: payments_df\n",
      "Processing → payments with 12 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['payment_provider', 'transaction_id', 'refund_date']\n",
      "['payment_id', 'order_id', 'payment_method', 'payment_provider', 'payment_status', 'payment_date', 'amount', 'transaction_id', 'processing_fee', 'refund_amount', 'refund_date', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: gateway_transaction_id -> transaction_id: 77.78\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['payment_provider', 'refund_date']\n",
      "['payment_id', 'order_id', 'payment_method', 'payment_status', 'payment_date', 'amount', 'transaction_id', 'processing_fee', 'refund_amount', 'created_at', 'payment_provider', 'refund_date']\n",
      "Implementing NLTK Combination Mapping...\n",
      "NLTK Mapping: refund_processed_date -> refund_date: 0.71\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['payment_provider']\n",
      "['payment_id', 'order_id', 'payment_method', 'payment_status', 'payment_date', 'amount', 'transaction_id', 'processing_fee', 'refund_amount', 'refund_date', 'created_at', 'payment_provider']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['payment_provider']\n",
      "['payment_id', 'order_id', 'payment_method', 'payment_status', 'payment_date', 'amount', 'transaction_id', 'processing_fee', 'refund_amount', 'refund_date', 'created_at', 'payment_provider']\n",
      "Implementing spaCy Mapping...\n",
      "spaCy Mapping: gateway_provider -> payment_provider: 0.74\n",
      "✅ Completed payments_df → payments\n",
      "   Missing cols: []\n",
      "   Extra cols: ['risk_score', 'billing_country', 'authorization_code', 'customer_ip', 'currency_code', 'card_last_four', 'card_brand', 'retry_attempt']\n",
      "   Preview:\n",
      "+----------+----------+--------------+----------------+--------------+--------------------+-----------+--------------------+--------------+-------------+-----------+--------------------+\n",
      "|payment_id|  order_id|payment_method|payment_provider|payment_status|        payment_date|     amount|      transaction_id|processing_fee|refund_amount|refund_date|          created_at|\n",
      "+----------+----------+--------------+----------------+--------------+--------------------+-----------+--------------------+--------------+-------------+-----------+--------------------+\n",
      "|   1002257|ORD_100814|     Gift Card|    Store Credit|   Completed  |2025-04-24 19:11:...|     772.11|TXN-D64F7187-44A9-42|          19.3|            0|       NULL|2025-04-24 19:35:...|\n",
      "|   1002725|ORD_101391|          NULL|          PayPal|     Completed|                NULL|    1812.66|PP-86958649-41EE-...|          NULL|            0|       NULL|2025-03-17 05:19:...|\n",
      "|   1001497|ORD_101115|Cryptocurrency|          BitPay|     Completed|2025-04-16 06:26:...|  1116.97  |  TXN-11B0830F-71...|         27.92|            0|       NULL|2025-04-16 07:02:...|\n",
      "+----------+----------+--------------+----------------+--------------+--------------------+-----------+--------------------+--------------+-------------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: products_df\n",
      "Processing → products with 19 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['product_name', 'cost_price', 'average_rating']\n",
      "['product_id', 'product_name', 'sku', 'category_id', 'brand', 'supplier_id', 'cost_price', 'selling_price', 'launch_date', 'product_status', 'is_digital', 'average_rating', 'total_reviews', 'created_at']\n",
      "Implementing RapidFuzz Mapping...\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['product_name', 'cost_price', 'average_rating']\n",
      "['product_id', 'sku', 'category_id', 'brand', 'supplier_id', 'selling_price', 'launch_date', 'product_status', 'is_digital', 'total_reviews', 'created_at', 'product_name', 'cost_price', 'average_rating']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['product_name', 'cost_price', 'average_rating']\n",
      "['product_id', 'sku', 'category_id', 'brand', 'supplier_id', 'selling_price', 'launch_date', 'product_status', 'is_digital', 'total_reviews', 'created_at', 'product_name', 'cost_price', 'average_rating']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "Wordnet Mapping: unit_cost -> cost_price: 1.00\n",
      "\n",
      "After WordNet Semantic Mapping:\n",
      "Missing columns: ['product_name', 'average_rating']\n",
      "['product_id', 'sku', 'category_id', 'brand', 'supplier_id', 'cost_price', 'selling_price', 'launch_date', 'product_status', 'is_digital', 'total_reviews', 'created_at', 'product_name', 'average_rating']\n",
      "Implementing spaCy Mapping...\n",
      "spaCy Mapping: product_description -> product_name: 0.82\n",
      "spaCy Mapping: avg_customer_rating -> average_rating: 0.92\n",
      "✅ Completed products_df → products\n",
      "   Missing cols: []\n",
      "   Extra cols: ['color', 'size', 'weight', 'dimensions', 'inventory_qty', 'discount_pct']\n",
      "   Preview:\n",
      "+----------+--------------------+-------------+-----------+-----+-----------+----------+-------------+-------------------+----------------+----------+--------------+-------------+--------------------+\n",
      "|product_id|        product_name|          sku|category_id|brand|supplier_id|cost_price|selling_price|        launch_date|  product_status|is_digital|average_rating|total_reviews|          created_at|\n",
      "+----------+--------------------+-------------+-----------+-----+-----------+----------+-------------+-------------------+----------------+----------+--------------+-------------+--------------------+\n",
      "|     10369|           JBL Cable|JB-EL0369-GRE|    CAT_115|ASICS|   SUPP_004|    444.81|       548.78|2023-09-10 00:00:00|          Active|      True|        2.84  |           91|2025-05-05 09:59:...|\n",
      "|     10078|Google Basketball...|GO-CL0078-BRO|    CAT_119|Nikon|   SUPP_003|     64.32|       152.87|2023-11-23 00:00:00|    Out of Stock|      True|          2.17|            1|2024-11-14 14:49:...|\n",
      "|     10299|Adidas Screen Pro...|AD-EL0299-WHI|    CAT_105|Nikon|   SUPP_013|    243.35|       442.69|2022-08-03 00:00:00|  Discontinued  |      True|          3.12|           56|2025-01-19 02:38:...|\n",
      "+----------+--------------------+-------------+-----------+-----+-----------+----------+-------------+-------------------+----------------+----------+--------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: reviews_df\n",
      "Processing → reviews with 8 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['customer_id']\n",
      "['review_id', 'product_id', 'customer_id', 'rating', 'review_title', 'review_text', 'review_date', 'is_verified_purchase']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: customer_ref -> customer_id: 78.26\n",
      "✅ Completed reviews_df → reviews\n",
      "   Missing cols: []\n",
      "   Extra cols: ['moderation_status', 'submission_device', 'helpful_count', 'total_votes', 'image_count', 'review_language', 'reviewer_name', 'seller_response', 'review_date_only']\n",
      "   Preview:\n",
      "+----------+----------+-----------+------+--------------+--------------------+--------------------+--------------------+\n",
      "| review_id|product_id|customer_id|rating|  review_title|         review_text|         review_date|is_verified_purchase|\n",
      "+----------+----------+-----------+------+--------------+--------------------+--------------------+--------------------+\n",
      "|  101338  | PROD_0016| CUST_01336|     4|Great product!|Consumer ball nor...|2024-11-06 18:18:...|               False|\n",
      "|    100419| PROD_0020| CUST_01253|   1  |  Not worth it|Sit doloribus tem...|2025-09-05 18:18:...|                True|\n",
      "|    100887| PROD_0103| CUST_01390|     1|  Disappointed|Eligendi eligendi...|2025-08-22 18:18:...|               false|\n",
      "+----------+----------+-----------+------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: shopping_carts_df\n",
      "Processing → shopping_cart with 9 canonical columns\n",
      "\n",
      "After Initial Normalization:\n",
      "Missing columns: ['cart_id', 'customer_id', 'session_id', 'product_id', 'quantity', 'added_date']\n",
      "['cart_id', 'customer_id', 'session_id', 'product_id', 'quantity', 'unit_price', 'added_date', 'cart_status', 'converted_order_id']\n",
      "Implementing RapidFuzz Mapping...\n",
      "RapidFuzz Mapping: cart_item_id -> cart_id: 73.68\n",
      "RapidFuzz Mapping: customer_ref -> customer_id: 78.26\n",
      "RapidFuzz Mapping: session_identifier -> session_id: 71.43\n",
      "RapidFuzz Mapping: product_ref -> product_id: 76.19\n",
      "RapidFuzz Mapping: item_quantity -> quantity: 76.19\n",
      "\n",
      "After RapidFuzz Mapping:\n",
      "Missing columns: ['added_date']\n",
      "['cart_id', 'customer_id', 'session_id', 'product_id', 'quantity', 'unit_price', 'cart_status', 'converted_order_id', 'added_date']\n",
      "Implementing NLTK Combination Mapping...\n",
      "\n",
      "After NLTK Combination Mapping:\n",
      "Missing columns: ['added_date']\n",
      "['cart_id', 'customer_id', 'session_id', 'product_id', 'quantity', 'unit_price', 'cart_status', 'converted_order_id', 'added_date']\n",
      "Implementing WordNet Semantic Mapping...\n",
      "Wordnet Mapping: date_added_to_cart -> added_date: 1.00\n",
      "✅ Completed shopping_carts_df → shopping_cart\n",
      "   Missing cols: []\n",
      "   Extra cols: ['device_type', 'discount_amount', 'promo_code', 'ip_address', 'tax_amount', 'last_updated', 'saved_for_later']\n",
      "   Preview:\n",
      "+-----------+-----------+--------------------+-------------+--------+----------+--------------------+-----------+------------------+\n",
      "|    cart_id|customer_id|          session_id|   product_id|quantity|unit_price|          added_date|cart_status|converted_order_id|\n",
      "+-----------+-----------+--------------------+-------------+--------+----------+--------------------+-----------+------------------+\n",
      "|  1001338  |       NULL|3dabba61-8a69-465...|    PROD_0046|       1|      8.39|2025-08-17 06:41:...|  Abandoned|              NULL|\n",
      "|    1000419| CUST_01154|94fb09f0-53c9-4ae...|  PROD_0103  |       1|    179.01|2025-09-05 23:55:...|     Active|              NULL|\n",
      "|    1000887| CUST_01027|                NULL|    PROD_0234|       1|    354.62|2025-08-29 05:51:...|  ABANDONED|              NULL|\n",
      "+-----------+-----------+--------------------+-------------+--------+----------+--------------------+-----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: suppliers_df\n",
      "Processing → suppliers with 6 canonical columns\n",
      "✅ Completed suppliers_df → suppliers\n",
      "   Missing cols: []\n",
      "   Extra cols: ['status', 'supplier_category', 'tax_id']\n",
      "   Preview:\n",
      "+-----------+--------------------+--------------------+--------------+-----------+--------------------+\n",
      "|supplier_id|       supplier_name|       contact_email|  phone_number|    country|          created_at|\n",
      "+-----------+--------------------+--------------------+--------------+-----------+--------------------+\n",
      "|  SUP_01369|     Pearce-Williams|rikafujiwara@koba...| 080-8687-0079|Puerto Rico|2020-09-30 15:41:...|\n",
      "|  SUP_01078|         Allison Ltd|        esato@ito.jp|    0909275571|      Chile|2022-07-31 18:52:...|\n",
      "|  SUP_01299|黄石金承网络有限公司|wigginsjesse@newm...|06 33 49 24 24|    Liberia|2025-08-07 09:33:...|\n",
      "+-----------+--------------------+--------------------+--------------+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "Incoming dataframe: wishlists_df\n",
      "Processing → wishlist with 7 canonical columns\n",
      "✅ Completed wishlists_df → wishlist\n",
      "   Missing cols: []\n",
      "   Extra cols: ['user_notes', 'price_alert_enabled', 'notification_sent', 'current_price', 'desired_quantity', 'list_name', 'price_at_addition', 'add_source']\n",
      "   Preview:\n",
      "+-----------+-----------+-------------+--------------------+--------------+--------------+--------------------+\n",
      "|wishlist_id|customer_id|   product_id|          added_date|priority_level|purchased_date|        removed_date|\n",
      "+-----------+-----------+-------------+--------------------+--------------+--------------+--------------------+\n",
      "|     100219| CUST_01117|    PROD_0074|2024-12-02 22:44:...|        Medium|          NULL|                NULL|\n",
      "|     100172| CUST_01192|    PROD_0064|2024-02-21 06:06:...|           Low|          NULL|                NULL|\n",
      "|     101366| CUST_01087|  PROD_0052  |2025-03-26 08:09:...|           Low|          NULL|2025-07-10 16:54:...|\n",
      "+-----------+-----------+-------------+--------------------+--------------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = process_all_dataframes(all_dataframes, columns_info, mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframes_to_minio(results, client, bucket_name):\n",
    "    client = Minio(\n",
    "    \"localhost:9000\",\n",
    "    access_key=\"minioadmin\",\n",
    "    secret_key=\"minioadmin\",\n",
    "    secure=False,\n",
    "    )\n",
    "\n",
    "    bucket_name = 'mapped'\n",
    "    \n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "        print(f\"Created bucket: {bucket_name}\")\n",
    "    else:\n",
    "        print(f\"Bucket already exists: {bucket_name}\")\n",
    "    \n",
    "    for result_key, result_data in results.items():\n",
    "        table_name = result_data['table_name']\n",
    "        final_df = result_data['final_df']\n",
    "        \n",
    "        print(f\"Saving {table_name} to MinIO...\")\n",
    "        \n",
    "        # Convert Spark DataFrame to Pandas\n",
    "        pdf = final_df.toPandas()\n",
    "        \n",
    "        # Save as CSV to MinIO\n",
    "        csv_buffer = BytesIO()\n",
    "        pdf.to_csv(csv_buffer, index=False)\n",
    "        csv_buffer.seek(0)\n",
    "        \n",
    "        # Upload to MinIO\n",
    "        file_name = table_name\n",
    "        minio_client.put_object(\n",
    "            bucket_name,\n",
    "            file_name,\n",
    "            csv_buffer,\n",
    "            length=len(csv_buffer.getvalue()),\n",
    "            content_type='text/csv'\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Saved {file_name} ({len(pdf)} rows)\")\n",
    "        csv_buffer.close()\n",
    "\n",
    "\n",
    "# save_dataframes_to_minio(results, client, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6e1f0247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'addresses_df__addresses': {'table_name': 'addresses',\n",
       "  'final_df': DataFrame[address_id: string, customer_id: string, address_type: string, city: string, state_province: string, postal_code: string, country: string, is_default: string, created_at: string],\n",
       "  'extra_df': DataFrame[address_id: string, customer_id: string, address_type: void, city: string, state_province: void, postal_code: void, country: string, is_default: string, created_at: string, address_category: string, street_line1: string, state_region: string, zip_postal: string, latitude: string, longitude: string, street_line2: string, address_verified: string, last_modified: string],\n",
       "  'extra_cols': ['street_line1',\n",
       "   'latitude',\n",
       "   'longitude',\n",
       "   'street_line2',\n",
       "   'address_verified',\n",
       "   'last_modified'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['is_default',\n",
       "   'address_created_at',\n",
       "   'address_id',\n",
       "   'customer_id',\n",
       "   'address_type',\n",
       "   'city',\n",
       "   'state_province',\n",
       "   'postal_code',\n",
       "   'country',\n",
       "   'created_at']},\n",
       " 'categories_df__categories': {'table_name': 'categories',\n",
       "  'final_df': DataFrame[cat_id: string, cat_name: string, parent_cat_id: string, active_flag: string, date_created: string, category_desc: string, product_count: string, breadcrumb_path: string, display_sequence: string, hierarchy_level: string, url_slug: string],\n",
       "  'extra_df': DataFrame[category_id: string, category_name: string, parent_category_id: string, is_active: string, created_at: void, date_created: string, category_desc: string, product_count: string, breadcrumb_path: string, display_sequence: string, hierarchy_level: string, url_slug: string],\n",
       "  'extra_cols': ['date_created',\n",
       "   'category_desc',\n",
       "   'product_count',\n",
       "   'breadcrumb_path',\n",
       "   'display_sequence',\n",
       "   'hierarchy_level',\n",
       "   'url_slug'],\n",
       "  'missing_cols': ['created_at'],\n",
       "  'mapped_cols': ['is_active',\n",
       "   'category_created_at',\n",
       "   'category_id',\n",
       "   'category_name',\n",
       "   'parent_category_id']},\n",
       " 'customer_sessions_df__customer_sessions': {'table_name': 'customer_sessions',\n",
       "  'final_df': DataFrame[session_id: string, customer_id: string, session_start: string, session_end: string, device_type: string, referrer_source: string, pages_viewed: string, products_viewed: string, conversion_flag: string, cart_abandonment_flag: string],\n",
       "  'extra_df': DataFrame[session_id: void, customer_id: string, session_start: string, session_end: string, device_type: string, referrer_source: string, pages_viewed: string, products_viewed: string, conversion_flag: string, cart_abandonment_flag: string, session_ref: string, bounce_session: string, entry_page: string, exit_page: string, visitor_type: string, session_duration_sec: string, cart_items_count: string, browser_name: string, geo_country: string, session_revenue: string],\n",
       "  'extra_cols': ['bounce_session',\n",
       "   'entry_page',\n",
       "   'exit_page',\n",
       "   'visitor_type',\n",
       "   'session_duration_sec',\n",
       "   'cart_items_count',\n",
       "   'browser_name',\n",
       "   'geo_country',\n",
       "   'session_revenue'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['session_end',\n",
       "   'pages_viewed',\n",
       "   'products_viewed',\n",
       "   'conversion_flag',\n",
       "   'cart_abandonment_flag',\n",
       "   'session_start',\n",
       "   'session_id',\n",
       "   'customer_id',\n",
       "   'device_type',\n",
       "   'referrer_source']},\n",
       " 'customers_df__customers': {'table_name': 'customers',\n",
       "  'final_df': DataFrame[customer_id: string, customer_name: string, customer_type: string, gender: string, date_of_birth: string, registration_date: string, customer_status: string, acquisition_channel: string, customer_segment: string, created_at: string],\n",
       "  'extra_df': DataFrame[customer_id: string, customer_name: string, customer_type: string, gender: string, date_of_birth: string, registration_date: string, customer_status: string, acquisition_channel: string, customer_segment: string, created_at: string, email_address: string, phone_number: string, last_purchase_date: string, total_purchases: string, loyalty_points: string, age: string, lifetime_value: string, country: string],\n",
       "  'extra_cols': ['email_address',\n",
       "   'phone_number',\n",
       "   'last_purchase_date',\n",
       "   'total_purchases',\n",
       "   'loyalty_points',\n",
       "   'age',\n",
       "   'lifetime_value',\n",
       "   'country'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['date_of_birth',\n",
       "   'customers_created_at',\n",
       "   'registration_date',\n",
       "   'customer_name',\n",
       "   'customer_type',\n",
       "   'gender',\n",
       "   'customer_status',\n",
       "   'acquisition_channel',\n",
       "   'customer_segment',\n",
       "   'customer_id',\n",
       "   'created_at']},\n",
       " 'inventories_df__inventory': {'table_name': 'inventory',\n",
       "  'final_df': DataFrame[inv_id: string, product_ref: string, vendor_id: string, current_stock: string, reserved_stock: string, min_stock_level: string, last_restock_date: string, last_sale_date: string, monthly_storage_cost: string, created_date: string, available_qty: string, days_since_last_sale: string, stock_status: string, warehouse_location: string, total_stock_value: string, restock_lead_time_days: string, expiry_date: string],\n",
       "  'extra_df': DataFrame[inventory_id: string, product_id: string, supplier_id: string, stock_quantity: string, reserved_quantity: void, reorder_level: void, last_restocked_date: void, last_sold_date: void, storage_cost: void, created_at: void, reserved_stock: string, min_stock_level: string, last_restock_date: string, last_sale_date: string, monthly_storage_cost: string, created_date: string, available_qty: string, days_since_last_sale: string, stock_status: string, warehouse_location: string, total_stock_value: string, restock_lead_time_days: string, expiry_date: string],\n",
       "  'extra_cols': ['reserved_stock',\n",
       "   'min_stock_level',\n",
       "   'available_qty',\n",
       "   'days_since_last_sale',\n",
       "   'stock_status',\n",
       "   'warehouse_location',\n",
       "   'total_stock_value',\n",
       "   'restock_lead_time_days',\n",
       "   'expiry_date'],\n",
       "  'missing_cols': ['reserved_quantity', 'reorder_level'],\n",
       "  'mapped_cols': ['stock_quantity',\n",
       "   'reserved_quantity',\n",
       "   'reorder_level',\n",
       "   'last_restocked_date',\n",
       "   'last_sold_date',\n",
       "   'storage_cost',\n",
       "   'inventory_created_at',\n",
       "   'inventory_id',\n",
       "   'product_id',\n",
       "   'supplier_id',\n",
       "   'created_at']},\n",
       " 'marketing_campaigns_df__marketing_campaigns': {'table_name': 'marketing_campaigns',\n",
       "  'final_df': DataFrame[campaign_ref: string, campaign_title: string, channel_type: string, launch_date: string, completion_date: string, allocated_budget: string, current_spend: string, total_impressions: string, total_clicks: string, conversion_count: string, target_segment: string, campaign_status: string, created_timestamp: string, ctr_rate: string, avg_cpc: string, campaign_manager: string, test_variant: string, roi_percentage: string, conversion_rate: string, ad_platform: string],\n",
       "  'extra_df': DataFrame[campaign_id: string, campaign_name: void, campaign_type: void, start_date: string, end_date: void, budget: string, spent_amount: void, impressions: void, clicks: void, conversions: void, target_audience: void, campaign_status: string, created_at: string, campaign_title: string, channel_type: string, completion_date: string, current_spend: string, total_impressions: string, total_clicks: string, conversion_count: string, target_segment: string, ctr_rate: string, avg_cpc: string, campaign_manager: string, test_variant: string, roi_percentage: string, conversion_rate: string, ad_platform: string],\n",
       "  'extra_cols': ['channel_type',\n",
       "   'completion_date',\n",
       "   'conversion_count',\n",
       "   'target_segment',\n",
       "   'ctr_rate',\n",
       "   'avg_cpc',\n",
       "   'test_variant',\n",
       "   'roi_percentage',\n",
       "   'ad_platform'],\n",
       "  'missing_cols': ['spent_amount', 'target_audience'],\n",
       "  'mapped_cols': ['start_date',\n",
       "   'end_date',\n",
       "   'budget',\n",
       "   'spent_amount',\n",
       "   'impressions',\n",
       "   'clicks',\n",
       "   'conversions',\n",
       "   'campaign_created_at',\n",
       "   'campaign_id',\n",
       "   'campaign_name',\n",
       "   'campaign_type',\n",
       "   'target_audience',\n",
       "   'campaign_status',\n",
       "   'created_at']},\n",
       " 'order_items_df__order_items': {'table_name': 'order_items',\n",
       "  'final_df': DataFrame[line_item_id: string, order_ref: string, product_ref: string, qty_ordered: string, unit_selling_price: string, item_discount: string, line_total: string, unit_cost: string, created_timestamp: string, product_sku: string, total_weight_kg: string, is_gift: string, tax_amount: string, profit_margin: string, fulfillment_location: string, return_status: string],\n",
       "  'extra_df': DataFrame[order_item_id: string, order_id: string, product_id: string, quantity: void, unit_price: void, total_price: string, discount_amount: string, product_cost: string, created_at: void, qty_ordered: string, unit_selling_price: string, created_timestamp: string, product_sku: string, total_weight_kg: string, is_gift: string, tax_amount: string, profit_margin: string, fulfillment_location: string, return_status: string],\n",
       "  'extra_cols': ['qty_ordered',\n",
       "   'created_timestamp',\n",
       "   'product_sku',\n",
       "   'total_weight_kg',\n",
       "   'is_gift',\n",
       "   'profit_margin',\n",
       "   'fulfillment_location',\n",
       "   'return_status'],\n",
       "  'missing_cols': ['created_at'],\n",
       "  'mapped_cols': ['quantity',\n",
       "   'unit_price',\n",
       "   'total_price',\n",
       "   'discount_amount',\n",
       "   'product_cost',\n",
       "   'order_item_created_at',\n",
       "   'order_item_id',\n",
       "   'order_id',\n",
       "   'product_id']},\n",
       " 'orders_df__orders': {'table_name': 'orders',\n",
       "  'final_df': DataFrame[order_id: string, session_id: string, order_date: string, order_status: string, subtotal: string, tax_amount: string, shipping_cost: string, discount_amount: string, total_amount: string, currency: string, acquisition_channel: string, device_type: string, shipped_date: string, delivered_date: string, created_at: string, customer_id: string],\n",
       "  'extra_df': DataFrame[order_id: string, customer_id: void, session_id: void, order_date: string, order_status: string, subtotal: void, tax_amount: string, shipping_cost: string, discount_amount: string, total_amount: string, currency: string, acquisition_channel: string, device_type: void, shipped_date: string, delivered_date: string, created_at: string, customer_ref: string, session_ref: string, order_subtotal: string, device_category: string, coupon_code: string, payment_method: string, shipping_method: string, item_count: string, customer_ip: string, refund_amount: string],\n",
       "  'extra_cols': ['customer_ref',\n",
       "   'coupon_code',\n",
       "   'payment_method',\n",
       "   'shipping_method',\n",
       "   'item_count',\n",
       "   'refund_amount'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['order_date',\n",
       "   'subtotal',\n",
       "   'tax_amount',\n",
       "   'shipping_cost',\n",
       "   'discount_amount',\n",
       "   'total_amount',\n",
       "   'shipped_date',\n",
       "   'delivered_date',\n",
       "   'order_created_at',\n",
       "   'device_type',\n",
       "   'order_id',\n",
       "   'customer_id',\n",
       "   'session_id',\n",
       "   'order_status',\n",
       "   'currency',\n",
       "   'acquisition_channel',\n",
       "   'created_at']},\n",
       " 'payments_df__payments': {'table_name': 'payments',\n",
       "  'final_df': DataFrame[payment_id: string, order_id: string, payment_method: string, payment_provider: string, payment_status: string, payment_date: string, amount: string, transaction_id: string, processing_fee: string, refund_amount: string, refund_date: string, created_at: string],\n",
       "  'extra_df': DataFrame[payment_id: string, order_id: string, payment_method: string, payment_provider: void, payment_status: string, payment_date: string, amount: string, transaction_id: void, processing_fee: string, refund_amount: string, refund_date: void, created_at: string, gateway_provider: string, gateway_transaction_id: string, refund_processed_date: string, risk_score: string, billing_country: string, authorization_code: string, customer_ip: string, currency_code: string, card_last_four: string, card_brand: string, retry_attempt: string],\n",
       "  'extra_cols': ['risk_score',\n",
       "   'billing_country',\n",
       "   'authorization_code',\n",
       "   'customer_ip',\n",
       "   'currency_code',\n",
       "   'card_last_four',\n",
       "   'card_brand',\n",
       "   'retry_attempt'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['payment_date',\n",
       "   'amount',\n",
       "   'processing_fee',\n",
       "   'refund_amount',\n",
       "   'refund_date',\n",
       "   'payment_created_at',\n",
       "   'payment_method',\n",
       "   'payment_provider',\n",
       "   'payment_status',\n",
       "   'transaction_id',\n",
       "   'payment_id',\n",
       "   'order_id',\n",
       "   'created_at']},\n",
       " 'products_df__products': {'table_name': 'products',\n",
       "  'final_df': DataFrame[product_id: string, product_name: string, sku: string, category_id: string, brand: string, supplier_id: string, cost_price: string, selling_price: string, launch_date: string, product_status: string, is_digital: string, average_rating: string, total_reviews: string, created_at: string],\n",
       "  'extra_df': DataFrame[product_id: string, product_name: void, sku: string, category_id: string, brand: string, supplier_id: string, cost_price: void, selling_price: string, launch_date: string, product_status: string, is_digital: string, average_rating: void, total_reviews: string, created_at: string, product_description: string, unit_cost: string, color: string, size: string, weight: string, dimensions: string, avg_customer_rating: string, inventory_qty: string, discount_pct: string],\n",
       "  'extra_cols': ['color',\n",
       "   'size',\n",
       "   'weight',\n",
       "   'dimensions',\n",
       "   'inventory_qty',\n",
       "   'discount_pct'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['cost_price',\n",
       "   'selling_price',\n",
       "   'launch_date',\n",
       "   'weight',\n",
       "   'is_digital',\n",
       "   'average_rating',\n",
       "   'total_reviews',\n",
       "   'product_created_at',\n",
       "   'product_id',\n",
       "   'product_name',\n",
       "   'sku',\n",
       "   'category_id',\n",
       "   'brand',\n",
       "   'supplier_id',\n",
       "   'dimensions',\n",
       "   'color',\n",
       "   'size',\n",
       "   'material',\n",
       "   'product_status',\n",
       "   'created_at']},\n",
       " 'reviews_df__reviews': {'table_name': 'reviews',\n",
       "  'final_df': DataFrame[review_id: string, product_id: string, customer_id: string, rating: string, review_title: string, review_text: string, review_date: string, is_verified_purchase: string],\n",
       "  'extra_df': DataFrame[review_id: string, product_id: string, customer_id: void, rating: string, review_title: string, review_text: string, review_date: string, is_verified_purchase: string, customer_ref: string, moderation_status: string, submission_device: string, helpful_count: string, total_votes: string, image_count: string, review_language: string, reviewer_name: string, seller_response: string, review_date_only: date],\n",
       "  'extra_cols': ['moderation_status',\n",
       "   'submission_device',\n",
       "   'helpful_count',\n",
       "   'total_votes',\n",
       "   'image_count',\n",
       "   'review_language',\n",
       "   'reviewer_name',\n",
       "   'seller_response',\n",
       "   'review_date_only'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['rating',\n",
       "   'review_date',\n",
       "   'is_verified_purchase',\n",
       "   'review_id',\n",
       "   'product_id',\n",
       "   'customer_id',\n",
       "   'review_title',\n",
       "   'review_text']},\n",
       " 'shopping_carts_df__shopping_cart': {'table_name': 'shopping_cart',\n",
       "  'final_df': DataFrame[cart_id: string, customer_id: string, session_id: string, product_id: string, quantity: string, unit_price: string, added_date: string, cart_status: string, converted_order_id: string],\n",
       "  'extra_df': DataFrame[cart_id: void, customer_id: void, session_id: void, product_id: void, quantity: void, unit_price: string, added_date: void, cart_status: string, converted_order_id: string, cart_item_id: string, customer_ref: string, session_identifier: string, product_ref: string, item_quantity: string, date_added_to_cart: string, device_type: string, discount_amount: string, promo_code: string, ip_address: string, tax_amount: string, last_updated: string, saved_for_later: string],\n",
       "  'extra_cols': ['device_type',\n",
       "   'discount_amount',\n",
       "   'promo_code',\n",
       "   'ip_address',\n",
       "   'tax_amount',\n",
       "   'last_updated',\n",
       "   'saved_for_later'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['quantity',\n",
       "   'unit_price',\n",
       "   'added_date',\n",
       "   'cart_id',\n",
       "   'customer_id',\n",
       "   'session_id',\n",
       "   'product_id',\n",
       "   'cart_status',\n",
       "   'converted_order_id']},\n",
       " 'suppliers_df__suppliers': {'table_name': 'suppliers',\n",
       "  'final_df': DataFrame[supplier_id: string, supplier_name: string, contact_email: string, phone_number: string, country: string, created_at: string],\n",
       "  'extra_df': DataFrame[supplier_id: string, supplier_name: string, contact_email: string, phone_number: string, country: string, created_at: string, status: string, supplier_category: string, tax_id: string],\n",
       "  'extra_cols': ['status', 'supplier_category', 'tax_id'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['supplier_created_at',\n",
       "   'supplier_id',\n",
       "   'supplier_name',\n",
       "   'contact_email',\n",
       "   'phone_number',\n",
       "   'country',\n",
       "   'created_at']},\n",
       " 'wishlists_df__wishlist': {'table_name': 'wishlist',\n",
       "  'final_df': DataFrame[wishlist_id: string, customer_id: string, product_id: string, added_date: string, priority_level: string, purchased_date: string, removed_date: string],\n",
       "  'extra_df': DataFrame[wishlist_id: string, customer_id: string, product_id: string, added_date: string, priority_level: string, purchased_date: string, removed_date: string, user_notes: string, price_alert_enabled: string, notification_sent: string, current_price: string, desired_quantity: string, list_name: string, price_at_addition: string, add_source: string],\n",
       "  'extra_cols': ['user_notes',\n",
       "   'price_alert_enabled',\n",
       "   'notification_sent',\n",
       "   'current_price',\n",
       "   'desired_quantity',\n",
       "   'list_name',\n",
       "   'price_at_addition',\n",
       "   'add_source'],\n",
       "  'missing_cols': [],\n",
       "  'mapped_cols': ['added_date',\n",
       "   'purchased_date',\n",
       "   'removed_date',\n",
       "   'wishlist_id',\n",
       "   'customer_id',\n",
       "   'product_id',\n",
       "   'priority_level']}}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3d5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"customers\"][\"missing_cols\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5563d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-------------+------+-------------------+--------------------+---------------+-------------------+----------------+--------------------+\n",
      "|customer_id|     customer_name|customer_type|gender|      date_of_birth|   registration_date|customer_status|acquisition_channel|customer_segment|          created_at|\n",
      "+-----------+------------------+-------------+------+-------------------+--------------------+---------------+-------------------+----------------+--------------------+\n",
      "|      10369|   Cynthia Gregory|          B2C|  Male|1971-10-04 00:00:00|2022-11-11 11:16:...|       Inactive|           LinkedIn|      High Value|2024-12-21 08:06:...|\n",
      "|      10078|   Ms Karen Turner|       Guest*|Female|1994-04-30 00:00:00|2025-05-16 17:36:...|       Inactive|            Twitter|Occasional Buyer|2025-07-26 11:31:...|\n",
      "|      10299|     Maureen Lewis|          B2C| Other|1996-07-11 00:00:00|2022-12-17 23:00:...|        Blocked|           Referral|         Churned|2025-05-27 04:16:...|\n",
      "|      10890|       James Lopez|          B2C|  Male|         1992-09-22|2024-07-03 16:48:...|        Blocked|             Direct|         At Risk|2025-03-01 14:08:...|\n",
      "|      10821|Charlotte Anderson|          B2C| Other|1966-08-28 00:00:00|2025-07-16 12:12:...|         Active|             Direct|      High Value|2024-10-04 00:42:...|\n",
      "+-----------+------------------+-------------+------+-------------------+--------------------+---------------+-------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results[\"customers\"][\"final_df\"].show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
