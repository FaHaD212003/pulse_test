{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86723c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, when, length \n",
    "from pyspark.sql.types import NumericType, IntegerType, LongType, FloatType, DoubleType, DecimalType, DateType, TimestampType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import psycopg2\n",
    "load_dotenv(find_dotenv())\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d970c",
   "metadata": {},
   "source": [
    "JAR_PATH_1 = os.path.abspath(\"./jars/hadoop-aws-3.4.0.jar\")\n",
    "JAR_PATH_2 = os.path.abspath(\"./jars/aws-sdk-s3-2.29.52.jar\")\n",
    "\n",
    "JARS_LIST = f\"{JAR_PATH_1},{JAR_PATH_2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6076f5e",
   "metadata": {},
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"analysis\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "    )\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.jars.repositories\", \"https://repo1.maven.org/maven2/\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"MINIO_ENDPOINT\"))\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"MINIO_ACCESS_KEY\"))\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"MINIO_SECRET_KEY\"))\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659c528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"analysis\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"MINIO_ENDPOINT\"))\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"MINIO_ACCESS_KEY\"))\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"MINIO_SECRET_KEY\"))\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    "    )\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015ebabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-TMGISFL.mshome.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d7a2ed1090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2c046bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=os.getenv(\"POSTGRES_DATABASE_NAME\"),\n",
    "    user=os.getenv(\"POSTGRES_USER\"),\n",
    "    password=os.getenv(\"POSTGRES_PASSWORD\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fbf29",
   "metadata": {},
   "source": [
    "LOAD DATA FROM POSTGRE DATABASE REMAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b383ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Postgres at localhost:5432...\n",
      "Found tables: ['agg_customer_sessions', 'agg_customers', 'agg_inventory', 'agg_marketing_campaigns', 'agg_order_items', 'agg_orders', 'agg_payments', 'agg_products', 'agg_reviews', 'agg_shopping_cart', 'agg_suppliers', 'agg_wishlist', 'agg_categories', 'agg_daily_aggregations', 'agg_weekly_aggregations', 'agg_monthly_aggregations', 'agg_country_aggregations', 'agg_state_aggregations', 'agg_city_aggregations', 'agg_cart_abandonment_analysis', 'agg_product_inventory_health', 'agg_supplier_inventory_health', 'agg_rfm_segmentation', 'agg_rfm_segment_summary', 'agg_product_affinity', 'agg_top_product_pairs', 'agg_product_recommendations', 'agg_category_affinity', 'agg_global_aggregations']\n",
      "Processing table: agg_customer_sessions...\n",
      "Processing table: agg_customers...\n",
      "Processing table: agg_inventory...\n",
      "Processing table: agg_marketing_campaigns...\n",
      "Processing table: agg_order_items...\n",
      "Processing table: agg_orders...\n",
      "Processing table: agg_payments...\n",
      "Processing table: agg_products...\n",
      "Processing table: agg_reviews...\n",
      "Processing table: agg_shopping_cart...\n",
      "Processing table: agg_suppliers...\n",
      "Processing table: agg_wishlist...\n",
      "Processing table: agg_categories...\n",
      "Processing table: agg_daily_aggregations...\n",
      "Processing table: agg_weekly_aggregations...\n",
      "Processing table: agg_monthly_aggregations...\n",
      "Processing table: agg_country_aggregations...\n",
      "Processing table: agg_state_aggregations...\n",
      "Processing table: agg_city_aggregations...\n",
      "Processing table: agg_cart_abandonment_analysis...\n",
      "Processing table: agg_product_inventory_health...\n",
      "Processing table: agg_supplier_inventory_health...\n",
      "Processing table: agg_rfm_segmentation...\n",
      "Processing table: agg_rfm_segment_summary...\n",
      "Processing table: agg_product_affinity...\n",
      "Processing table: agg_top_product_pairs...\n",
      "Processing table: agg_product_recommendations...\n",
      "Processing table: agg_category_affinity...\n",
      "Processing table: agg_global_aggregations...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "DB_HOST = \"localhost\" \n",
    "DB_PORT = \"5432\" \n",
    "DB_NAME = os.getenv(\"POSTGRES_DATABASE_NAME\", \"pulse\")\n",
    "DB_USER = os.getenv(\"POSTGRES_USER\", \"postgres\")\n",
    "DB_PASS = os.getenv(\"POSTGRES_PASSWORD\", \"postgres\")\n",
    "\n",
    "def get_agg_tables():\n",
    "    try:\n",
    "        print(f\"Connecting to Postgres at {DB_HOST}:{DB_PORT}...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASS\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name LIKE 'agg_%'\n",
    "        \"\"\")\n",
    "        \n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        print(f\"Found tables: {tables}\")\n",
    "\n",
    "        spark_dfs = {}\n",
    "\n",
    "        jdbc_url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "        connection_properties = {\n",
    "            \"user\": DB_USER,\n",
    "            \"password\": DB_PASS,\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "\n",
    "        for table in tables:\n",
    "            print(f\"Processing table: {table}...\")\n",
    "            df = spark.read.jdbc(url=jdbc_url, table=f'\"{table}\"', properties=connection_properties)\n",
    "            spark_dfs[table]= df\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return spark_dfs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # Print full stack trace for debugging if needed\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "dataframes = get_agg_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d52541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------+------------------+------------------+--------------------+------------------+\n",
      "|customer_segment_label|customer_count|       avg_revenue|        avg_orders|avg_days_since_order|     avg_rfm_score|\n",
      "+----------------------+--------------+------------------+------------------+--------------------+------------------+\n",
      "|       Loyal Customers|           159|2098.1374913081772|1.8238993710691824|   144.8679245283019|3.8385744234800834|\n",
      "|                Others|           153|  567.066213124183|1.1830065359477124|  2778.0653594771243|2.2135076252723316|\n",
      "|             Champions|           127|3141.9913143149593| 2.826771653543307|   30.20472440944882| 4.711286089238845|\n",
      "|               At Risk|           126| 2062.546100238095|1.5952380952380953|  3751.5873015873017| 3.066137566137567|\n",
      "|         New Customers|            78| 658.0441025641026| 1.294871794871795|  27.333333333333332|3.1837606837606844|\n",
      "+----------------------+--------------+------------------+------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[\"agg_rfm_segment_summary\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451ddee",
   "metadata": {},
   "source": [
    "ALL NULL ROWs AND DATAFRAME CHECK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212c3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_column_all_null_or_zero(df, col_name):\n",
    "    if df is None:\n",
    "        return True                    \n",
    "\n",
    "    if col_name not in df.columns:\n",
    "        return True                   \n",
    "\n",
    "    col_type = dict(df.dtypes)[col_name]\n",
    "    non_null_count = df.agg(\n",
    "        F.count(F.col(col_name)).alias(\"non_null_count\")\n",
    "    ).collect()[0][\"non_null_count\"]\n",
    "    if non_null_count == 0:\n",
    "        return True                   \n",
    "\n",
    "    \n",
    "    if col_type in (\"int\", \"bigint\", \"double\", \"float\", \"decimal\", \"smallint\", \"tinyint\"):\n",
    "        non_zero_non_null_count = df.agg(\n",
    "            F.sum(\n",
    "                F.when(\n",
    "                    (F.col(col_name).isNotNull()) & (F.col(col_name) != 0), 1\n",
    "                ).otherwise(0)\n",
    "            ).alias(\"non_zero_non_null_count\")\n",
    "        ).collect()[0][\"non_zero_non_null_count\"]\n",
    "\n",
    "        if non_zero_non_null_count == 0:\n",
    "            return True                 \n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da25b62",
   "metadata": {},
   "source": [
    "Time Grain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c44ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_grain(df, date_col, grain=\"day\"):\n",
    "    if grain == \"day\":\n",
    "        return df.withColumn(\"grain_date\", F.col(date_col))\n",
    "    elif grain == \"week\":\n",
    "        return df.withColumn(\"grain_year\", F.year(date_col)) \\\n",
    "                 .withColumn(\"grain_week\", F.weekofyear(date_col))\n",
    "    elif grain == \"month\":\n",
    "        return df.withColumn(\"grain_year\", F.year(date_col)) \\\n",
    "                 .withColumn(\"grain_month\", F.month(date_col))\n",
    "    else:\n",
    "        raise ValueError(\"grain must be 'day', 'week', or 'month'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17d506",
   "metadata": {},
   "source": [
    "A dataframe to contain all the analysis results to keep track easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf22e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683425d",
   "metadata": {},
   "source": [
    "# Customer Related Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309f92b",
   "metadata": {},
   "source": [
    "Adding Date Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15791a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_at\"):\n",
    "    dataframes[\"agg_customers\"] = dataframes[\"agg_customers\"].withColumn(\n",
    "    \"account_created_date\",\n",
    "    F.to_date(\"account_created_at\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de1c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_placed_at\"):\n",
    "    dataframes[\"agg_orders\"] = dataframes[\"agg_orders\"].withColumn(\n",
    "    \"order_date\",\n",
    "    F.to_date(\"order_placed_at\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c68b4e",
   "metadata": {},
   "source": [
    "“total units sold” per order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "252dd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_units = (\n",
    "    dataframes[\"agg_order_items\"]\n",
    "    .groupBy(\"order_id\")\n",
    "    .agg(F.sum(\"quantity\").alias(\"units_sold\"))\n",
    ")\n",
    "\n",
    "dataframes[\"agg_orders\"] = (\n",
    "    dataframes[\"agg_orders\"]\n",
    "    .join(order_units, on=\"order_id\", how=\"left\")\n",
    "    .fillna({\"units_sold\": 0})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80f9bf",
   "metadata": {},
   "source": [
    "Core KPIs per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c54b465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_kpis_over_time(df,date_col, grain=\"day\"):\n",
    "    df_g = add_time_grain(df, date_col=\"order_placed_at\", grain=grain)\n",
    "\n",
    "    if grain == \"day\":\n",
    "        group_cols = [\"grain_date\"]\n",
    "        order_cols = [\"grain_date\"]\n",
    "    elif grain == \"week\":\n",
    "        group_cols = [\"grain_year\", \"grain_week\"]\n",
    "        order_cols = [\"grain_year\", \"grain_week\"]\n",
    "    else:  # \"month\"\n",
    "        group_cols = [\"grain_year\", \"grain_month\"]\n",
    "        order_cols = [\"grain_year\", \"grain_month\"]\n",
    "\n",
    "    kpi = (\n",
    "        df_g.groupBy(*group_cols)\n",
    "            .agg(\n",
    "                F.countDistinct(\"order_id\").alias(\"total_orders\"),\n",
    "                F.sum(\"units_sold\").alias(\"total_units_sold\"),\n",
    "                F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "                F.sum(\"order_profit\").alias(\"gross_profit\"),\n",
    "                F.sum(\"net_profit\").alias(\"net_profit\")\n",
    "            ).fillna({\n",
    "                \"total_units_sold\": 0, \n",
    "                \"total_revenue\": 0, \n",
    "                \"gross_profit\": 0, \n",
    "                \"net_profit\": 0\n",
    "            })\n",
    "            .withColumn(\n",
    "                \"aov\",\n",
    "                F.when(F.col(\"total_orders\") > 0,\n",
    "                       F.col(\"total_revenue\") / F.col(\"total_orders\"))\n",
    "                 .otherwise(F.lit(0.0))\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"margin_pct\",\n",
    "                F.when(F.col(\"total_revenue\") > 0,\n",
    "                       F.col(\"gross_profit\") / F.col(\"total_revenue\"))\n",
    "                 .otherwise(F.lit(0.0))\n",
    "            )\n",
    "            .orderBy(*order_cols)\n",
    "    )\n",
    "\n",
    "    return kpi\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_placed_at\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"units_sold\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"total_amount\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_profit\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"net_profit\"):\n",
    "    analysis[\"business_health_daily\"]= core_kpis_over_time(dataframes[\"agg_orders\"],\"order_placed_at\", grain=\"day\")\n",
    "    analysis[\"business_health_weekly\"] = core_kpis_over_time(dataframes[\"agg_orders\"],\"order_placed_at\", grain=\"week\")\n",
    "    analysis[\"business_health_monthly\"] = core_kpis_over_time(dataframes[\"agg_orders\"],\"order_placed_at\", grain=\"month\")\n",
    "else:\n",
    "    print(\"One or more required columns are all null or zero in agg_orders dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b911a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|grain_year|grain_month|total_orders|total_units_sold|total_revenue|       gross_profit|         net_profit|               aov|        margin_pct|\n",
      "+----------+-----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|      1900|          1|          86|            8171|  89514.41972|-4260099.7420000015|-2424411.9420000007|1040.8653455813953|-47.59121217928397|\n",
      "|      2016|          1|           1|              49|       199.88| -9148.730000000001| -9158.550000000001|            199.88|-45.77111266760057|\n",
      "|      2016|          9|           1|               0|      1648.61|                0.0|                0.0|           1648.61|               0.0|\n",
      "+----------+-----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis[\"business_health_monthly\"].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6e8aa",
   "metadata": {},
   "source": [
    "Analysis: Margin by Category (The \"Drag\" Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbefa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_category_margins(products_df):\n",
    "   \n",
    "    category_df = (\n",
    "        products_df.groupBy(\"category\")\n",
    "            .agg(\n",
    "                F.avg(\"profit_margin\").alias(\"avg_profit_margin\"),\n",
    "                F.sum(\"total_profit\").alias(\"total_category_profit\"),\n",
    "                F.sum(\"total_revenue\").alias(\"total_category_revenue\"),\n",
    "                F.sum(\"total_units_sold\").alias(\"units_sold\")\n",
    "            ).fillna({\n",
    "                \"avg_profit_margin\": 0,\n",
    "                \"total_category_profit\": 0,\n",
    "                \"total_category_revenue\": 0,\n",
    "                \"units_sold\": 0\n",
    "            })\n",
    "            .orderBy(F.col(\"avg_profit_margin\").asc())\n",
    "    )\n",
    "    \n",
    "    return category_df\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\"):\n",
    "    analysis[\"low_margin_categories\"] = analyze_category_margins(dataframes[\"agg_products\"])\n",
    "else: \n",
    "    print(\"Category column is all NULL or zero; skipping category margin analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1ee62",
   "metadata": {},
   "source": [
    "account_status over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ed6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_distribution_over_time(df,date_col, grain=\"day\"):\n",
    "\n",
    "    # Apply shared time-grain helper\n",
    "    df_g = add_time_grain(df,  date_col=\"account_created_date\", grain=grain)\n",
    "\n",
    "    # Group/order columns based on grain\n",
    "    if grain == \"day\":\n",
    "        group_cols = [\"grain_date\"]\n",
    "        order_cols = [\"grain_date\"]\n",
    "    elif grain == \"week\":\n",
    "        group_cols = [\"grain_year\", \"grain_week\"]\n",
    "        order_cols = [\"grain_year\", \"grain_week\"]\n",
    "    elif grain == \"month\":\n",
    "        group_cols = [\"grain_year\", \"grain_month\"]\n",
    "        order_cols = [\"grain_year\", \"grain_month\"]\n",
    "    else:\n",
    "        raise ValueError(\"grain must be 'day', 'week', or 'month'\")\n",
    "\n",
    "    return (\n",
    "        df_g.groupBy(*(group_cols + [\"account_status\"]))\n",
    "            .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "            .fillna({\"customer_count\": 0})\n",
    "            .orderBy(*order_cols, \"account_status\")\n",
    "    )\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_at\"):\n",
    "    analysis[\"customer_account_status_distribution_daily\"]   = status_distribution_over_time(dataframes[\"agg_customers\"], \"day\")\n",
    "    analysis[\"customer_account_status_distribution_weekly\"]  = status_distribution_over_time(dataframes[\"agg_customers\"], \"week\")\n",
    "    analysis[\"customer_account_status_distribution_monthly\"] = status_distribution_over_time(dataframes[\"agg_customers\"], \"month\")\n",
    "else:\n",
    "    print(\"Account created at column is all NULL or zero; skipping status distribution over time analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de517c98",
   "metadata": {},
   "source": [
    "New customers per day/week/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8a837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_customers(df, date_col=\"account_created_date\", grain=\"day\"):\n",
    "    df_g = add_time_grain(df, date_col=date_col, grain=grain)\n",
    "\n",
    "    if grain == \"day\":\n",
    "        group_cols = [\"grain_date\"]\n",
    "        order_cols = [\"grain_date\"]\n",
    "    elif grain == \"week\":\n",
    "        group_cols = [\"grain_year\", \"grain_week\"]\n",
    "        order_cols = [\"grain_year\", \"grain_week\"]\n",
    "    else:   # month\n",
    "        group_cols = [\"grain_year\", \"grain_month\"]\n",
    "        order_cols = [\"grain_year\", \"grain_month\"]\n",
    "\n",
    "    new_df = (\n",
    "        df_g.groupBy(*group_cols)\n",
    "            .agg(F.countDistinct(\"customer_id\").alias(\"new_customers\"))\n",
    "            .fillna({\"new_customers\": 0})\n",
    "            .orderBy(*order_cols)\n",
    "    )\n",
    "    return new_df\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_date\"):\n",
    "    analysis[\"new_customers_daily\"] = new_customers(dataframes[\"agg_customers\"],\"account_created_date\", \"day\")\n",
    "    analysis[\"new_customers_weekly\"]  = new_customers(dataframes[\"agg_customers\"], \"account_created_date\", \"week\")\n",
    "    analysis[\"new_customers_monthly\"]= new_customers(dataframes[\"agg_customers\"], \"account_created_date\", \"month\")\n",
    "else:\n",
    "    print(\"Account created at column is all NULL or zero; skipping new customers analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea68403",
   "metadata": {},
   "source": [
    "Cumulative customer growth curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d46a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_customers(df, date_col=\"account_created_at\", grain=\"day\"):\n",
    "    new_df = new_customers(df, date_col, grain)\n",
    "\n",
    "    # Define window by time order\n",
    "    if grain == \"day\":\n",
    "        window = Window.orderBy(\"grain_date\") \\\n",
    "                       .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    elif grain == \"week\":\n",
    "        window = Window.orderBy(\"grain_year\", \"grain_week\") \\\n",
    "                       .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    else:   # month\n",
    "        window = Window.orderBy(\"grain_year\", \"grain_month\") \\\n",
    "                       .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    cum_df = new_df.withColumn(\n",
    "        \"cumulative_customers\",\n",
    "        F.sum(\"new_customers\").over(window)\n",
    "    ).fillna({\"cumulative_customers\": 0})   \n",
    "    \n",
    "    return cum_df\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_date\"):\n",
    "    analysis[\"cumulative_customers_daily\"]   = cumulative_customers(dataframes[\"agg_customers\"],\"account_created_date\", \"day\")\n",
    "    analysis[\"cumulative_customers_weekly\"]  = cumulative_customers(dataframes[\"agg_customers\"], \"account_created_date\", \"week\")\n",
    "    analysis[\"cumulative_customers_monthly\"] = cumulative_customers(dataframes[\"agg_customers\"], \"account_created_date\", \"month\")\n",
    "else:\n",
    "    print(\"Account created at column is all NULL or zero; skipping cumulative customers analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec8b1370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+--------------------+\n",
      "|grain_year|grain_month|new_customers|cumulative_customers|\n",
      "+----------+-----------+-------------+--------------------+\n",
      "|      1900|          1|           13|                  13|\n",
      "|      2021|          1|           14|                  27|\n",
      "|      2021|          2|           14|                  41|\n",
      "|      2021|          3|           14|                  55|\n",
      "|      2021|          4|           20|                  75|\n",
      "+----------+-----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis[\"cumulative_customers_monthly\"].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ccaf1",
   "metadata": {},
   "source": [
    "Total new customers by geography + time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f408aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_at\"):\n",
    "    geo_acquisition = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\", \"state_province\", \"city\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"new_customers\"))\n",
    "    )\n",
    "else:\n",
    "    print(\"Account created at column is all NULL or zero; skipping geo acquisition analysis.\")\n",
    "\n",
    "def geo_acquisition_over_time(df, date_col=\"account_created_at\", grain=\"day\"):\n",
    "    df_g = add_time_grain(df, date_col=date_col, grain=grain)\n",
    "\n",
    "    if grain == \"day\":\n",
    "        group_cols = [\"grain_date\", \"country\", \"state_province\", \"city\"]\n",
    "        order_cols = [\"grain_date\", \"country\", \"state_province\", \"city\"]\n",
    "    elif grain == \"week\":\n",
    "        group_cols = [\"grain_year\", \"grain_week\", \"country\", \"state_province\", \"city\"]\n",
    "        order_cols = [\"grain_year\", \"grain_week\", \"country\", \"state_province\", \"city\"]\n",
    "    else:  # month\n",
    "        group_cols = [\"grain_year\", \"grain_month\", \"country\", \"state_province\", \"city\"]\n",
    "        order_cols = [\"grain_year\", \"grain_month\", \"country\", \"state_province\", \"city\"]\n",
    "\n",
    "    return (\n",
    "        df_g.groupBy(*group_cols)\n",
    "            .agg(F.countDistinct(\"customer_id\").alias(\"new_customers\"))\n",
    "            .fillna({\"new_customers\": 0})\n",
    "            .orderBy(*order_cols)\n",
    "    )\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_date\"):\n",
    "    analysis[\"new_customers_geo_acquisition_daily\"]   = geo_acquisition_over_time(dataframes[\"agg_customers\"], \"account_created_date\", \"day\")\n",
    "    analysis[\"new_customers_geo_acquisition_monthly\"] = geo_acquisition_over_time(dataframes[\"agg_customers\"], \"account_created_date\", \"month\")\n",
    "else:\n",
    "    print(\"Account created at column is all NULL or zero; skipping geo acquisition over time analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ffcd7",
   "metadata": {},
   "source": [
    "Customer distribution by age group, city, state, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74127471",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_age_group\"):\n",
    "    analysis[\"customer_age_group_distribution\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"customer_age_group\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "        .orderBy(\"customer_age_group\")\n",
    "    )\n",
    "else: \n",
    "    print(\"Customer age group column is all NULL or zero; skipping age group distribution analysis.\")\n",
    "    \n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"country\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"state_province\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"city\"):\n",
    "    analysis[\"customer_city_distribution\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\", \"state_province\", \"city\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "        .orderBy(\"country\", \"state_province\", \"city\")\n",
    ")\n",
    "else:\n",
    "    print(\"Country column is all NULL or zero; skipping city distribution analysis.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"country\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"state_province\"):\n",
    "    analysis[\"customer_state_distribution\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\", \"state_province\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "        .orderBy(\"country\", \"state_province\")\n",
    ")\n",
    "    \n",
    "else:\n",
    "    print(\"Country or state_province column is all NULL or zero; skipping state distribution analysis.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"country\"):\n",
    "    analysis[\"customer_country_distribution\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "    .orderBy(\"country\")\n",
    ")\n",
    "else:\n",
    "    print(\"Country column is all NULL or zero; skipping country distribution analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115a49e",
   "metadata": {},
   "source": [
    "# Age group distribution and spending patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "450d7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_age_group\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"order_total_spent\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\"):\n",
    "    analysis[\"customer_age_group_spending\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"customer_age_group\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.avg(\"order_total_spent\").alias(\"avg_order_total_spent\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.sum(\"order_total_spent\").alias(\"total_spent\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue_age_group\")\n",
    "        ).fillna({\"avg_order_total_spent\": 0,\n",
    "                \"avg_clv\": 0,\n",
    "                \"total_spent\": 0,\n",
    "                \"total_revenue_age_group\": 0})\n",
    "        .orderBy(\"customer_age_group\")\n",
    "    )\n",
    "else: \n",
    "    print(\"Customer age group column is all NULL or zero; skipping age group spending analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416ec82",
   "metadata": {},
   "source": [
    "Gender-based product preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "914a1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"customer_id\"):\n",
    "    cust_orders = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .select(\"order_id\", \"customer_id\")\n",
    "        .join(\n",
    "            dataframes[\"agg_customers\"].select(\"customer_id\", \"gender\"),\n",
    "            on=\"customer_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"Order ID or Customer ID column is all NULL or zero; skipping customer orders join.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\") and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"order_id\") and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"product_id\") and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"quantity\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"sub_category\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"brand\"):\n",
    "    cust_order_items = (\n",
    "        cust_orders\n",
    "        .join(dataframes[\"agg_order_items\"].select(\"order_id\", \"product_id\", \"quantity\"), on=\"order_id\", how=\"inner\")\n",
    "        .join(dataframes[\"agg_products\"].select(\"product_id\", \"product_name\", \"category\", \"sub_category\", \"brand\"),\n",
    "            on=\"product_id\",\n",
    "            how=\"left\")\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in orders, order items, or products is all NULL or zero; skipping customer order items join.\")\n",
    "# Gender-based preferences by category\n",
    "if not is_column_all_null_or_zero(cust_order_items, \"gender\") and not is_column_all_null_or_zero(cust_order_items, \"category\") and not is_column_all_null_or_zero(cust_order_items, \"quantity\") and not is_column_all_null_or_zero(cust_order_items, \"product_id\") and not is_column_all_null_or_zero(cust_order_items, \"order_id\"):    \n",
    "    analysis[\"gender_category_preference\"] = (\n",
    "        cust_order_items\n",
    "        .groupBy(\"gender\", \"category\")\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"total_units\"),\n",
    "            F.countDistinct(\"product_id\").alias(\"distinct_products\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\")\n",
    "        )\n",
    "        .fillna({\"total_units\": 0, \"distinct_products\": 0, \"orders_count\": 0, \"gender\": \"Unknown\"})\n",
    "        .orderBy(\"gender\", F.col(\"total_units\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in customer order items is all NULL or zero; skipping gender category preference analysis.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(cust_order_items, \"gender\") and not is_column_all_null_or_zero(cust_order_items, \"product_id\") and not is_column_all_null_or_zero(cust_order_items, \"product_name\") and not is_column_all_null_or_zero(cust_order_items, \"category\") and not is_column_all_null_or_zero(cust_order_items, \"quantity\") and not is_column_all_null_or_zero(cust_order_items, \"order_id\"):\n",
    "    analysis[\"gender_product_preference\"] = (\n",
    "        cust_order_items\n",
    "        .groupBy(\"gender\", \"product_id\", \"product_name\", \"category\")\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"total_units\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\")\n",
    "        ).fillna({\"total_units\": 0, \"orders_count\": 0, \"gender\": \"Unknown\"})\n",
    "        .orderBy(\"gender\", F.col(\"total_units\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in customer order items is all NULL or zero; skipping gender product preference analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "980f8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+-----------------+-----------+------------+\n",
      "|gender|product_id|        product_name|         category|total_units|orders_count|\n",
      "+------+----------+--------------------+-----------------+-----------+------------+\n",
      "|  Male|      1014|Microsoft Smartph...|Sports & Outdoors|         35|           1|\n",
      "|  Male|      1738|Canon Laptop Prem...|Sports & Outdoors|         31|           1|\n",
      "|  Male|      1282|Microsoft Tablet Max|  Electronics 305|         18|           1|\n",
      "+------+----------+--------------------+-----------------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis[\"gender_product_preference\"].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427fce9",
   "metadata": {},
   "source": [
    "New vs returning customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3993eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"is_repeat_customer\"):\n",
    "    dataframes[\"agg_customers\"] = dataframes[\"agg_customers\"].withColumn(\n",
    "        \"customer_type\",\n",
    "        F.when(F.col(\"is_repeat_customer\") == 1, F.lit(\"returning\"))\n",
    "        .otherwise(F.lit(\"new\"))\n",
    "    )\n",
    "else:\n",
    "    print(\"is_repeat_customer column is all NULL or zero; skipping customer type classification.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_type\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"country\"):\n",
    "    analysis[\"new_vs_returning_customer_country\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\", \"customer_type\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "        .orderBy(\"country\", \"customer_type\")\n",
    "    )\n",
    "else:\n",
    "    print(\"Country or customer_type column is all NULL or zero; skipping new vs returning country analysis.\")\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_type\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"country\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"state_province\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"city\"):\n",
    "    analysis[\"new_vs_returning_customer_city\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\", \"state_province\", \"city\", \"customer_type\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .orderBy(\"country\", \"state_province\", \"city\", \"customer_type\")\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns (customer_type, country, state_province, city) is all NULL or zero; skipping new vs returning city analysis.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_type\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"country\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"state_province\"):\n",
    "    new_vs_returning_state = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"country\", \"state_province\", \"customer_type\")\n",
    "    .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "    .fillna({\"customer_count\": 0})\n",
    "    .orderBy(\"country\", \"state_province\", \"customer_type\")\n",
    ")\n",
    "else:\n",
    "    print(\"One of the required columns (customer_type, country, state_province) is all NULL or zero; skipping new vs returning state analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd0b73",
   "metadata": {},
   "source": [
    "Total & average engagement per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72d23fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_sessions\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_pages_viewed\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_products_viewed\"):\n",
    "    analysis[\"customer_engagement\"] = dataframes[\"agg_customers\"].select(\n",
    "        \"customer_id\",\n",
    "        \"total_sessions\",\n",
    "        \"total_pages_viewed\",\n",
    "        \"total_products_viewed\"\n",
    "    )\n",
    "\n",
    "    analysis[\"customer_engagement_summary\"] = dataframes[\"agg_customers\"].agg(\n",
    "        F.sum(\"total_sessions\").alias(\"total_sessions_all_customers\"),\n",
    "        F.avg(\"total_sessions\").alias(\"avg_sessions_per_customer\"),\n",
    "        F.sum(\"total_pages_viewed\").alias(\"total_pages_viewed_all_customers\"),\n",
    "        F.avg(\"total_pages_viewed\").alias(\"avg_pages_viewed_per_customer\"),\n",
    "        F.sum(\"total_products_viewed\").alias(\"total_products_viewed_all_customers\"),\n",
    "        F.avg(\"total_products_viewed\").alias(\"avg_products_viewed_per_customer\")\n",
    "    ).fillna({\"total_sessions_all_customers\": 0,\n",
    "            \"avg_sessions_per_customer\": 0,\n",
    "            \"total_pages_viewed_all_customers\": 0,\n",
    "            \"avg_pages_viewed_per_customer\": 0,\n",
    "            \"total_products_viewed_all_customers\": 0,\n",
    "            \"avg_products_viewed_per_customer\": 0\n",
    "            })\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping engagement analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ce05e",
   "metadata": {},
   "source": [
    "Session-to-order behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99d54c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"session_conversion_rate\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"cart_abandonment_rate\"):\n",
    "    analysis[\"session_to_order_analysis\"] = dataframes[\"agg_customers\"].agg(\n",
    "        F.avg(\"session_conversion_rate\").alias(\"avg_session_conversion_rate\"),\n",
    "        F.avg(\"cart_abandonment_rate\").alias(\"avg_cart_abandonment_rate\")\n",
    "    ).fillna({\n",
    "        \"avg_session_conversion_rate\": 0,\n",
    "        \"avg_cart_abandonment_rate\": 0\n",
    "    })\n",
    "else:\n",
    "    print(\"One of the required columns (session_conversion_rate, cart_abandonment_rate) is all NULL or zero; skipping session to order analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb43b4",
   "metadata": {},
   "source": [
    "Top customers based on revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6fbc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_segment\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"rfm_segment\"):\n",
    "    analysis[\"top_customers_by_revenue\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .fillna({\"total_revenue\": 0.0})\n",
    "        .select(\"customer_id\", \"total_revenue\", \"customer_lifetime_value\", \"customer_segment\", \"rfm_segment\")\n",
    "        .orderBy(F.col(\"total_revenue\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping top customers by revenue analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55e5c2",
   "metadata": {},
   "source": [
    "top customers based on Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7078dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_profit\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"net_profit\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_segment\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"rfm_segment\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\"):\n",
    "    customer_profit = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            F.sum(\"order_profit\").alias(\"total_order_profit\"),\n",
    "            F.sum(\"net_profit\").alias(\"total_net_profit\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    customer_profit_enriched = (\n",
    "        customer_profit.alias(\"p\")\n",
    "        .join(\n",
    "            dataframes[\"agg_customers\"].select(\"customer_id\", \"customer_segment\", \"rfm_segment\", \"total_revenue\", \"customer_lifetime_value\").alias(\"c\"),\n",
    "            on=\"customer_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    analysis[\"top_customers_by_profit\"] = (\n",
    "        customer_profit_enriched\n",
    "        .orderBy(F.col(\"total_net_profit\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"One of the required columns in agg_orders or agg_customers is all NULL or zero; skipping top customers by profit analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cb149",
   "metadata": {},
   "source": [
    "distribution percentage for conversion & abandonment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "886d9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"session_conversion_rate\"):\n",
    "    conv_percentage = dataframes[\"agg_customers\"].withColumn(\n",
    "        \"session_conversion_percentage\",\n",
    "        F.when(F.col(\"session_conversion_rate\") < 0.1, \"<10%\")\n",
    "        .when(F.col(\"session_conversion_rate\") < 0.25, \"10–25%\")\n",
    "        .when(F.col(\"session_conversion_rate\") < 0.5, \"25–50%\")\n",
    "        .when(F.col(\"session_conversion_rate\") < 0.75, \"50–75%\")\n",
    "        .otherwise(\"75%+\")\n",
    "    )\n",
    "    analysis[\"session_conversion_distribution\"] = (\n",
    "        conv_percentage\n",
    "        .groupBy(\"session_conversion_percentage\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "        .orderBy(\"session_conversion_percentage\")\n",
    "    )\n",
    "else:\n",
    "    print(\"session_conversion_rate column is all NULL or zero; skipping session conversion percentage calculation.\")\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"cart_abandonment_rate\"):\n",
    "    abandon_percentage = dataframes[\"agg_customers\"].withColumn(\n",
    "        \"cart_abandonment_percentage\",\n",
    "        F.when(F.col(\"cart_abandonment_rate\") < 0.1, \"<10%\")\n",
    "        .when(F.col(\"cart_abandonment_rate\") < 0.25, \"10–25%\")\n",
    "        .when(F.col(\"cart_abandonment_rate\") < 0.5, \"25–50%\")\n",
    "        .when(F.col(\"cart_abandonment_rate\") < 0.75, \"50–75%\")\n",
    "        .otherwise(\"75%+\")\n",
    "    )\n",
    "\n",
    "    analysis[\"cart_abandonment_distribution\"] = (\n",
    "        abandon_percentage\n",
    "        .groupBy(\"cart_abandonment_percentage\")\n",
    "        .agg(F.countDistinct(\"customer_id\").alias(\"customer_count\"))\n",
    "        .fillna({\"customer_count\": 0})\n",
    "        .orderBy(\"cart_abandonment_percentage\")\n",
    "    )\n",
    "else:\n",
    "    print(\"cart_abandonment_rate column is all NULL or zero; skipping cart abandonment percentage calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd2044",
   "metadata": {},
   "source": [
    "# Overall CLV summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c99cfc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"avg_order_value\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "):\n",
    "    analysis[\"clv_summary\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customers\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.expr(\n",
    "                \"percentile_approx(customer_lifetime_value, array(0.25, 0.5, 0.75))\"\n",
    "            ).alias(\"clv_percentiles\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_total_revenue\"),\n",
    "            F.avg(\"avg_order_value\").alias(\"avg_order_value_overall\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue_all_customers\"),\n",
    "        )\n",
    "        # only fill scalar columns; leave clv_percentiles as-is\n",
    "        .fillna({\n",
    "            \"customers\": 0,\n",
    "            \"avg_clv\": 0.0,\n",
    "            \"avg_total_revenue\": 0.0,\n",
    "            \"avg_order_value_overall\": 0.0,\n",
    "            \"total_revenue_all_customers\": 0.0,\n",
    "        })\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_customers is all NULL or zero; skipping CLV summary calculation.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2f1ed",
   "metadata": {},
   "source": [
    "Revenue based on customersegemtns and geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f25f5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revenue_by_segment(df, group_cols):\n",
    "    grouped = (\n",
    "        df.groupBy(*group_cols)\n",
    "          .agg(\n",
    "              F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "              F.sum(\"total_revenue\").alias(\"segment_revenue\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    total_rev = grouped.agg(F.sum(\"segment_revenue\").alias(\"total_revenue_all\")).first()[0] or 0.0\n",
    "\n",
    "    result = (\n",
    "        grouped\n",
    "        .withColumn(\n",
    "            \"revenue_per_customer\",\n",
    "            F.when(F.col(\"customer_count\") > 0,\n",
    "                   F.col(\"segment_revenue\") / F.col(\"customer_count\"))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"revenue_share\",\n",
    "            F.when(F.lit(total_rev) > 0,\n",
    "                   F.col(\"segment_revenue\") / F.lit(total_rev))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .orderBy(F.col(\"segment_revenue\").desc_nulls_last())\n",
    "    )\n",
    "    return result\n",
    "\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\"):\n",
    "    analysis[\"rev_by_country_city\"] = revenue_by_segment(dataframes[\"agg_customers\"], [\"country\", \"city\"])\n",
    "    analysis[\"rev_by_customer_segment\"] = revenue_by_segment(dataframes[\"agg_customers\"], [\"customer_segment\"])\n",
    "    analysis[\"rev_by_rfm_segment\"] = revenue_by_segment(dataframes[\"agg_customers\"], [\"rfm_segment\"])\n",
    "    analysis[\"rev_by_segment_label\"] = revenue_by_segment(dataframes[\"agg_customers\"], [\"customer_segment_label\"])\n",
    "    analysis[\"rev_by_referrer\"] = revenue_by_segment(dataframes[\"agg_customers\"], [\"preferred_referrer_source\"])\n",
    "    analysis[\"rev_by_device\"] = revenue_by_segment(dataframes[\"agg_customers\"], [\"preferred_device_type\"])\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping revenue by segment analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92ef24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------+---------------+--------------------+-------------+\n",
      "|country|             city|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+-------+-----------------+--------------+---------------+--------------------+-------------+\n",
      "|Ecuador|      Wintersport|             1|        9114.86|             9114.86|          1.0|\n",
      "|Namibia|   West Frankview|             1|            0.0|                 0.0|          0.0|\n",
      "|  Benin|Lake Samanthatown|             1|            0.0|                 0.0|          0.0|\n",
      "+-------+-----------------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis[\"rev_by_country_city\"].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98c819",
   "metadata": {},
   "source": [
    "Average order value trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "504867e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# 1) Daily AOV trend\n",
    "if (\n",
    "    \"agg_daily_aggregations\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_daily_aggregations\"], \"order_date\")\n",
    "):\n",
    "    daily = dataframes[\"agg_daily_aggregations\"].fillna(\n",
    "        {\n",
    "            \"avg_order_value\": 0.0,\n",
    "            \"total_orders\": 0,\n",
    "            \"total_revenue\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    analysis[\"aov_trend_daily\"] = (\n",
    "        daily.select(\n",
    "            \"order_date\",\n",
    "            \"order_year\",\n",
    "            \"order_month\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_order_value\",\n",
    "        )\n",
    "        .orderBy(F.col(\"order_date\").asc())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"agg_daily_aggregations not available, or order_date all NULL/zero; \"\n",
    "        \"skipping daily AOV trend.\"\n",
    "    )\n",
    "\n",
    "# 2) Weekly AOV trend\n",
    "if (\n",
    "    \"agg_weekly_aggregations\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_weekly_aggregations\"], \"year_week\")\n",
    "):\n",
    "    weekly = dataframes[\"agg_weekly_aggregations\"].fillna(\n",
    "        {\n",
    "            \"avg_order_value\": 0.0,\n",
    "            \"total_orders\": 0,\n",
    "            \"total_revenue\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    analysis[\"aov_trend_weekly\"] = (\n",
    "        weekly.select(\n",
    "            \"year_week\",\n",
    "            \"order_year\",\n",
    "            \"order_week\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_order_value\",\n",
    "        )\n",
    "        .orderBy(F.col(\"order_year\").asc(), F.col(\"order_week\").asc())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"agg_weekly_aggregations not available, or year_week all NULL/zero; \"\n",
    "        \"skipping weekly AOV trend.\"\n",
    "    )\n",
    "\n",
    "# 3) Monthly AOV trend\n",
    "if (\n",
    "    \"agg_monthly_aggregations\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_monthly_aggregations\"], \"year_month\")\n",
    "):\n",
    "    monthly = dataframes[\"agg_monthly_aggregations\"].fillna(\n",
    "        {\n",
    "            \"avg_order_value\": 0.0,\n",
    "            \"total_orders\": 0,\n",
    "            \"total_revenue\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    analysis[\"aov_trend_monthly\"] = (\n",
    "        monthly.select(\n",
    "            \"year_month\",\n",
    "            \"order_year\",\n",
    "            \"order_month\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_order_value\",\n",
    "        )\n",
    "        .orderBy(F.col(\"order_year\").asc(), F.col(\"order_month\").asc())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"agg_monthly_aggregations not available, or year_month all NULL/zero; \"\n",
    "        \"skipping monthly AOV trend.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00248505",
   "metadata": {},
   "source": [
    "Discount based analysis, customers contribution based on level of discount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "059b4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_discount_received\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"avg_discount_per_order\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_orders\"):\n",
    "    disc_df = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .fillna({\n",
    "            \"total_discount_received\": 0.0,\n",
    "            \"total_revenue\": 0.0,\n",
    "            \"avg_discount_per_order\": 0.0,\n",
    "            \"customer_lifetime_value\": 0.0,\n",
    "            \"total_orders\": 0\n",
    "        })\n",
    "        .withColumn(\n",
    "            \"discount_share_of_revenue\",\n",
    "            F.when(F.col(\"total_revenue\") > 0,\n",
    "                F.col(\"total_discount_received\") / F.col(\"total_revenue\"))\n",
    "            .otherwise(F.lit(0.0))\n",
    "        )\n",
    "    )\n",
    "    analysis[\"discount_customers\"] = (\n",
    "        disc_df\n",
    "        .withColumn(\n",
    "            \"is_discount_hunter\",\n",
    "            F.when(\n",
    "                (F.col(\"discount_share_of_revenue\") >= 0.3) &\n",
    "                (F.col(\"total_orders\") >= 3),\n",
    "                F.lit(1)\n",
    "            ).otherwise(F.lit(0))\n",
    "        )\n",
    "    )\n",
    "    analysis[\"discount_customers_summary\"] = (\n",
    "        analysis[\"discount_customers\"]\n",
    "        .groupBy(\"is_discount_hunter\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.avg(\"discount_share_of_revenue\").alias(\"avg_discount_share\"),\n",
    "            F.avg(\"avg_discount_per_order\").alias(\"avg_discount_per_order\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue\")\n",
    "        ).fillna({\n",
    "            \"customer_count\": 0,\n",
    "            \"avg_discount_share\": 0.0,\n",
    "            \"avg_discount_per_order\": 0.0,\n",
    "            \"avg_clv\": 0.0,\n",
    "            \"avg_revenue\": 0.0\n",
    "        })\n",
    "    )\n",
    "\n",
    "    analysis[\"correlation_discount_vs_clv\"] = disc_df.select(\n",
    "    F.corr(\"discount_share_of_revenue\", \"customer_lifetime_value\").alias(\"corr_discount_share_clv\"),\n",
    "    F.corr(\"avg_discount_per_order\", \"customer_lifetime_value\").alias(\"corr_avg_discount_clv\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"analysis skipped because one of the above columns is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319fe89f",
   "metadata": {},
   "source": [
    "Discount/CLV buckets to see patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb91fb7",
   "metadata": {},
   "source": [
    "Discount Behavior & Margin Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dc7a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_discount_received\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"avg_discount_per_order\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\"):\n",
    "    discount_behavior = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"total_revenue\",\n",
    "            \"total_discount_received\",\n",
    "            (F.col(\"total_discount_received\") / F.col(\"total_revenue\")).alias(\"discount_to_revenue_ratio\"),\n",
    "            \"avg_discount_per_order\",\n",
    "            \"customer_lifetime_value\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    analysis[\"high_discount_customers\"] = (\n",
    "        discount_behavior\n",
    "        .filter(F.col(\"discount_to_revenue_ratio\") > 0.3)  # threshold example\n",
    "        .orderBy(F.col(\"discount_to_revenue_ratio\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping discount behavior analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e63377",
   "metadata": {},
   "source": [
    " Cart & Checkout Health (Abandonment & Lost Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the required columns in agg_customers is all NULL or zero; skipping cart behavior analysis.\n"
     ]
    }
   ],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_carts_created\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_abandoned_carts\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_purchased_carts\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"cart_abandonment_rate\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_abandoned_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"avg_time_in_cart_days\"):\n",
    "    analysis[\"cart_behavior_summary\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .agg(\n",
    "            F.sum(\"total_carts_created\").alias(\"total_carts_created\"),\n",
    "            F.sum(\"total_abandoned_carts\").alias(\"total_abandoned_carts\"),\n",
    "            F.sum(\"total_purchased_carts\").alias(\"total_purchased_carts\"),\n",
    "            F.avg(\"cart_abandonment_rate\").alias(\"avg_cart_abandonment_rate\"),\n",
    "            F.sum(\"total_abandoned_value\").alias(\"total_abandoned_value\"),\n",
    "            F.avg(\"avg_time_in_cart_days\").alias(\"avg_time_in_cart_days\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    analysis[\"high_value_abandoners\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"total_abandoned_carts\",\n",
    "            \"total_abandoned_value\",\n",
    "            \"cart_abandonment_rate\",\n",
    "            \"total_revenue\",\n",
    "            \"customer_lifetime_value\"\n",
    "        )\n",
    "        .filter(F.col(\"total_abandoned_value\") > 0)\n",
    "        .orderBy(F.col(\"total_abandoned_value\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping cart behavior analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffbfef",
   "metadata": {},
   "source": [
    "Churn Risk Distribution (Portfolio Health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa0c692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"churn_risk\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"order_recency_days\"):\n",
    "    analysis[\"churn_risk_summary\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"churn_risk\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"num_customers\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.avg(\"order_recency_days\").alias(\"avg_recency_days\")\n",
    "        )\n",
    "        .orderBy(\"churn_risk\")\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping churn risk analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563deb4",
   "metadata": {},
   "source": [
    "High‑CLV Customers at Risk of Churn (Immediate Action List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f353781",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"churn_risk\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_activity_score\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"order_recency_days\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"days_since_last_purchase\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"days_since_last_login\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_segment_label\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"rfm_segment\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"rfm_category\"):\n",
    "    analysis[\"high_clv_at_risk\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"customer_lifetime_value\",\n",
    "            \"churn_risk\",\n",
    "            \"customer_activity_score\",\n",
    "            \"order_recency_days\",\n",
    "            \"days_since_last_purchase\",\n",
    "            \"days_since_last_login\",\n",
    "            \"customer_segment_label\",\n",
    "            \"rfm_segment\",\n",
    "            \"rfm_category\"\n",
    "        )\n",
    "        .filter(\n",
    "            (F.col(\"churn_risk\").isin(\"medium\", \"high\")) &\n",
    "            (F.col(\"customer_lifetime_value\") > 0)\n",
    "        )\n",
    "        .orderBy(F.col(\"customer_lifetime_value\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping high CLV at risk analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66108539",
   "metadata": {},
   "source": [
    "RFM Segment Summary (Champions, At Risk, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7931d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"rfm_segment\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"order_recency_days\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_orders\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"avg_order_value\"):\n",
    "    analysis[\"rfm_segment_summary\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"rfm_segment\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"num_customers\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.avg(\"order_recency_days\").alias(\"avg_recency_days\"),\n",
    "            F.avg(\"total_orders\").alias(\"avg_total_orders\"),\n",
    "            F.avg(\"avg_order_value\").alias(\"avg_aov\")\n",
    "        )\n",
    "        .orderBy(F.col(\"total_revenue\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping RFM segment summary analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccffdaf",
   "metadata": {},
   "source": [
    "High‑Intent Non‑Buyers (Fix Funnel / UX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43c4472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\") and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\"):\n",
    "    analysis[\"high_intent_non_buyers\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"total_products_viewed\",\n",
    "        \"wishlist_items_count\",\n",
    "        \"total_carts_created\",\n",
    "        \"total_purchased_carts\",\n",
    "        \"cart_abandonment_rate\",\n",
    "        \"session_conversion_rate\"\n",
    "    )\n",
    "    .filter(\n",
    "        (F.col(\"total_revenue\") == 0) &\n",
    "        (\n",
    "            (F.col(\"total_products_viewed\") > 10) |\n",
    "            (F.col(\"wishlist_items_count\") > 5) |\n",
    "            (F.col(\"total_carts_created\") > 3)\n",
    "        )\n",
    "    )\n",
    "    .orderBy(F.col(\"total_products_viewed\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_customers is all NULL or zero; skipping high intent non-buyers analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0d03c",
   "metadata": {},
   "source": [
    "# OVERALL CUSTOMER ANALYSIS SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565111d8",
   "metadata": {},
   "source": [
    "Signup cohort: retention by month (based on first order month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2b61ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"account_created_at\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"first_order_date\")\n",
    "):\n",
    "    customers_with_first_order = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            F.to_date(\"account_created_at\").alias(\"signup_date\"),\n",
    "            F.to_date(\"first_order_date\").alias(\"first_order_date\")\n",
    "        )\n",
    "        .filter(F.col(\"first_order_date\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Define signup cohort month and first-order month\n",
    "    analysis[\"customers_cohorts\"] = (\n",
    "        customers_with_first_order\n",
    "        .withColumn(\n",
    "            \"signup_cohort_month\",\n",
    "            F.date_trunc(\"month\", F.col(\"signup_date\")).cast(\"date\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"first_order_month\",\n",
    "            F.date_trunc(\"month\", F.col(\"first_order_date\")).cast(\"date\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    analysis[\"signup_cohort_summary\"] = (\n",
    "        analysis[\"customers_cohorts\"]\n",
    "        .groupBy(\"signup_cohort_month\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"cohort_customers\")\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"account_created_at or first_order_date is all NULL/zero; skipping signup cohort prep.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7e2bdf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+-------------------+-----------------+\n",
      "|customer_id|signup_date|first_order_date|signup_cohort_month|first_order_month|\n",
      "+-----------+-----------+----------------+-------------------+-----------------+\n",
      "|       1000| 2022-10-17|      2024-10-22|         2022-10-01|       2024-10-01|\n",
      "+-----------+-----------+----------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis[\"customers_cohorts\"].show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55498a85",
   "metadata": {},
   "source": [
    "Order‑based monthly cohorts and retention curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3e55a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_placed_at\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"customer_id\")\n",
    "    and analysis[\"customers_cohorts\"] is not None\n",
    "):\n",
    "    # Orders with order month\n",
    "    orders_with_month = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            F.date_trunc(\"month\", \"order_placed_at\").cast(\"date\").alias(\"order_month\")\n",
    "        )\n",
    "        .filter(F.col(\"order_month\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Join customers to their signup cohort\n",
    "    orders_with_cohort = (\n",
    "        orders_with_month\n",
    "        .join(\n",
    "            analysis[\"customers_cohorts\"].select(\"customer_id\", \"signup_cohort_month\"),\n",
    "            on=\"customer_id\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate months_since_cohort_start: 0 = cohort month, 1 = next month, etc.\n",
    "    orders_with_relative_month = (\n",
    "        orders_with_cohort\n",
    "        .withColumn(\n",
    "            \"months_since_signup\",\n",
    "            (\n",
    "                (F.month(\"order_month\") - F.month(\"signup_cohort_month\"))\n",
    "                + 12 * (F.year(\"order_month\") - F.year(\"signup_cohort_month\"))\n",
    "            ).cast(\"int\")\n",
    "        )\n",
    "        .filter(F.col(\"months_since_signup\") >= 0)\n",
    "    )\n",
    "\n",
    "    # Retention: did customer order in each relative month\n",
    "    analysis[\"customer_cohort_retention\"] = (\n",
    "        orders_with_relative_month\n",
    "        .select(\"signup_cohort_month\", \"customer_id\", \"months_since_signup\")\n",
    "        .dropDuplicates([\"signup_cohort_month\", \"customer_id\", \"months_since_signup\"])\n",
    "        .groupBy(\"signup_cohort_month\", \"months_since_signup\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"active_customers\")\n",
    "        )\n",
    "        .join(\n",
    "            analysis[\"signup_cohort_summary\"],\n",
    "            on=\"signup_cohort_month\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"retention_rate\",\n",
    "            F.when(\n",
    "                F.col(\"cohort_customers\") > 0,\n",
    "                F.col(\"active_customers\") / F.col(\"cohort_customers\")\n",
    "            ).otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .orderBy(\"signup_cohort_month\", \"months_since_signup\")\n",
    "    )\n",
    "else:\n",
    "    print(\"Required columns for cohort retention are missing; skipping monthly cohort retention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917e6da",
   "metadata": {},
   "source": [
    "Customer 360 / health table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dec4e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols_360 = [\n",
    "    \"customer_id\",\n",
    "    \"account_created_at\",\n",
    "    \"account_status\",\n",
    "    \"is_active\",\n",
    "    \"is_repeat_customer\",\n",
    "    \"total_orders\",\n",
    "    \"total_items_purchased\",\n",
    "    \"total_cancelled_orders\",\n",
    "    \"total_reviews_written\",\n",
    "    \"total_sessions\",\n",
    "    \"total_pages_viewed\",\n",
    "    \"total_products_viewed\",\n",
    "    \"wishlist_items_count\",\n",
    "    \"total_carts_created\",\n",
    "    \"total_abandoned_carts\",\n",
    "    \"total_purchased_carts\",\n",
    "    \"order_frequency\",\n",
    "    \"gender\",\n",
    "    \"customer_age_group\",\n",
    "    \"city\",\n",
    "    \"state_province\",\n",
    "    \"country\",\n",
    "    \"order_recency_days\",\n",
    "    \"customer_tenure_days\",\n",
    "    \"days_since_last_login\",\n",
    "    \"customer_activity_status\",\n",
    "    \"customer_segment\",\n",
    "    \"customer_segment_label\",\n",
    "    \"rfm_segment\",\n",
    "    \"rfm_category\",\n",
    "    \"churn_risk\",\n",
    "    \"customer_lifetime_value\",\n",
    "    \"total_revenue\",\n",
    "    \"avg_order_value\",\n",
    "    \"avg_items_per_order\",\n",
    "    \"total_discount_received\",\n",
    "    \"avg_discount_per_order\",\n",
    "    \"avg_session_duration\",\n",
    "    \"session_conversion_rate\",\n",
    "    \"cart_abandonment_rate\",\n",
    "    \"preferred_device_type\",\n",
    "    \"preferred_referrer_source\",\n",
    "    \"preferred_payment_method\",\n",
    "    \"wishlist_conversion_rate\",\n",
    "    \"days_since_last_purchase\",\n",
    "    \"cancellation_rate\",\n",
    "    \"customer_activity_score\",\n",
    "    \"total_abandoned_value\",\n",
    "    \"avg_time_in_cart_days\",\n",
    "    \"customer_abandonment_rate\",\n",
    "    \"customer_purchase_rate\",\n",
    "]\n",
    "\n",
    "if \"agg_customers\" in dataframes:\n",
    "    missing_for_360 = [c for c in required_cols_360 if c not in dataframes[\"agg_customers\"].columns]\n",
    "    if missing_for_360:\n",
    "        print(f\"Some 360 columns are missing in agg_customers: {missing_for_360}\")\n",
    "    \n",
    "    available_cols_360 = [c for c in required_cols_360 if c in dataframes[\"agg_customers\"].columns]\n",
    "    \n",
    "    analysis[\"customer_overall_health_summary\"] = dataframes[\"agg_customers\"].select(*available_cols_360)\n",
    "else:\n",
    "    print(\"agg_customers dataframe not found; skipping customer 360 table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625fd2e",
   "metadata": {},
   "source": [
    "Systematic cross‑tabs of segmentation fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938801c8",
   "metadata": {},
   "source": [
    " RFM segment × churn_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "85cb0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tab: RFM segment x churn_risk\n",
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"rfm_segment\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"churn_risk\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\")\n",
    "):\n",
    "    analysis[\"rfm_churn_crosstab\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"rfm_segment\", \"churn_risk\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.sum(\"total_revenue\").alias(\"segment_revenue\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\")\n",
    "        )\n",
    "        .orderBy(\"rfm_segment\", \"churn_risk\")\n",
    "    )\n",
    "else:\n",
    "    print(\"One of (rfm_segment, churn_risk, customer_id, total_revenue, customer_lifetime_value) is all NULL/zero; skipping rfm x churn analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c4213132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tab: customer_segment_label x preferred_referrer_source\n",
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_segment_label\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"preferred_referrer_source\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\")\n",
    "):\n",
    "    analysis[\"seg_referrer_crosstab\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"customer_segment_label\", \"preferred_referrer_source\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.sum(\"total_revenue\").alias(\"segment_revenue\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue_per_customer\")\n",
    "        )\n",
    "        .orderBy(\"customer_segment_label\", F.col(\"segment_revenue\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns is all NULL/zero; skipping segment x referrer analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2757fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tab: customer_segment_label x preferred_device_type\n",
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_segment_label\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"preferred_device_type\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\")\n",
    "):\n",
    "    analysis[\"seg_device_crosstab\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"customer_segment_label\", \"preferred_device_type\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.sum(\"total_revenue\").alias(\"segment_revenue\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue_per_customer\")\n",
    "        )\n",
    "        .orderBy(\"customer_segment_label\", F.col(\"segment_revenue\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns is all NULL/zero; skipping segment x device analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a8088",
   "metadata": {},
   "source": [
    "Payment method vs CLV / churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06840187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of (preferred_payment_method, CLV, churn_risk, total_revenue) is all NULL/zero; skipping payment vs CLV/churn analysis.\n"
     ]
    }
   ],
   "source": [
    "# Payment method vs CLV and churn\n",
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"preferred_payment_method\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"churn_risk\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\")\n",
    "):\n",
    "    analysis[\"payment_method_vs_clv_churn\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"preferred_payment_method\", \"churn_risk\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue_per_customer\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .orderBy(\"preferred_payment_method\", \"churn_risk\")\n",
    "    )\n",
    "\n",
    "    # Also a simpler view aggregated only by payment method\n",
    "    analysis[\"payment_method_summary\"] = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .groupBy(\"preferred_payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue_per_customer\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .orderBy(F.col(\"total_revenue\").desc())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of (preferred_payment_method, CLV, churn_risk, total_revenue) is all NULL/zero; skipping payment vs CLV/churn analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e6aa7",
   "metadata": {},
   "source": [
    " Channel (referrer) vs CLV / churn / discount reliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "27c67ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel (referrer) vs CLV, churn, and discount reliance\n",
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"preferred_referrer_source\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"churn_risk\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"total_discount_received\")\n",
    "):\n",
    "    referrer_df = (\n",
    "        dataframes[\"agg_customers\"]\n",
    "        .withColumn(\n",
    "            \"discount_share_of_revenue\",\n",
    "            F.when(F.col(\"total_revenue\") > 0,\n",
    "                   F.col(\"total_discount_received\") / F.col(\"total_revenue\"))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    analysis[\"referrer_source_summary\"] = (\n",
    "        referrer_df\n",
    "        .groupBy(\"preferred_referrer_source\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue_per_customer\"),\n",
    "            F.avg(\"discount_share_of_revenue\").alias(\"avg_discount_share\")\n",
    "        ).fillna({\n",
    "            \"customer_count\": 0,\n",
    "            \"avg_clv\": 0.0,\n",
    "            \"total_revenue\": 0.0,\n",
    "            \"avg_revenue_per_customer\": 0.0,\n",
    "            \"avg_discount_share\": 0.0\n",
    "        })\n",
    "        .orderBy(F.col(\"total_revenue\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    analysis[\"referrer_churn_summary\"] = (\n",
    "        referrer_df\n",
    "        .groupBy(\"preferred_referrer_source\", \"churn_risk\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "            F.avg(\"discount_share_of_revenue\").alias(\"avg_discount_share\")\n",
    "        ).fillna({\n",
    "            \"customer_count\": 0,\n",
    "            \"avg_clv\": 0.0,\n",
    "            \"avg_discount_share\": 0.0\n",
    "        })\n",
    "        .orderBy(\"preferred_referrer_source\", \"churn_risk\")\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns for referrer vs CLV/churn/discount is all NULL/zero; skipping analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3eea2",
   "metadata": {},
   "source": [
    "Profit‑per‑customer derived from agg_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f0089a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit per customer from agg_orders\n",
    "\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"customer_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_profit\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"net_profit\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "):\n",
    "    customer_profit = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count_profit\"),\n",
    "            F.sum(\"order_profit\").alias(\"total_order_profit\"),\n",
    "            F.avg(\"order_profit\").alias(\"avg_profit_per_order\"),\n",
    "            F.sum(\"net_profit\").alias(\"total_net_profit\"),\n",
    "            F.avg(\"net_profit\").alias(\"avg_net_profit_per_order\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join with agg_customers to see CLV vs profit\n",
    "    if \"agg_customers\" in dataframes:\n",
    "        customers_with_profit = (\n",
    "            dataframes[\"agg_customers\"]\n",
    "            .join(customer_profit, on=\"customer_id\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        # Example aggregated view: CLV vs profit per customer segment\n",
    "        if (\n",
    "            \"customer_segment_label\" in customers_with_profit.columns\n",
    "            and \"total_revenue\" in customers_with_profit.columns\n",
    "            and \"customer_lifetime_value\" in customers_with_profit.columns\n",
    "        ):\n",
    "            analysis[\"customer_profit_per_segment\"] = (\n",
    "                customers_with_profit\n",
    "                .groupBy(\"customer_segment_label\")\n",
    "                .agg(\n",
    "                    F.countDistinct(\"customer_id\").alias(\"customer_count\"),\n",
    "                    F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "                    F.sum(\"total_order_profit\").alias(\"total_order_profit\"),\n",
    "                    F.sum(\"total_net_profit\").alias(\"total_net_profit\"),\n",
    "                    F.avg(\"customer_lifetime_value\").alias(\"avg_clv\"),\n",
    "                    F.avg(\"total_order_profit\").alias(\"avg_profit_per_customer\")\n",
    "                )\n",
    "                .orderBy(F.col(\"total_order_profit\").desc_nulls_last())\n",
    "            )\n",
    "    else:\n",
    "        print(\"agg_customers not found; created customer_profit only.\")\n",
    "else:\n",
    "    print(\"One of the required columns in agg_orders is all NULL/zero; skipping customer profit analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10e8c3",
   "metadata": {},
   "source": [
    "Segment-Specific AOV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de7ad0",
   "metadata": {},
   "source": [
    "Segment AOV using agg_rfm_segmentation + agg_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2b3a19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "if (\n",
    "    \"agg_rfm_segmentation\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_rfm_segmentation\"], \"customer_id\")\n",
    "    and \"agg_orders\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "):\n",
    "    rfm = dataframes[\"agg_rfm_segmentation\"].select(\n",
    "        \"customer_id\",\n",
    "        \"rfm_segment\",          # high_value, at_risk, etc.\n",
    "    )\n",
    "\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"total_amount\"\n",
    "    ).fillna({\"total_amount\": 0.0})\n",
    "\n",
    "    seg_orders = (\n",
    "        orders.alias(\"o\")\n",
    "        .join(rfm.alias(\"r\"), on=\"customer_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Example: AOV by overall RFM segment\n",
    "    analysis[\"segment_aov_by_rfm\"] = (\n",
    "        seg_orders.groupBy(\"rfm_segment\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"order_id\").alias(\"orders\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_order_value_segment\"),\n",
    "            F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"avg_order_value_segment\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # Example: AOV by monetary_segment\n",
    "   \n",
    "else:\n",
    "    print(\n",
    "        \"Either agg_rfm_segmentation or agg_orders missing/invalid; \"\n",
    "        \"cannot compute segment-specific AOV with this option.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38f8c9",
   "metadata": {},
   "source": [
    "# Product-level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7215e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_analysis = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6caf6",
   "metadata": {},
   "source": [
    "Overall Best Selling Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ace9a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_orders\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "    product_analysis[\"best_selling_products\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"total_units_sold\").desc_nulls_last(),\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping best selling products analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d57d1e",
   "metadata": {},
   "source": [
    "product level seasonal trends analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9a22b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (\n",
    "    not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_placed_at\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"order_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"quantity\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "):\n",
    "    # Base joined fact\n",
    "    order_product = (\n",
    "        dataframes[\"agg_order_items\"]\n",
    "        .join(\n",
    "            dataframes[\"agg_orders\"]\n",
    "            .select(\"order_id\", \"order_placed_at\"),\n",
    "            on=\"order_id\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(\n",
    "            dataframes[\"agg_products\"]\n",
    "            .select(\"product_id\", \"product_name\", \"category\", \"sub_category\", \"brand\"),\n",
    "            on=\"product_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\"order_date\", F.to_date(\"order_placed_at\"))\n",
    "    )\n",
    "\n",
    "    # Monthly grain: year + month\n",
    "    order_product_monthly = add_time_grain(\n",
    "        order_product, date_col=\"order_date\", grain=\"month\"\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_monthly_trends\"] = (\n",
    "        order_product_monthly\n",
    "        .groupBy(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"grain_year\",\n",
    "            \"grain_month\",\n",
    "        )\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"units_sold\": 0,\n",
    "                \"orders_count\": 0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"grain_year\", \"grain_month\")\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_orders, agg_order_items, or agg_products is all NULL or zero; \"\n",
    "        \"skipping product monthly seasonal trends.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb391e",
   "metadata": {},
   "source": [
    "Category-level monthly seasonal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3472f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"order_product\" in locals():\n",
    "    order_product_monthly_cat = add_time_grain(\n",
    "        order_product, date_col=\"order_date\", grain=\"month\"\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_monthly_trends\"] = (\n",
    "        order_product_monthly_cat\n",
    "        .groupBy(\"category\", \"grain_year\", \"grain_month\")\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"units_sold\": 0,\n",
    "                \"orders_count\": 0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\"category\", \"grain_year\", \"grain_month\")\n",
    "    )\n",
    "else:\n",
    "    print(\"order_product dataset not available; skipping category monthly trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607fa91",
   "metadata": {},
   "source": [
    " “Classical” calendar-month seasonality (across years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2c0ac830",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"order_product\" in locals():\n",
    "    product_analysis[\"product_calendar_month_seasonality\"] = (\n",
    "        order_product\n",
    "        .withColumn(\"calendar_month\", F.month(\"order_date\"))\n",
    "        .groupBy(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"calendar_month\",\n",
    "        )\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"units_sold\": 0,\n",
    "                \"orders_count\": 0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"calendar_month\")\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_calendar_month_seasonality\"] = (\n",
    "        order_product\n",
    "        .withColumn(\"calendar_month\", F.month(\"order_date\"))\n",
    "        .groupBy(\"category\", \"calendar_month\")\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"units_sold\": 0,\n",
    "                \"orders_count\": 0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\"category\", \"calendar_month\")\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"order_product dataset not available; skipping calendar-month seasonality analyses.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6749c3f",
   "metadata": {},
   "source": [
    "Products with highest profit margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a6c2b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the required columns in agg_products is all NULL or zero; skipping highest margin products analysis.\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"profit_margin\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "):\n",
    "    product_analysis[\"highest_margin_products\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"profit_margin\",\n",
    "            \"total_revenue\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"profit_margin\": 0.0,\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"profit_margin\").desc_nulls_last(),\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping highest margin products analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ee97a",
   "metadata": {},
   "source": [
    "Products with low profit margin but high traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "56c8c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the required columns in agg_products is all NULL or zero; skipping low-margin high-traffic products analysis.\n"
     ]
    }
   ],
   "source": [
    "# Products with low margin but high traffic / interest\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"profit_margin\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_orders\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"view_to_purchase_rate\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"revenue_per_view\")\n",
    "):\n",
    "    product_analysis[\"low_margin_high_traffic_products\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"profit_margin\",\n",
    "            \"total_revenue\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"view_to_purchase_rate\",\n",
    "            \"revenue_per_view\",\n",
    "            \"total_wishlist_adds\",\n",
    "            \"total_cart_adds\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"profit_margin\": 0.0,\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"view_to_purchase_rate\": 0.0,\n",
    "                \"revenue_per_view\": 0.0,\n",
    "                \"total_wishlist_adds\": 0,\n",
    "                \"total_cart_adds\": 0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional: derive a simple \"traffic_score\" to help rank\n",
    "    product_analysis[\"low_margin_high_traffic_products\"] = (\n",
    "        product_analysis[\"low_margin_high_traffic_products\"]\n",
    "        .withColumn(\n",
    "            \"traffic_score\",\n",
    "            (\n",
    "                F.col(\"total_units_sold\")\n",
    "                + F.col(\"total_orders\")\n",
    "                + F.col(\"total_wishlist_adds\")\n",
    "                + F.col(\"total_cart_adds\")\n",
    "            )\n",
    "        )\n",
    "        # Focus on *low* margin but *high* traffic\n",
    "        # You can tune thresholds; here we don't hard-filter, we just sort to surface likely candidates\n",
    "        .orderBy(\n",
    "            F.col(\"profit_margin\").asc_nulls_last(),      # lowest margin first\n",
    "            F.col(\"traffic_score\").desc_nulls_last(),     # highest traffic next\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),     # then revenue\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping low-margin high-traffic products analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42071d",
   "metadata": {},
   "source": [
    "Out-of-stock products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01d45265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-stock products (product-centric view)\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and (\"current_stock_level\" in dataframes[\"agg_products\"].columns\n",
    "         or \"current_stock\" in dataframes[\"agg_products\"].columns)\n",
    "):\n",
    "    products_df = dataframes[\"agg_products\"]\n",
    "\n",
    "    # Prefer current_stock_level if present, else fall back to current_stock\n",
    "    stock_col = \"current_stock_level\" if \"current_stock_level\" in products_df.columns else \"current_stock\"\n",
    "\n",
    "    product_analysis[\"out_of_stock_products\"] = (\n",
    "        products_df\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            F.col(stock_col).alias(\"current_stock_level\"),\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"current_stock_level\": 0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .filter(F.col(\"current_stock_level\") <= 0)\n",
    "        .orderBy(\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "            F.col(\"total_units_sold\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero (product_id, product_name, category, stock); \"\n",
    "        \"skipping product out-of-stock analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85d0a4",
   "metadata": {},
   "source": [
    "Products with low sell-through rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "aa3367c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the required columns in agg_products is all NULL or zero; skipping low view/cart/wishlist to purchase rate analysis.\n"
     ]
    }
   ],
   "source": [
    "# Products with low view-to-purchase / cart-to-purchase / wishlist-to-purchase rates\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"view_to_purchase_rate\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"cart_to_purchase_rate\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"wishlist_to_purchase_rate\")\n",
    "):\n",
    "    product_analysis[\"low_conversion_products\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"view_to_purchase_rate\",\n",
    "            \"cart_to_purchase_rate\",\n",
    "            \"wishlist_to_purchase_rate\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_wishlist_adds\",\n",
    "            \"total_cart_adds\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"view_to_purchase_rate\": 0.0,\n",
    "                \"cart_to_purchase_rate\": 0.0,\n",
    "                \"wishlist_to_purchase_rate\": 0.0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_wishlist_adds\": 0,\n",
    "                \"total_cart_adds\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"view_to_purchase_rate\").asc_nulls_last(),\n",
    "            F.col(\"cart_to_purchase_rate\").asc_nulls_last(),\n",
    "            F.col(\"wishlist_to_purchase_rate\").asc_nulls_last(),\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping low view/cart/wishlist to purchase rate analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f2543",
   "metadata": {},
   "source": [
    "product rating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "590a9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    \"agg_reviews\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"rating\")\n",
    "):\n",
    "    product_analysis[\"product_rating_summary\"] = (\n",
    "        dataframes[\"agg_reviews\"]\n",
    "        .groupBy(\"product_id\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating\")\n",
    "        )\n",
    "        .fillna({\"total_reviews\": 0, \"avg_rating\": 0.0})\n",
    "        .orderBy(F.col(\"avg_rating\").desc_nulls_last())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93637b44",
   "metadata": {},
   "source": [
    "Product/category viewing patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09318b1",
   "metadata": {},
   "source": [
    "Category-level viewing effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6946ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_orders\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"view_to_purchase_rate\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"revenue_per_view\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\"):\n",
    "    product_analysis[\"category_view_patterns\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"product_id\").alias(\"products_in_category\"),\n",
    "            F.sum(\"total_units_sold\").alias(\"total_units_sold\"),\n",
    "            F.sum(\"total_orders\").alias(\"total_orders\"),\n",
    "            F.avg(\"view_to_purchase_rate\").alias(\"avg_view_to_purchase_rate\"),\n",
    "            F.avg(\"revenue_per_view\").alias(\"avg_revenue_per_view\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\")\n",
    "        ).fillna({\n",
    "            \"products_in_category\": 0,\n",
    "            \"total_units_sold\": 0,\n",
    "            \"total_orders\": 0,\n",
    "            \"avg_view_to_purchase_rate\": 0.0,\n",
    "            \"avg_revenue_per_view\": 0.0,\n",
    "            \"total_revenue\": 0.0\n",
    "        })\n",
    "        .orderBy(F.col(\"total_revenue\").desc_nulls_last())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_products is all NULL or zero; skipping category view patterns analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f981f7",
   "metadata": {},
   "source": [
    "Product-level Top-View-to-Purchase Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2ea71c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"view_to_purchase_rate\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"revenue_per_view\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\") and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_orders\"):\n",
    "    product_analysis[\"top_view_to_purchase_products\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"view_to_purchase_rate\",\n",
    "            \"revenue_per_view\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\"\n",
    "        ).fillna({\n",
    "            \"view_to_purchase_rate\": 0.0,\n",
    "            \"revenue_per_view\": 0.0,\n",
    "            \"total_units_sold\": 0,\n",
    "            \"total_orders\": 0\n",
    "        })\n",
    "        .orderBy(F.col(\"view_to_purchase_rate\").desc_nulls_last())\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_products is all NULL or zero; skipping top view to purchase products analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad752c4",
   "metadata": {},
   "source": [
    "Product Performance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "db740dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "\n",
    "    # Base product metrics\n",
    "    products_base_perf = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"view_to_purchase_rate\",\n",
    "            \"product_performance_score\"  # keep existing if present\n",
    "        )\n",
    "        .fillna({\n",
    "            \"total_units_sold\": 0,\n",
    "            \"total_orders\": 0,\n",
    "            \"total_revenue\": 0.0,\n",
    "            \"view_to_purchase_rate\": 0.0,\n",
    "            \"product_performance_score\": 0.0\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Join ratings if we have them\n",
    "    if product_analysis[\"product_rating_summary\"] is not None:\n",
    "        products_perf = (\n",
    "            products_base_perf\n",
    "            .join(\n",
    "                product_analysis[\"product_rating_summary\"],\n",
    "                on=\"product_id\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "            .fillna({\n",
    "                \"total_reviews\": 0,\n",
    "                \"avg_rating\": 0.0\n",
    "            })\n",
    "        )\n",
    "    else:\n",
    "        products_perf = (\n",
    "            products_base_perf\n",
    "            .withColumn(\"total_reviews\", F.lit(0))\n",
    "            .withColumn(\"avg_rating\", F.lit(0.0))\n",
    "        )\n",
    "\n",
    "    # 2) Compute max values for normalization\n",
    "    max_vals = products_perf.agg(\n",
    "        F.max(\"total_revenue\").alias(\"max_revenue\"),\n",
    "        F.max(\"total_units_sold\").alias(\"max_units\"),\n",
    "        F.max(\"avg_rating\").alias(\"max_rating\"),\n",
    "        F.max(\"view_to_purchase_rate\").alias(\"max_view_to_purchase\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    max_revenue = max_vals[\"max_revenue\"] or 0.0\n",
    "    max_units = max_vals[\"max_units\"] or 0\n",
    "    max_rating = max_vals[\"max_rating\"] or 0.0\n",
    "    max_view_to_purchase = max_vals[\"max_view_to_purchase\"] or 0.0\n",
    "\n",
    "    # Avoid division by zero by using F.lit(...) and checks\n",
    "    products_perf_scored = (\n",
    "        products_perf\n",
    "        .withColumn(\n",
    "            \"revenue_score\",\n",
    "            F.when(F.lit(max_revenue) > 0,\n",
    "                   F.col(\"total_revenue\") / F.lit(max_revenue))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"volume_score\",\n",
    "            F.when(F.lit(max_units) > 0,\n",
    "                   F.col(\"total_units_sold\") / F.lit(max_units))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"rating_score\",\n",
    "            F.when(F.lit(max_rating) > 0,\n",
    "                   F.col(\"avg_rating\") / F.lit(max_rating))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"view_to_purchase_score\",\n",
    "            F.when(F.lit(max_view_to_purchase) > 0,\n",
    "                   F.col(\"view_to_purchase_rate\") / F.lit(max_view_to_purchase))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        # 3) Weighted composite score (tweak weights as desired)\n",
    "        .withColumn(\n",
    "            \"product_performance_score_computed\",\n",
    "            0.4 * F.col(\"revenue_score\")\n",
    "            + 0.3 * F.col(\"volume_score\")\n",
    "            + 0.2 * F.col(\"rating_score\")\n",
    "            + 0.1 * F.col(\"view_to_purchase_score\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_performance_score\"] = (\n",
    "        products_perf_scored\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_rating\",\n",
    "            \"view_to_purchase_rate\",\n",
    "            \"revenue_score\",\n",
    "            \"volume_score\",\n",
    "            \"rating_score\",\n",
    "            \"view_to_purchase_score\",\n",
    "            \"product_performance_score_computed\"\n",
    "        )\n",
    "        .fillna({\n",
    "            \"revenue_score\": 0.0,\n",
    "            \"volume_score\": 0.0,\n",
    "            \"rating_score\": 0.0,\n",
    "            \"view_to_purchase_score\": 0.0,\n",
    "            \"product_performance_score_computed\": 0.0\n",
    "        })\n",
    "        .orderBy(\n",
    "            F.col(\"product_performance_score_computed\").desc_nulls_last()\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping product performance score analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f3b87",
   "metadata": {},
   "source": [
    "Category revenue share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1dfb13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category revenue share\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "    # Aggregate revenue per category\n",
    "    category_rev_df = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "            F.countDistinct(\"product_id\").alias(\"products_in_category\"),\n",
    "            F.sum(\"total_units_sold\").alias(\"total_units_sold\")\n",
    "        )\n",
    "        .fillna({\n",
    "            \"category_revenue\": 0.0,\n",
    "            \"products_in_category\": 0,\n",
    "            \"total_units_sold\": 0\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Compute total revenue across all categories\n",
    "    total_rev_all = (\n",
    "        category_rev_df\n",
    "        .agg(F.sum(\"category_revenue\").alias(\"total_revenue_all\"))\n",
    "        .first()[0] or 0.0\n",
    "    )\n",
    "\n",
    "    # Add revenue share and revenue per product\n",
    "    product_analysis[\"category_revenue_share\"] = (\n",
    "        category_rev_df\n",
    "        .withColumn(\n",
    "            \"revenue_per_product\",\n",
    "            F.when(F.col(\"products_in_category\") > 0,\n",
    "                   F.col(\"category_revenue\") / F.col(\"products_in_category\"))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"revenue_share\",\n",
    "            F.when(F.lit(total_rev_all) > 0,\n",
    "                   F.col(\"category_revenue\") / F.lit(total_rev_all))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .orderBy(F.col(\"category_revenue\").desc_nulls_last())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping category revenue share analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0927d2",
   "metadata": {},
   "source": [
    "Low-performing categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "14417545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-performing categories\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_orders\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"view_to_purchase_rate\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"revenue_per_view\")\n",
    "):\n",
    "    category_perf = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"product_id\").alias(\"products_in_category\"),\n",
    "            F.sum(\"total_units_sold\").alias(\"total_units_sold\"),\n",
    "            F.sum(\"total_orders\").alias(\"total_orders\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "            F.avg(\"view_to_purchase_rate\").alias(\"avg_view_to_purchase_rate\"),\n",
    "            F.avg(\"revenue_per_view\").alias(\"avg_revenue_per_view\"),\n",
    "            F.avg(\"profit_margin\").alias(\"avg_profit_margin\")\n",
    "        )\n",
    "        .fillna({\n",
    "            \"products_in_category\": 0,\n",
    "            \"total_units_sold\": 0,\n",
    "            \"total_orders\": 0,\n",
    "            \"total_revenue\": 0.0,\n",
    "            \"avg_view_to_purchase_rate\": 0.0,\n",
    "            \"avg_revenue_per_view\": 0.0,\n",
    "            \"avg_profit_margin\": 0.0\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Compute overall totals to derive revenue share\n",
    "    total_rev_all = (\n",
    "        category_perf\n",
    "        .agg(F.sum(\"total_revenue\").alias(\"total_revenue_all\"))\n",
    "        .first()[0] or 0.0\n",
    "    )\n",
    "\n",
    "    category_perf_with_share = (\n",
    "        category_perf\n",
    "        .withColumn(\n",
    "            \"revenue_share\",\n",
    "            F.when(F.lit(total_rev_all) > 0,\n",
    "                   F.col(\"total_revenue\") / F.lit(total_rev_all))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # \"Low-performing\" = low revenue & low efficiency metrics.\n",
    "    # Here we just *rank* by these; you can add explicit filters later in BI.\n",
    "    product_analysis[\"low_performing_categories\"] = (\n",
    "        category_perf_with_share\n",
    "        .orderBy(\n",
    "            F.col(\"total_revenue\").asc_nulls_last(),          # lowest revenue first\n",
    "            F.col(\"avg_view_to_purchase_rate\").asc_nulls_last(),\n",
    "            F.col(\"avg_revenue_per_view\").asc_nulls_last(),\n",
    "            F.col(\"avg_profit_margin\").asc_nulls_last()\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping low-performing categories analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ee1b6",
   "metadata": {},
   "source": [
    "Category popularity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7e9d6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category popularity score\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_orders\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "    category_pop_df = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"product_id\").alias(\"products_in_category\"),\n",
    "            F.sum(\"total_units_sold\").alias(\"total_units_sold\"),\n",
    "            F.sum(\"total_orders\").alias(\"total_orders\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "            F.sum(\"total_wishlist_adds\").alias(\"total_wishlist_adds\"),\n",
    "            F.sum(\"total_cart_adds\").alias(\"total_cart_adds\")\n",
    "        )\n",
    "        .fillna({\n",
    "            \"products_in_category\": 0,\n",
    "            \"total_units_sold\": 0,\n",
    "            \"total_orders\": 0,\n",
    "            \"total_revenue\": 0.0,\n",
    "            \"total_wishlist_adds\": 0,\n",
    "            \"total_cart_adds\": 0\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # Get max values for normalization\n",
    "    max_vals_cat = category_pop_df.agg(\n",
    "        F.max(\"total_units_sold\").alias(\"max_units\"),\n",
    "        F.max(\"total_orders\").alias(\"max_orders\"),\n",
    "        F.max(\"total_revenue\").alias(\"max_revenue\"),\n",
    "        F.max(\"total_wishlist_adds\").alias(\"max_wishlist\"),\n",
    "        F.max(\"total_cart_adds\").alias(\"max_cart\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    max_units = max_vals_cat[\"max_units\"] or 0\n",
    "    max_orders = max_vals_cat[\"max_orders\"] or 0\n",
    "    max_revenue = max_vals_cat[\"max_revenue\"] or 0.0\n",
    "    max_wishlist = max_vals_cat[\"max_wishlist\"] or 0\n",
    "    max_cart = max_vals_cat[\"max_cart\"] or 0\n",
    "\n",
    "    category_pop_scored = (\n",
    "        category_pop_df\n",
    "        .withColumn(\n",
    "            \"units_score\",\n",
    "            F.when(F.lit(max_units) > 0,\n",
    "                   F.col(\"total_units_sold\") / F.lit(max_units))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"orders_score\",\n",
    "            F.when(F.lit(max_orders) > 0,\n",
    "                   F.col(\"total_orders\") / F.lit(max_orders))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"revenue_score\",\n",
    "            F.when(F.lit(max_revenue) > 0,\n",
    "                   F.col(\"total_revenue\") / F.lit(max_revenue))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"wishlist_score\",\n",
    "            F.when(F.lit(max_wishlist) > 0,\n",
    "                   F.col(\"total_wishlist_adds\") / F.lit(max_wishlist))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"cart_score\",\n",
    "            F.when(F.lit(max_cart) > 0,\n",
    "                   F.col(\"total_cart_adds\") / F.lit(max_cart))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        # Composite popularity score (tweak weights as needed)\n",
    "        .withColumn(\n",
    "            \"category_popularity_score\",\n",
    "            0.3 * F.col(\"revenue_score\")\n",
    "            + 0.25 * F.col(\"units_score\")\n",
    "            + 0.2 * F.col(\"orders_score\")\n",
    "            + 0.15 * F.col(\"wishlist_score\")\n",
    "            + 0.1 * F.col(\"cart_score\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_popularity_score\"] = (\n",
    "        category_pop_scored\n",
    "        .select(\n",
    "            \"category\",\n",
    "            \"products_in_category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"total_wishlist_adds\",\n",
    "            \"total_cart_adds\",\n",
    "            \"units_score\",\n",
    "            \"orders_score\",\n",
    "            \"revenue_score\",\n",
    "            \"wishlist_score\",\n",
    "            \"cart_score\",\n",
    "            \"category_popularity_score\"\n",
    "        )\n",
    "        .fillna({\n",
    "            \"units_score\": 0.0,\n",
    "            \"orders_score\": 0.0,\n",
    "            \"revenue_score\": 0.0,\n",
    "            \"wishlist_score\": 0.0,\n",
    "            \"cart_score\": 0.0,\n",
    "            \"category_popularity_score\": 0.0\n",
    "        })\n",
    "        .orderBy(F.col(\"category_popularity_score\").desc_nulls_last())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping category popularity score analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fe246",
   "metadata": {},
   "source": [
    "Category profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "764beab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the required columns in agg_products is all NULL or zero; skipping category profitability analysis.\n"
     ]
    }
   ],
   "source": [
    "# Category profitability\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_profit\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"profit_margin\")\n",
    "):\n",
    "    category_profit_df = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"product_id\").alias(\"products_in_category\"),\n",
    "            F.sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "            F.sum(\"total_profit\").alias(\"category_profit\"),\n",
    "            F.avg(\"profit_margin\").alias(\"avg_profit_margin\"),\n",
    "            F.sum(\"total_units_sold\").alias(\"total_units_sold\"),\n",
    "            F.sum(\"total_orders\").alias(\"total_orders\")\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"products_in_category\": 0,\n",
    "                \"category_revenue\": 0.0,\n",
    "                \"category_profit\": 0.0,\n",
    "                \"avg_profit_margin\": 0.0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Total profit to compute profit share\n",
    "    total_profit_all = (\n",
    "        category_profit_df\n",
    "        .agg(F.sum(\"category_profit\").alias(\"total_profit_all\"))\n",
    "        .first()[0] or 0.0\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_profitability\"] = (\n",
    "        category_profit_df\n",
    "        .withColumn(\n",
    "            \"profit_per_product\",\n",
    "            F.when(F.col(\"products_in_category\") > 0,\n",
    "                   F.col(\"category_profit\") / F.col(\"products_in_category\"))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"revenue_per_product\",\n",
    "            F.when(F.col(\"products_in_category\") > 0,\n",
    "                   F.col(\"category_revenue\") / F.col(\"products_in_category\"))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"profit_share\",\n",
    "            F.when(F.lit(total_profit_all) > 0,\n",
    "                   F.col(\"category_profit\") / F.lit(total_profit_all))\n",
    "             .otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"category_profit\").desc_nulls_last()\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping category profitability analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a54af",
   "metadata": {},
   "source": [
    "Category peak season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c97a62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and \"agg_order_items\" in dataframes\n",
    "    and \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_placed_at\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"order_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"quantity\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "):\n",
    "    # 1) Base joined fact with order date\n",
    "    category_season_fact = (\n",
    "        dataframes[\"agg_order_items\"]\n",
    "        .join(\n",
    "            dataframes[\"agg_orders\"].select(\"order_id\", \"order_placed_at\"),\n",
    "            on=\"order_id\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .join(\n",
    "            dataframes[\"agg_products\"].select(\n",
    "                \"product_id\",\n",
    "                \"category\",\n",
    "                \"sub_category\",\n",
    "                \"brand\",\n",
    "                \"sell_price\",\n",
    "            ),\n",
    "            on=\"product_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\"order_date\", F.to_date(\"order_placed_at\"))\n",
    "    )\n",
    "\n",
    "    category_season_fact = category_season_fact.withColumn(\n",
    "        \"calendar_month\", F.month(\"order_date\")\n",
    "    )\n",
    "\n",
    "    category_season_fact = (\n",
    "        category_season_fact\n",
    "        .withColumn(\n",
    "            \"line_revenue\",\n",
    "            F.when(\n",
    "                F.col(\"sell_price\").isNotNull(),\n",
    "                F.col(\"quantity\") * F.col(\"sell_price\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"quantity\": 0,\n",
    "                \"line_revenue\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    category_monthly_season = (\n",
    "        category_season_fact\n",
    "        .groupBy(\"category\", \"calendar_month\")\n",
    "        .agg(\n",
    "            F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "            F.sum(\"line_revenue\").alias(\"total_revenue_month\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"units_sold\": 0,\n",
    "                \"orders_count\": 0,\n",
    "                \"total_revenue_month\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_monthly_seasonality\"] = (\n",
    "        category_monthly_season.orderBy(\"category\", \"calendar_month\")\n",
    "    )\n",
    "\n",
    "\n",
    "    category_max_revenue = (\n",
    "        category_monthly_season\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.max(\"total_revenue_month\").alias(\"max_revenue_month\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    category_peak_season = (\n",
    "        category_monthly_season.alias(\"m\")\n",
    "        .join(\n",
    "            category_max_revenue.alias(\"mx\"),\n",
    "            on=\"category\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .filter(F.col(\"m.total_revenue_month\") == F.col(\"mx.max_revenue_month\"))\n",
    "        .select(\n",
    "            F.col(\"m.category\").alias(\"category\"),\n",
    "            F.col(\"m.calendar_month\").alias(\"peak_season_month\"),\n",
    "            F.col(\"m.units_sold\"),\n",
    "            F.col(\"m.orders_count\"),\n",
    "            F.col(\"m.total_revenue_month\").alias(\"peak_season_revenue\"),\n",
    "        )\n",
    "        .orderBy(\"category\", \"peak_season_month\")\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_peak_season\"] = category_peak_season\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_orders, agg_order_items, or agg_products \"\n",
    "        \"is all NULL or zero; skipping category peak season analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65973f96",
   "metadata": {},
   "source": [
    "Poduct lifecycle and inventory health analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a29981",
   "metadata": {},
   "source": [
    "Product lifecycle segments (new vs mature) and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "13556288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product lifecycle segments\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"days_since_launch\")\n",
    "):\n",
    "    lifecycle_df = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"launch_date\",\n",
    "            \"days_since_launch\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"view_to_purchase_rate\",\n",
    "            \"avg_rating\",\n",
    "            \"current_stock_level\",\n",
    "            \"current_stock\",\n",
    "            \"days_of_supply\",\n",
    "            \"inventory_turnover_rate\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"days_since_launch\": 0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"view_to_purchase_rate\": 0.0,\n",
    "                \"avg_rating\": 0.0,\n",
    "                \"current_stock_level\": 0,\n",
    "                \"current_stock\": 0,\n",
    "                \"days_of_supply\": 0.0,\n",
    "                \"inventory_turnover_rate\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        # Segment by lifecycle\n",
    "        .withColumn(\n",
    "            \"lifecycle_stage\",\n",
    "            F.when(F.col(\"days_since_launch\") < 90, F.lit(\"New (<90 days)\"))\n",
    "             .when(F.col(\"days_since_launch\") < 365, F.lit(\"Growth (90-365 days)\"))\n",
    "             .otherwise(F.lit(\"Mature (>=365 days)\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_lifecycle_segments\"] = lifecycle_df\n",
    "\n",
    "    # Aggregate summary per lifecycle stage\n",
    "    product_analysis[\"product_lifecycle_summary\"] = (\n",
    "        lifecycle_df\n",
    "        .groupBy(\"lifecycle_stage\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"product_id\").alias(\"products_count\"),\n",
    "            F.avg(\"view_to_purchase_rate\").alias(\"avg_view_to_purchase_rate\"),\n",
    "            F.avg(\"avg_rating\").alias(\"avg_rating\"),\n",
    "            F.avg(\"total_units_sold\").alias(\"avg_units_sold\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue\"),\n",
    "            F.avg(\"inventory_turnover_rate\").alias(\"avg_inventory_turnover_rate\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"products_count\": 0,\n",
    "                \"avg_view_to_purchase_rate\": 0.0,\n",
    "                \"avg_rating\": 0.0,\n",
    "                \"avg_units_sold\": 0.0,\n",
    "                \"avg_revenue\": 0.0,\n",
    "                \"avg_inventory_turnover_rate\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\"lifecycle_stage\")\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping product lifecycle segments analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecc6e2",
   "metadata": {},
   "source": [
    "Stockout risk: high units sold but low stock / days_of_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6e9c5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product stockout risk\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_name\")\n",
    "    and (\n",
    "        \"current_stock_level\" in dataframes[\"agg_products\"].columns\n",
    "        or \"current_stock\" in dataframes[\"agg_products\"].columns\n",
    "    )\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"days_of_supply\")\n",
    "):\n",
    "    prod_df = dataframes[\"agg_products\"]\n",
    "\n",
    "    # Prefer current_stock_level if present\n",
    "    stock_col = (\n",
    "        \"current_stock_level\"\n",
    "        if \"current_stock_level\" in prod_df.columns\n",
    "        else \"current_stock\"\n",
    "    )\n",
    "\n",
    "    stockout_risk_df = (\n",
    "        prod_df\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            F.col(stock_col).alias(\"current_stock_level\"),\n",
    "            \"days_of_supply\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"current_stock_level\": 0,\n",
    "                \"days_of_supply\": 0.0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_high_seller\",\n",
    "            F.col(\"total_units_sold\") > 0,\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_low_stock\",\n",
    "            (F.col(\"current_stock_level\") <= 0)\n",
    "            | (F.col(\"days_of_supply\") <= 3),  # threshold: <= 3 days of supply\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"stockout_risk_flag\",\n",
    "            F.when(F.col(\"is_high_seller\") & F.col(\"is_low_stock\"), F.lit(1)).otherwise(F.lit(0)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_stockout_risk\"] = (\n",
    "        stockout_risk_df\n",
    "        .orderBy(\n",
    "            F.col(\"stockout_risk_flag\").desc_nulls_last(),\n",
    "            F.col(\"total_units_sold\").desc_nulls_last(),\n",
    "            F.col(\"days_of_supply\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping stockout risk analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f4e4e",
   "metadata": {},
   "source": [
    "Replenishment issues: repeated stockouts + high revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "475851e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product stockout replenishment priority\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"stockout_occurrences\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"stockout_days\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "    product_analysis[\"product_stockout_replenishment\"] = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"stockout_occurrences\",\n",
    "            \"stockout_days\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"stockout_occurrences\": 0,\n",
    "                \"stockout_days\": 0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"replenishment_priority_score\",\n",
    "            F.col(\"stockout_occurrences\") * 1.0\n",
    "            + F.col(\"stockout_days\") * 0.1\n",
    "            + (F.col(\"total_revenue\") / F.lit(1000.0)),  # scale revenue\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"replenishment_priority_score\").desc_nulls_last(),\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping stockout replenishment analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0be24",
   "metadata": {},
   "source": [
    "\"Dead stock\": high stock, low sales & low turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "099754c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product dead stock analysis\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and (\n",
    "        \"current_stock_level\" in dataframes[\"agg_products\"].columns\n",
    "        or \"current_stock\" in dataframes[\"agg_products\"].columns\n",
    "    )\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"inventory_turnover_rate\")\n",
    "):\n",
    "    prod_df2 = dataframes[\"agg_products\"]\n",
    "\n",
    "    stock_col2 = (\n",
    "        \"current_stock_level\"\n",
    "        if \"current_stock_level\" in prod_df2.columns\n",
    "        else \"current_stock\"\n",
    "    )\n",
    "\n",
    "    dead_stock_df = (\n",
    "        prod_df2\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            F.col(stock_col2).alias(\"current_stock_level\"),\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"total_revenue\",\n",
    "            \"inventory_turnover_rate\",\n",
    "            \"days_of_supply\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"current_stock_level\": 0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"inventory_turnover_rate\": 0.0,\n",
    "                \"days_of_supply\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        # Heuristic \"dead stock\" score\n",
    "        .withColumn(\n",
    "            \"dead_stock_score\",\n",
    "            F.col(\"current_stock_level\") * 1.0\n",
    "            - F.col(\"total_units_sold\") * 0.5\n",
    "            - F.col(\"inventory_turnover_rate\") * 10.0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_dead_stock\"] = (\n",
    "        dead_stock_df\n",
    "        .orderBy(\n",
    "            F.col(\"dead_stock_score\").desc_nulls_last(),\n",
    "            F.col(\"current_stock_level\").desc_nulls_last(),\n",
    "            F.col(\"inventory_turnover_rate\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products is all NULL or zero; \"\n",
    "        \"skipping dead stock analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958c5e2",
   "metadata": {},
   "source": [
    "Base per‑product inventory health (with revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1652058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Product inventory health (joined with revenue from agg_products)\n",
    "\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"current_stock\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"available_stock\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"days_of_supply\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"inventory_turnover_ratio\")\n",
    "):\n",
    "    inv_health = (\n",
    "        dataframes[\"agg_product_inventory_health\"]\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"current_stock\",\n",
    "            \"available_stock\",\n",
    "            \"reorder_point_breach_count\",\n",
    "            \"stockout_frequency\",\n",
    "            \"avg_stock_quantity\",\n",
    "            \"days_of_supply\",\n",
    "            \"inventory_turnover_ratio\",\n",
    "            \"reorder_urgency\",\n",
    "            \"stock_health_score\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"current_stock\": 0,\n",
    "                \"available_stock\": 0,\n",
    "                \"reorder_point_breach_count\": 0,\n",
    "                \"stockout_frequency\": 0.0,\n",
    "                \"avg_stock_quantity\": 0.0,\n",
    "                \"days_of_supply\": 0.0,\n",
    "                \"inventory_turnover_ratio\": 0.0,\n",
    "                \"reorder_urgency\": 0.0,\n",
    "                \"stock_health_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod_basic = (\n",
    "            dataframes[\"agg_products\"]\n",
    "            .select(\n",
    "                \"product_id\",\n",
    "                \"product_name\",\n",
    "                \"category\",\n",
    "                \"sub_category\",\n",
    "                \"brand\",\n",
    "                \"total_units_sold\",\n",
    "                \"total_orders\",\n",
    "                \"total_revenue\",\n",
    "            )\n",
    "            .fillna(\n",
    "                {\n",
    "                    \"product_name\": \"Unknown\",\n",
    "                    \"category\": \"Unknown\",\n",
    "                    \"sub_category\": \"Unknown\",\n",
    "                    \"brand\": \"Unknown\",\n",
    "                    \"total_units_sold\": 0,\n",
    "                    \"total_orders\": 0,\n",
    "                    \"total_revenue\": 0.0,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        product_inventory_health_df = (\n",
    "            inv_health.join(prod_basic, on=\"product_id\", how=\"left\")\n",
    "        )\n",
    "    else:\n",
    "        product_inventory_health_df = inv_health\n",
    "\n",
    "    product_analysis[\"product_inventory_health\"] = product_inventory_health_df\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_product_inventory_health is all NULL or zero; \"\n",
    "        \"skipping base product inventory health view.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7257ab1",
   "metadata": {},
   "source": [
    "Critical products (high revenue + low days_of_supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "44d1f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Critical products: high revenue + low days_of_supply (urgent replenishment)\n",
    "\n",
    "if \"product_inventory_health\" in product_analysis:\n",
    "    base = (\n",
    "        product_analysis[\"product_inventory_health\"]\n",
    "        .fillna(\n",
    "            {\n",
    "                \"days_of_supply\": 0.0,\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"reorder_urgency\": 0.0,\n",
    "                \"stock_health_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "   \n",
    "    critical_products = (\n",
    "        base\n",
    "        .withColumn(\n",
    "            \"is_low_days_of_supply\",\n",
    "            F.col(\"days_of_supply\") <= 3,\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"criticality_score\",\n",
    "            F.when(F.col(\"is_low_days_of_supply\"), 1.0).otherwise(0.0)\n",
    "            + (F.col(\"total_revenue\") / F.lit(1000.0))\n",
    "            + (F.col(\"reorder_urgency\") * 0.5)\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"criticality_score\").desc_nulls_last(),\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "            F.col(\"days_of_supply\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_inventory_critical\"] = critical_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e6041",
   "metadata": {},
   "source": [
    "Supplier × Product performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1866c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Supplier × Product performance\n",
    "\n",
    "if (\n",
    "    \"agg_products\" in dataframes\n",
    "    and \"agg_suppliers\" in dataframes\n",
    "    and \"agg_supplier_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"supplier_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "\n",
    "    # --- Base product + supplier_id info from products (p_ prefix) ---\n",
    "    products_with_supplier = (\n",
    "        dataframes[\"agg_products\"]\n",
    "        .select(\n",
    "            F.col(\"product_id\").alias(\"p_product_id\"),\n",
    "            F.col(\"product_name\").alias(\"p_product_name\"),\n",
    "            F.col(\"category\").alias(\"p_category\"),\n",
    "            F.col(\"sub_category\").alias(\"p_sub_category\"),\n",
    "            F.col(\"brand\").alias(\"p_brand\"),\n",
    "            \"supplier_id\",                               # join key, keep as-is\n",
    "            F.col(\"total_units_sold\").alias(\"p_total_units_sold\"),\n",
    "            F.col(\"total_orders\").alias(\"p_total_orders\"),\n",
    "            F.col(\"total_revenue\").alias(\"p_total_revenue\"),\n",
    "            F.col(\"total_profit\").alias(\"p_total_profit\"),\n",
    "            F.col(\"profit_margin\").alias(\"p_profit_margin\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"p_total_units_sold\": 0,\n",
    "                \"p_total_orders\": 0,\n",
    "                \"p_total_revenue\": 0.0,\n",
    "                \"p_total_profit\": 0.0,\n",
    "                \"p_profit_margin\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Supplier-level metrics (s_ prefix) ---\n",
    "    suppliers_basic = (\n",
    "        dataframes[\"agg_suppliers\"]\n",
    "        .select(\n",
    "            \"supplier_id\",\n",
    "            F.col(\"supplier_status\").alias(\"s_supplier_status\"),\n",
    "            F.col(\"is_preferred\").alias(\"s_is_preferred\"),\n",
    "            F.col(\"is_verified\").alias(\"s_is_verified\"),\n",
    "            F.col(\"total_products_supplied\").alias(\"s_total_products_supplied\"),\n",
    "            F.col(\"total_units_sold\").alias(\"s_total_units_sold\"),\n",
    "            F.col(\"total_orders_fulfilled\").alias(\"s_total_orders_fulfilled\"),\n",
    "            F.col(\"total_reviews\").alias(\"s_total_reviews\"),\n",
    "            F.col(\"total_stockouts\").alias(\"s_total_stockouts\"),\n",
    "            F.col(\"total_revenue_generated\").alias(\"s_total_revenue_generated\"),\n",
    "            F.col(\"avg_profit_margin\").alias(\"s_avg_profit_margin\"),\n",
    "            F.col(\"avg_product_rating\").alias(\"s_avg_product_rating\"),\n",
    "            F.col(\"supplier_performance_score\").alias(\"s_supplier_performance_score\"),\n",
    "            F.col(\"supplier_reliability_score\").alias(\"s_supplier_reliability_score\"),\n",
    "            F.col(\"stock_efficiency_ratio\").alias(\"s_stock_efficiency_ratio\"),\n",
    "            F.col(\"stockout_rate\").alias(\"s_stockout_rate\"),\n",
    "            F.col(\"supplier_inventory_health_score\").alias(\"s_inventory_health_score\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"s_supplier_status\": \"Unknown\",\n",
    "                \"s_is_preferred\": False,\n",
    "                \"s_is_verified\": False,\n",
    "                \"s_total_products_supplied\": 0,\n",
    "                \"s_total_units_sold\": 0,\n",
    "                \"s_total_orders_fulfilled\": 0,\n",
    "                \"s_total_reviews\": 0,\n",
    "                \"s_total_stockouts\": 0,\n",
    "                \"s_total_revenue_generated\": 0.0,\n",
    "                \"s_avg_profit_margin\": 0.0,\n",
    "                \"s_avg_product_rating\": 0.0,\n",
    "                \"s_supplier_performance_score\": 0.0,\n",
    "                \"s_supplier_reliability_score\": 0.0,\n",
    "                \"s_stock_efficiency_ratio\": 0.0,\n",
    "                \"s_stockout_rate\": 0.0,\n",
    "                \"s_inventory_health_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Supplier inventory health (si_ prefix) ---\n",
    "    supplier_inv_health = (\n",
    "        dataframes[\"agg_supplier_inventory_health\"]\n",
    "        .select(\n",
    "            \"supplier_id\",\n",
    "            F.col(\"total_products\").alias(\"si_total_products\"),\n",
    "            F.col(\"total_current_stock\").alias(\"si_total_current_stock\"),\n",
    "            F.col(\"total_available_stock\").alias(\"si_total_available_stock\"),\n",
    "            F.col(\"total_reorder_breaches\").alias(\"si_total_reorder_breaches\"),\n",
    "            F.col(\"total_stockouts\").alias(\"si_total_stockouts\"),\n",
    "            F.col(\"total_storage_cost\").alias(\"si_total_storage_cost\"),\n",
    "            F.col(\"avg_stock_per_product\").alias(\"si_avg_stock_per_product\"),\n",
    "            F.col(\"stockout_rate\").alias(\"si_stockout_rate\"),\n",
    "            F.col(\"supplier_inventory_health_score\").alias(\"si_inventory_health_score\"),\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"si_total_products\": 0,\n",
    "                \"si_total_current_stock\": 0,\n",
    "                \"si_total_available_stock\": 0,\n",
    "                \"si_total_reorder_breaches\": 0,\n",
    "                \"si_total_stockouts\": 0,\n",
    "                \"si_total_storage_cost\": 0.0,\n",
    "                \"si_avg_stock_per_product\": 0.0,\n",
    "                \"si_stockout_rate\": 0.0,\n",
    "                \"si_inventory_health_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Join all three on supplier_id ---\n",
    "    joined = (\n",
    "        products_with_supplier\n",
    "        .join(suppliers_basic, on=\"supplier_id\", how=\"left\")\n",
    "        .join(supplier_inv_health, on=\"supplier_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # --- Final, clean schema (no duplicate names) ---\n",
    "    supplier_product_performance = joined.select(\n",
    "        # product-level fields\n",
    "        F.col(\"p_product_id\").alias(\"product_id\"),\n",
    "        F.col(\"p_product_name\").alias(\"product_name\"),\n",
    "        F.col(\"p_category\").alias(\"category\"),\n",
    "        F.col(\"p_sub_category\").alias(\"sub_category\"),\n",
    "        F.col(\"p_brand\").alias(\"brand\"),\n",
    "        \"supplier_id\",\n",
    "        F.col(\"p_total_units_sold\").alias(\"total_units_sold\"),\n",
    "        F.col(\"p_total_orders\").alias(\"total_orders\"),\n",
    "        F.col(\"p_total_revenue\").alias(\"total_revenue\"),\n",
    "        F.col(\"p_total_profit\").alias(\"total_profit\"),\n",
    "        F.col(\"p_profit_margin\").alias(\"profit_margin\"),\n",
    "\n",
    "        # supplier-level fields\n",
    "        F.col(\"s_supplier_status\").alias(\"supplier_status\"),\n",
    "        F.col(\"s_is_preferred\").alias(\"is_preferred\"),\n",
    "        F.col(\"s_is_verified\").alias(\"is_verified\"),\n",
    "        F.col(\"s_total_products_supplied\").alias(\"sup_total_products_supplied\"),\n",
    "        F.col(\"s_total_units_sold\").alias(\"sup_total_units_sold\"),\n",
    "        F.col(\"s_total_orders_fulfilled\").alias(\"sup_total_orders_fulfilled\"),\n",
    "        F.col(\"s_total_reviews\").alias(\"sup_total_reviews\"),\n",
    "        F.col(\"s_total_stockouts\").alias(\"sup_total_stockouts\"),\n",
    "        F.col(\"s_total_revenue_generated\").alias(\"sup_total_revenue_generated\"),\n",
    "        F.col(\"s_avg_profit_margin\").alias(\"sup_avg_profit_margin\"),\n",
    "        F.col(\"s_avg_product_rating\").alias(\"sup_avg_product_rating\"),\n",
    "        F.col(\"s_supplier_performance_score\").alias(\"supplier_performance_score\"),\n",
    "        F.col(\"s_supplier_reliability_score\").alias(\"supplier_reliability_score\"),\n",
    "        F.col(\"s_stock_efficiency_ratio\").alias(\"stock_efficiency_ratio\"),\n",
    "        F.col(\"s_stockout_rate\").alias(\"sup_stockout_rate\"),\n",
    "        F.col(\"s_inventory_health_score\").alias(\"sup_inventory_health_score\"),\n",
    "\n",
    "        # supplier inventory health aggregate\n",
    "        F.col(\"si_total_products\").alias(\"inv_total_products\"),\n",
    "        F.col(\"si_total_current_stock\").alias(\"inv_total_current_stock\"),\n",
    "        F.col(\"si_total_available_stock\").alias(\"inv_total_available_stock\"),\n",
    "        F.col(\"si_total_reorder_breaches\").alias(\"inv_total_reorder_breaches\"),\n",
    "        F.col(\"si_total_stockouts\").alias(\"inv_total_stockouts\"),\n",
    "        F.col(\"si_total_storage_cost\").alias(\"inv_total_storage_cost\"),\n",
    "        F.col(\"si_avg_stock_per_product\").alias(\"inv_avg_stock_per_product\"),\n",
    "        F.col(\"si_stockout_rate\").alias(\"inv_stockout_rate\"),\n",
    "        F.col(\"si_inventory_health_score\").alias(\"inv_inventory_health_score\"),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"supplier_product_performance\"] = supplier_product_performance\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_products/agg_suppliers/agg_supplier_inventory_health \"\n",
    "        \"is all NULL or zero; skipping supplier x product performance view.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf846de",
   "metadata": {},
   "source": [
    "Stockout Rate by Product/Supplier: Critical for lost sales calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3f7c636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize dicts if not present\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "if \"supplier_analysis\" not in locals():\n",
    "    supplier_analysis = {}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Product-level stockout rate (by product)\n",
    "# -------------------------------------------------------\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    # Base inventory health\n",
    "    prod_inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"stockout_frequency\": 0,\n",
    "            \"reorder_point_breach_count\": 0,\n",
    "            \"current_stock\": 0,\n",
    "            \"available_stock\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional enrich with product name & category from agg_products\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod_meta = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\"\n",
    "            # NOTE: DO NOT bring agg_products.stock_status to avoid ambiguity\n",
    "        )\n",
    "\n",
    "        prod_inv = (\n",
    "            prod_inv.alias(\"i\")\n",
    "            .join(prod_meta.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "        )\n",
    "\n",
    "    # Use stock_status from agg_product_inventory_health ONLY\n",
    "    product_analysis[\"stockout_rate_by_product\"] = (\n",
    "        prod_inv.select(\n",
    "            \"product_id\",\n",
    "            \"supplier_id\",\n",
    "            \"stockout_frequency\",\n",
    "            \"reorder_point_breach_count\",\n",
    "            \"current_stock\",\n",
    "            \"available_stock\",\n",
    "            \"days_of_supply\",\n",
    "            \"stock_status\",      # this is i.stock_status\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"stockout_frequency\").desc_nulls_last(),\n",
    "            F.col(\"reorder_point_breach_count\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Supplier-level stockout rate\n",
    "# -------------------------------------------------------\n",
    "has_suppliers = (\n",
    "    \"agg_suppliers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_suppliers\"], \"supplier_id\")\n",
    ")\n",
    "has_sup_inv = (\n",
    "    \"agg_supplier_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_supplier_inventory_health\"], \"supplier_id\")\n",
    ")\n",
    "\n",
    "if has_suppliers or has_sup_inv:\n",
    "    # Supplier base (from agg_suppliers if available)\n",
    "    if has_suppliers:\n",
    "        s = dataframes[\"agg_suppliers\"].select(\n",
    "            \"supplier_id\",\n",
    "            \"supplier_status\",\n",
    "            \"total_products_supplied\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders_fulfilled\",\n",
    "            \"total_stockouts\",\n",
    "            \"stockout_rate\",                 # supplier-level stockout rate\n",
    "            \"supplier_reliability_score\",\n",
    "            \"supplier_inventory_health_score\",\n",
    "        )\n",
    "    else:\n",
    "        # Fallback: only supplier_id from inventory health\n",
    "        s = dataframes[\"agg_supplier_inventory_health\"].select(\"supplier_id\").distinct()\n",
    "\n",
    "    # Inventory health aggregates (renamed to avoid clashes)\n",
    "    if has_sup_inv:\n",
    "        i = dataframes[\"agg_supplier_inventory_health\"].select(\n",
    "            \"supplier_id\",\n",
    "            F.col(\"total_stockouts\").alias(\"inv_total_stockouts\"),\n",
    "            F.col(\"stockout_rate\").alias(\"inv_stockout_rate\"),\n",
    "            F.col(\"total_products\").alias(\"inv_total_products\"),\n",
    "            F.col(\"total_current_stock\").alias(\"inv_total_current_stock\"),\n",
    "            F.col(\"total_available_stock\").alias(\"inv_total_available_stock\"),\n",
    "        )\n",
    "        joined = s.alias(\"s\").join(i.alias(\"i\"), on=\"supplier_id\", how=\"left\")\n",
    "    else:\n",
    "        joined = s\n",
    "\n",
    "    supplier_analysis[\"stockout_rate_by_supplier\"] = (\n",
    "        joined.select(\n",
    "            \"supplier_id\",\n",
    "            F.coalesce(F.col(\"supplier_status\"), F.lit(\"unknown\")).alias(\"supplier_status\"),\n",
    "            F.coalesce(F.col(\"total_stockouts\"), F.lit(0)).alias(\"total_stockouts\"),\n",
    "            F.coalesce(F.col(\"stockout_rate\"), F.lit(0.0)).alias(\"supplier_stockout_rate\"),\n",
    "            F.coalesce(F.col(\"inv_total_stockouts\"), F.lit(0)).alias(\"inv_total_stockouts\"),\n",
    "            F.coalesce(F.col(\"inv_stockout_rate\"), F.lit(0.0)).alias(\"inv_stockout_rate\"),\n",
    "            F.coalesce(F.col(\"inv_total_products\"), F.lit(0)).alias(\"inv_total_products\"),\n",
    "            F.coalesce(F.col(\"inv_total_current_stock\"), F.lit(0)).alias(\"inv_total_current_stock\"),\n",
    "            F.coalesce(F.col(\"inv_total_available_stock\"), F.lit(0)).alias(\"inv_total_available_stock\"),\n",
    "            F.coalesce(F.col(\"supplier_reliability_score\"), F.lit(0.0)).alias(\"supplier_reliability_score\"),\n",
    "            F.coalesce(F.col(\"supplier_inventory_health_score\"), F.lit(0.0)).alias(\"supplier_inventory_health_score\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"supplier_stockout_rate\").desc_nulls_last(),\n",
    "            F.col(\"total_stockouts\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Neither agg_suppliers nor agg_supplier_inventory_health usable; \"\n",
    "        \"skipping stockout rate by supplier.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768c94e",
   "metadata": {},
   "source": [
    "Supplier Ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ac6e99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplier ranking: total_revenue_generated, avg_profit_margin, stockout_rate\n",
    "\n",
    "if (\n",
    "    \"agg_suppliers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_suppliers\"], \"supplier_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_suppliers\"], \"total_revenue_generated\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_suppliers\"], \"avg_profit_margin\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_suppliers\"], \"stockout_rate\")\n",
    "):\n",
    "    product_analysis[\"supplier_ranking_core\"] = (\n",
    "        dataframes[\"agg_suppliers\"]\n",
    "        .select(\n",
    "            \"supplier_id\",\n",
    "            \"supplier_status\",\n",
    "            \"is_preferred\",\n",
    "            \"is_verified\",\n",
    "            \"total_products_supplied\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders_fulfilled\",\n",
    "            \"total_stockouts\",\n",
    "            \"total_revenue_generated\",\n",
    "            \"avg_profit_margin\",\n",
    "            \"stockout_rate\",\n",
    "            \"supplier_inventory_health_score\",\n",
    "            \"supplier_performance_score\",\n",
    "            \"supplier_reliability_score\",\n",
    "            \"stock_efficiency_ratio\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"total_products_supplied\": 0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders_fulfilled\": 0,\n",
    "                \"total_stockouts\": 0,\n",
    "                \"total_revenue_generated\": 0.0,\n",
    "                \"avg_profit_margin\": 0.0,\n",
    "                \"stockout_rate\": 0.0,\n",
    "                \"supplier_inventory_health_score\": 0.0,\n",
    "                \"supplier_performance_score\": 0.0,\n",
    "                \"supplier_reliability_score\": 0.0,\n",
    "                \"stock_efficiency_ratio\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"total_revenue_generated\").desc_nulls_last(),  # high revenue first\n",
    "            F.col(\"avg_profit_margin\").desc_nulls_last(),        # then high margin\n",
    "            F.col(\"stockout_rate\").asc_nulls_last(),             # then low stockout_rate\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_suppliers is all NULL or zero; \"\n",
    "        \"skipping core supplier ranking analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e384b3",
   "metadata": {},
   "source": [
    " Supplier stockout impact on top‑revenue products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "39bb87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Supplier stockout impact: focus on top-revenue products\n",
    "\n",
    "if \"supplier_product_performance\" in product_analysis:\n",
    "    spp2 = (\n",
    "        product_analysis[\"supplier_product_performance\"]\n",
    "        .fillna(\n",
    "            {\n",
    "                \"total_revenue\": 0.0,\n",
    "                \"total_units_sold\": 0,\n",
    "                \"total_orders\": 0,\n",
    "                \"sup_total_stockouts\": 0,\n",
    "                \"sup_stockout_rate\": 0.0,\n",
    "                \"sup_inventory_health_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    TOP_REVENUE_THRESHOLD = 10000.0\n",
    "\n",
    "    supplier_stockout_impact = (\n",
    "        spp2\n",
    "        .withColumn(\n",
    "            \"is_top_revenue_product\",\n",
    "            F.col(\"total_revenue\") >= F.lit(TOP_REVENUE_THRESHOLD),\n",
    "        )\n",
    "        .filter(F.col(\"is_top_revenue_product\"))\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"sub_category\",\n",
    "            \"brand\",\n",
    "            \"supplier_id\",\n",
    "            \"supplier_status\",\n",
    "            \"is_preferred\",\n",
    "            \"is_verified\",\n",
    "            \"total_revenue\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders\",\n",
    "            \"profit_margin\",\n",
    "            # use disambiguated supplier-level columns\n",
    "            \"sup_total_stockouts\",\n",
    "            \"sup_stockout_rate\",\n",
    "            \"sup_inventory_health_score\",\n",
    "            \"supplier_performance_score\",\n",
    "            \"supplier_reliability_score\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"sup_total_stockouts\").desc_nulls_last(),\n",
    "            F.col(\"sup_stockout_rate\").desc_nulls_last(),\n",
    "            F.col(\"total_revenue\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"supplier_stockout_impact_on_products\"] = supplier_stockout_impact\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"supplier_product_performance not available; \"\n",
    "        \"skipping supplier stockout impact on products analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3048a5a",
   "metadata": {},
   "source": [
    "cateogory to category affinity / which categories are often bought together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7bc0cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-to-category affinity analysis\n",
    "\n",
    "if (\n",
    "    \"agg_category_affinity\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_category_affinity\"], \"product_a_category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_category_affinity\"], \"product_b_category\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_category_affinity\"], \"pair_count\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_category_affinity\"], \"total_co_occurrences\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_category_affinity\"], \"avg_lift_between_categories\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_category_affinity\"], \"avg_support\")\n",
    "):\n",
    "    category_affinity_base = (\n",
    "        dataframes[\"agg_category_affinity\"]\n",
    "        .select(\n",
    "            \"product_a_category\",\n",
    "            \"product_b_category\",\n",
    "            \"pair_count\",\n",
    "            \"total_co_occurrences\",\n",
    "            \"avg_lift_between_categories\",\n",
    "            \"avg_support\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"pair_count\": 0,\n",
    "                \"total_co_occurrences\": 0,\n",
    "                \"avg_lift_between_categories\": 0.0,\n",
    "                \"avg_support\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        # You may want to drop self‑pairs if they exist\n",
    "        .filter(F.col(\"product_a_category\") != F.col(\"product_b_category\"))\n",
    "    )\n",
    "\n",
    "    # Store full pair table ordered by lift\n",
    "    product_analysis[\"category_affinity_pairs\"] = (\n",
    "        category_affinity_base\n",
    "        .orderBy(\n",
    "            F.col(\"avg_lift_between_categories\").desc_nulls_last(),\n",
    "            F.col(\"avg_support\").desc_nulls_last(),\n",
    "            F.col(\"total_co_occurrences\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_category_affinity is all NULL or zero; \"\n",
    "        \"skipping category-to-category affinity analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cb65f",
   "metadata": {},
   "source": [
    "Top partner category per A-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "92296cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top affinity category per base category\n",
    "if \"category_affinity_pairs\" in product_analysis:\n",
    "    base = product_analysis[\"category_affinity_pairs\"]\n",
    "\n",
    "    \n",
    "    max_lift_per_a = (\n",
    "        base.groupBy(\"product_a_category\")\n",
    "        .agg(\n",
    "            F.max(\"avg_lift_between_categories\").alias(\"max_lift_for_a\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    category_affinity_top_per_category = (\n",
    "        base.alias(\"b\")\n",
    "        .join(\n",
    "            max_lift_per_a.alias(\"m\"),\n",
    "            on=\"product_a_category\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .filter(F.col(\"b.avg_lift_between_categories\") == F.col(\"m.max_lift_for_a\"))\n",
    "        .select(\n",
    "            F.col(\"b.product_a_category\").alias(\"base_category\"),\n",
    "            F.col(\"b.product_b_category\").alias(\"affinity_category\"),\n",
    "            F.col(\"b.pair_count\"),\n",
    "            F.col(\"b.total_co_occurrences\"),\n",
    "            F.col(\"b.avg_lift_between_categories\").alias(\"avg_lift\"),\n",
    "            F.col(\"b.avg_support\").alias(\"avg_support\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"avg_lift\").desc_nulls_last(),\n",
    "            F.col(\"avg_support\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"category_affinity_top_per_category\"] = category_affinity_top_per_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc6f361",
   "metadata": {},
   "source": [
    "Base product‑affinity pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "883efff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Product-to-product affinity: full pairs\n",
    "\n",
    "if (\n",
    "    \"agg_product_affinity\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_affinity\"], \"product_a_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_affinity\"], \"product_b_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_affinity\"], \"co_occurrence_count\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_affinity\"], \"support\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_affinity\"], \"avg_lift\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_affinity\"], \"affinity_score\")\n",
    "):\n",
    "    product_affinity_base = (\n",
    "        dataframes[\"agg_product_affinity\"]\n",
    "        .select(\n",
    "            \"product_a_id\",\n",
    "            \"product_a_name\",\n",
    "            \"product_a_category\",\n",
    "            \"product_b_id\",\n",
    "            \"product_b_name\",\n",
    "            \"product_b_category\",\n",
    "            \"co_occurrence_count\",\n",
    "            \"support\",\n",
    "            \"confidence_a_to_b\",\n",
    "            \"confidence_b_to_a\",\n",
    "            \"lift_a_to_b\",\n",
    "            \"lift_b_to_a\",\n",
    "            \"avg_lift\",\n",
    "            \"affinity_strength\",\n",
    "            \"affinity_score\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"co_occurrence_count\": 0,\n",
    "                \"support\": 0.0,\n",
    "                \"confidence_a_to_b\": 0.0,\n",
    "                \"confidence_b_to_a\": 0.0,\n",
    "                \"lift_a_to_b\": 0.0,\n",
    "                \"lift_b_to_a\": 0.0,\n",
    "                \"avg_lift\": 0.0,\n",
    "                \"affinity_strength\": 0.0,\n",
    "                \"affinity_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        # Optional: exclude self‑pairs if present\n",
    "        .filter(F.col(\"product_a_id\") != F.col(\"product_b_id\"))\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_affinity_pairs\"] = (\n",
    "        product_affinity_base\n",
    "        .orderBy(\n",
    "            F.col(\"affinity_score\").desc_nulls_last(),\n",
    "            F.col(\"avg_lift\").desc_nulls_last(),\n",
    "            F.col(\"co_occurrence_count\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_product_affinity is all NULL or zero; \"\n",
    "        \"skipping product-to-product affinity base analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5634c98",
   "metadata": {},
   "source": [
    "Top partner per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "28cbea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"product_affinity_pairs\" in product_analysis:\n",
    "    base = product_analysis[\"product_affinity_pairs\"]\n",
    "\n",
    "    # For each product_a_id, get max affinity_score\n",
    "    max_affinity_per_a = (\n",
    "        base.groupBy(\"product_a_id\")\n",
    "        .agg(\n",
    "            F.max(\"affinity_score\").alias(\"max_affinity_for_a\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_affinity_top_per_product = (\n",
    "        base.alias(\"p\")\n",
    "        .join(\n",
    "            max_affinity_per_a.alias(\"m\"),\n",
    "            on=\"product_a_id\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .filter(F.col(\"p.affinity_score\") == F.col(\"m.max_affinity_for_a\"))\n",
    "        .select(\n",
    "            F.col(\"p.product_a_id\"),\n",
    "            F.col(\"p.product_a_name\"),\n",
    "            F.col(\"p.product_a_category\"),\n",
    "            F.col(\"p.product_b_id\").alias(\"recommended_product_id\"),\n",
    "            F.col(\"p.product_b_name\").alias(\"recommended_product_name\"),\n",
    "            F.col(\"p.product_b_category\").alias(\"recommended_product_category\"),\n",
    "            F.col(\"p.co_occurrence_count\"),\n",
    "            F.col(\"p.support\"),\n",
    "            F.col(\"p.confidence_a_to_b\"),\n",
    "            F.col(\"p.lift_a_to_b\"),\n",
    "            F.col(\"p.avg_lift\"),\n",
    "            F.col(\"p.affinity_strength\"),\n",
    "            F.col(\"p.affinity_score\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"affinity_score\").desc_nulls_last(),\n",
    "            F.col(\"avg_lift\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_affinity_top_per_product\"] = product_affinity_top_per_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ac1a2",
   "metadata": {},
   "source": [
    "Approximate “Top 5 per product”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ef9ff41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Top 5 strong affinity recommendations per product\n",
    "\n",
    "if \"product_affinity_pairs\" in product_analysis:\n",
    "    base = product_analysis[\"product_affinity_pairs\"]\n",
    "\n",
    "    # Tune these thresholds for your data\n",
    "    strong_recos = (\n",
    "        base\n",
    "        .filter(\n",
    "            (F.col(\"affinity_score\") >= 0.5)    # strong normalized score\n",
    "            & (F.col(\"avg_lift\") >= 1.1)        # lift > 1 indicates positive association\n",
    "            & (F.col(\"co_occurrence_count\") >= 5)\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"product_a_id\"),\n",
    "            F.col(\"affinity_score\").desc_nulls_last(),\n",
    "            F.col(\"avg_lift\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"product_affinity_top5_candidates\"] = strong_recos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c524d",
   "metadata": {},
   "source": [
    "precomputed products recommendations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "939f5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    \"agg_product_recommendations\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_recommendations\"], \"product_a_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_recommendations\"], \"recommended_products\")\n",
    "):\n",
    "    \n",
    "    product_analysis[\"precomputed_product_recommendations\"] = (\n",
    "        dataframes[\"agg_product_recommendations\"]\n",
    "        .select(\n",
    "            \"product_a_id\",\n",
    "            \"recommended_products\",   \n",
    "            \"avg_affinity_score\",\n",
    "        )\n",
    "        .fillna(\n",
    "            {\n",
    "                \"recommended_products\": \"[]\",   \n",
    "                \"avg_affinity_score\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    reco_df = product_analysis[\"precomputed_product_recommendations\"]\n",
    "\n",
    "    reco_with_flag = (\n",
    "        reco_df\n",
    "        .withColumn(\n",
    "            \"has_recommendations\",\n",
    "            F.when(\n",
    "                (F.col(\"recommended_products\").isNotNull())\n",
    "                & (F.length(F.trim(F.col(\"recommended_products\"))) > 2),  # more than \"[]\"\n",
    "                F.lit(1)\n",
    "            ).otherwise(F.lit(0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"precomputed_product_recommendations\"] = reco_with_flag\n",
    "\n",
    "    product_analysis[\"precomputed_reco_coverage\"] = (\n",
    "        reco_with_flag\n",
    "        .agg(\n",
    "            F.countDistinct(\"product_a_id\").alias(\"total_products\"),\n",
    "            F.sum(\"has_recommendations\").alias(\"products_with_recommendations\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"coverage_rate\",\n",
    "            F.when(\n",
    "                F.col(\"total_products\") > 0,\n",
    "                F.col(\"products_with_recommendations\") / F.col(\"total_products\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"One of the required columns in agg_product_recommendations is all NULL or zero; \"\n",
    "        \"skipping precomputed product recommendations analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1deec7",
   "metadata": {},
   "source": [
    "Campaign effectiveness by product/category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "23229f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# 1) Campaign Performance Summary\n",
    "if (\n",
    "    \"agg_marketing_campaigns\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_marketing_campaigns\"], \"campaign_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_marketing_campaigns\"], \"revenue_generated\")\n",
    "):\n",
    "    analysis[\"campaign_performance_summary\"] = (\n",
    "        dataframes[\"agg_marketing_campaigns\"]\n",
    "        .select(\n",
    "            \"campaign_id\", \"campaign_name\", \"campaign_type\", \"start_date\", \"end_date\", \"campaign_status\",\n",
    "            \"impressions\", \"clicks\", \"orders_from_campaign\", \"revenue_generated\", \"avg_order_value\",\n",
    "            \"conversion_rate\", \"roas\", \"roi\", \"campaign_efficiency_score\", \"spent_amount\", \"budget\"\n",
    "        )\n",
    "        .fillna({\n",
    "            \"impressions\": 0, \"clicks\": 0, \"orders_from_campaign\": 0, \"revenue_generated\": 0.0,\n",
    "            \"avg_order_value\": 0.0, \"conversion_rate\": 0.0, \"roas\": 0.0, \"roi\": 0.0,\n",
    "            \"campaign_efficiency_score\": 0.0, \"spent_amount\": 0.0, \"budget\": 0.0\n",
    "        })\n",
    "        .withColumn(\"revenue_per_click\", F.when(F.col(\"clicks\") > 0, F.col(\"revenue_generated\") / F.col(\"clicks\")).otherwise(0.0))\n",
    "        .withColumn(\"roi_per_spend\", F.when(F.col(\"spent_amount\") > 0, F.col(\"revenue_generated\") / F.col(\"spent_amount\")).otherwise(None))\n",
    "        .orderBy(F.col(\"revenue_generated\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "# 2) Find Campaign Attribution Column\n",
    "campaign_order_col = None\n",
    "if \"agg_orders\" in dataframes: \n",
    "    for c in [\"campaign_id\", \"marketing_campaign_id\", \"attributed_campaign_id\", \"utm_campaign\", \"campaign\"]:\n",
    "        if c in dataframes[\"agg_orders\"].columns and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], c):\n",
    "            campaign_order_col = c\n",
    "            break\n",
    "\n",
    "# Calculate Line Revenue\n",
    "line_revenue_col = None\n",
    "if \"agg_order_items\" in dataframes:\n",
    "    for c in [\"line_total\", \"line_revenue\", \"line_price\", \"extended_price\"]:\n",
    "        if c in dataframes[\"agg_order_items\"]. columns and not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], c):\n",
    "            line_revenue_col = c\n",
    "            break\n",
    "    \n",
    "    if line_revenue_col is None and \"product_cost\" in dataframes[\"agg_order_items\"].columns:\n",
    "        if not is_column_all_null_or_zero(dataframes[\"agg_order_items\"], \"product_cost\"):\n",
    "            dataframes[\"agg_order_items\"] = dataframes[\"agg_order_items\"].withColumn(\n",
    "                \"calculated_line_revenue\", F.col(\"quantity\") * F.col(\"product_cost\")\n",
    "            )\n",
    "            line_revenue_col = \"calculated_line_revenue\"\n",
    "\n",
    "# 3) Campaign-Product Contribution\n",
    "if (\n",
    "    campaign_order_col is not None\n",
    "    and \"agg_orders\" in dataframes\n",
    "    and \"agg_order_items\" in dataframes\n",
    "    and \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "):\n",
    "    orders_with_campaign = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .select(\"order_id\", \"customer_id\", campaign_order_col, \"total_amount\")\n",
    "        .withColumnRenamed(campaign_order_col, \"campaign_id_attributed\")\n",
    "        .filter(F.col(\"campaign_id_attributed\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    order_items_cols = [\"order_id\", \"product_id\", \"quantity\"]\n",
    "    if line_revenue_col: \n",
    "        order_items_cols. append(line_revenue_col)\n",
    "    \n",
    "    order_product_campaign = (\n",
    "        dataframes[\"agg_order_items\"]. select(*order_items_cols)\n",
    "        .join(orders_with_campaign, on=\"order_id\", how=\"inner\")\n",
    "        .join(\n",
    "            dataframes[\"agg_products\"]. select(\"product_id\", \"product_name\", \"category\", \"sub_category\", \"brand\", \"profit_margin\"),\n",
    "            on=\"product_id\", how=\"left\"\n",
    "        )\n",
    "        .withColumn(\"campaign_id\", F.col(\"campaign_id_attributed\"))\n",
    "    )\n",
    "    \n",
    "    if line_revenue_col:\n",
    "        campaign_product_agg = (\n",
    "            order_product_campaign\n",
    "            .groupBy(\"campaign_id\", \"product_id\", \"product_name\", \"category\", \"sub_category\", \"brand\")\n",
    "            .agg(\n",
    "                F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "                F.sum(line_revenue_col).alias(\"product_revenue\"),\n",
    "                F.countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "                F.avg(\"profit_margin\").alias(\"avg_product_margin\")\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        product_campaign_units = (\n",
    "            order_product_campaign\n",
    "            .groupBy(\"campaign_id\", \"product_id\", \"product_name\", \"category\", \"sub_category\", \"brand\")\n",
    "            .agg(\n",
    "                F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "                F. countDistinct(\"order_id\").alias(\"orders_count\"),\n",
    "                F.avg(\"profit_margin\").alias(\"avg_product_margin\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        campaign_revenue = (\n",
    "            orders_with_campaign\n",
    "            .groupBy(\"campaign_id_attributed\")\n",
    "            .agg(F.sum(\"total_amount\").alias(\"campaign_revenue\"))\n",
    "            .withColumnRenamed(\"campaign_id_attributed\", \"campaign_id\")\n",
    "        )\n",
    "        \n",
    "        campaign_product_agg = (\n",
    "            product_campaign_units\n",
    "            .join(campaign_revenue, on=\"campaign_id\", how=\"left\")\n",
    "            .withColumn(\"product_revenue\", F.lit(None).cast(\"double\"))\n",
    "        )\n",
    "    \n",
    "    if (\n",
    "        \"agg_marketing_campaigns\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_marketing_campaigns\"], \"campaign_id\")\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_marketing_campaigns\"], \"revenue_generated\")\n",
    "    ):\n",
    "        campaign_totals = (\n",
    "            dataframes[\"agg_marketing_campaigns\"]\n",
    "            .select(\"campaign_id\", F.col(\"revenue_generated\").alias(\"campaign_revenue_marketing\"))\n",
    "        )\n",
    "        campaign_product_agg = (\n",
    "            campaign_product_agg\n",
    "            . join(campaign_totals, on=\"campaign_id\", how=\"left\")\n",
    "            .withColumn(\"campaign_revenue\", F.coalesce(F.col(\"campaign_revenue_marketing\"), F.col(\"campaign_revenue\")))\n",
    "            .drop(\"campaign_revenue_marketing\")\n",
    "        )\n",
    "    else:\n",
    "        if \"campaign_revenue\" not in campaign_product_agg.columns:\n",
    "            campaign_product_agg = campaign_product_agg.withColumn(\"campaign_revenue\", F.lit(None).cast(\"double\"))\n",
    "    \n",
    "    campaign_product_agg = (\n",
    "        campaign_product_agg\n",
    "        . withColumn(\n",
    "            \"product_revenue_share\",\n",
    "            F.when(\n",
    "                (F.col(\"product_revenue\").isNotNull()) & (F.col(\"campaign_revenue\").isNotNull()) & (F.col(\"campaign_revenue\") > 0),\n",
    "                F.col(\"product_revenue\") / F.col(\"campaign_revenue\")\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "        .fillna({\"units_sold\": 0, \"orders_count\": 0, \"product_revenue\": 0.0, \"campaign_revenue\": 0.0, \"avg_product_margin\": 0.0})\n",
    "    )\n",
    "    \n",
    "    analysis[\"campaign_product_contribution\"] = campaign_product_agg. orderBy(\n",
    "        F.col(\"campaign_revenue\").desc_nulls_last(),\n",
    "        F.col(\"product_revenue\").desc_nulls_last()\n",
    "    )\n",
    "\n",
    "# 4) Campaign-Customer LTV\n",
    "if (\n",
    "    campaign_order_col is not None\n",
    "    and \"agg_orders\" in dataframes\n",
    "    and \"agg_customers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_lifetime_value\")\n",
    "):\n",
    "    orders_with_campaign = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .select(\"order_id\", \"customer_id\", campaign_order_col, \"total_amount\")\n",
    "        .withColumnRenamed(campaign_order_col, \"campaign_id_attributed\")\n",
    "        .filter(F.col(\"campaign_id_attributed\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    orders_customers = orders_with_campaign.join(\n",
    "        dataframes[\"agg_customers\"]. select(\"customer_id\", \"customer_lifetime_value\"),\n",
    "        on=\"customer_id\", how=\"left\"\n",
    "    )\n",
    "    \n",
    "    analysis[\"campaign_ltv\"] = (\n",
    "        orders_customers\n",
    "        .groupBy(\"campaign_id_attributed\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"customer_id\").alias(\"distinct_customers\"),\n",
    "            F.sum(\"total_amount\").alias(\"campaign_revenue_from_orders\"),\n",
    "            F.avg(\"customer_lifetime_value\").alias(\"avg_customer_lifetime_value\"),\n",
    "            F.sum(F.when(F.col(\"customer_lifetime_value\").isNotNull(), 1).otherwise(0)).alias(\"num_customers_with_clv\")\n",
    "        )\n",
    "        .withColumnRenamed(\"campaign_id_attributed\", \"campaign_id\")\n",
    "        .fillna({\"distinct_customers\": 0, \"campaign_revenue_from_orders\": 0.0, \"avg_customer_lifetime_value\": 0.0, \"num_customers_with_clv\": 0})\n",
    "        .orderBy(F.col(\"avg_customer_lifetime_value\").desc_nulls_last())\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        high_clv_cutoff = dataframes[\"agg_customers\"].approxQuantile(\"customer_lifetime_value\", [0.9], 0.01)[0]\n",
    "        \n",
    "        if high_clv_cutoff and high_clv_cutoff > 0:\n",
    "            high_clv_share = (\n",
    "                orders_customers\n",
    "                .withColumn(\"is_high_clv\", F. when(F.col(\"customer_lifetime_value\") >= high_clv_cutoff, 1).otherwise(0))\n",
    "                .groupBy(\"campaign_id_attributed\")\n",
    "                .agg(\n",
    "                    F.sum(\"is_high_clv\").alias(\"high_clv_customers\"),\n",
    "                    F.countDistinct(\"customer_id\").alias(\"distinct_customers\")\n",
    "                )\n",
    "                .withColumnRenamed(\"campaign_id_attributed\", \"campaign_id\")\n",
    "                .withColumn(\"high_clv_share\", F.when(F.col(\"distinct_customers\") > 0, F.col(\"high_clv_customers\") / F.col(\"distinct_customers\")).otherwise(0.0))\n",
    "                .fillna({\"high_clv_customers\":  0, \"distinct_customers\":  0, \"high_clv_share\":  0.0})\n",
    "            )\n",
    "            \n",
    "            analysis[\"campaign_ltv\"] = (\n",
    "                analysis[\"campaign_ltv\"]\n",
    "                .join(high_clv_share. select(\"campaign_id\", \"high_clv_customers\", \"high_clv_share\"), on=\"campaign_id\", how=\"left\")\n",
    "                .fillna({\"high_clv_customers\": 0, \"high_clv_share\": 0.0})\n",
    "            )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    analysis[\"campaign_customer_ltv_summary\"] = analysis[\"campaign_ltv\"]\n",
    "\n",
    "# 5) Wasteful Campaigns\n",
    "if \"campaign_performance_summary\" in analysis: \n",
    "    cps = analysis[\"campaign_performance_summary\"]\n",
    "    \n",
    "    if \"clicks\" in cps.columns and \"revenue_generated\" in cps. columns:\n",
    "        if not is_column_all_null_or_zero(cps, \"clicks\") and not is_column_all_null_or_zero(cps, \"revenue_generated\"):\n",
    "            analysis[\"campaign_wasteful_campaigns\"] = (\n",
    "                cps\n",
    "                .withColumn(\"revenue_per_click_recalc\", F.when(F.col(\"clicks\") > 0, F.col(\"revenue_generated\") / F.col(\"clicks\")).otherwise(0.0))\n",
    "                .withColumn(\"is_low_efficiency\", F.when(F.col(\"revenue_per_click_recalc\") < 1.0, 1).otherwise(0))\n",
    "                .orderBy(F.col(\"revenue_per_click_recalc\").asc_nulls_last(), F.col(\"clicks\").desc_nulls_last())\n",
    "            )\n",
    "\n",
    "# 6) Campaign Margin Profile\n",
    "if \"campaign_product_contribution\" in analysis:\n",
    "    cpc = analysis[\"campaign_product_contribution\"]\n",
    "    \n",
    "    if \"avg_product_margin\" in cpc.columns and not is_column_all_null_or_zero(cpc, \"avg_product_margin\"):\n",
    "        analysis[\"campaign_margin_profile\"] = (\n",
    "            cpc\n",
    "            . groupBy(\"campaign_id\")\n",
    "            .agg(\n",
    "                F.avg(\"avg_product_margin\").alias(\"campaign_avg_product_margin\"),\n",
    "                F.sum(\"product_revenue\").alias(\"campaign_products_revenue\"),\n",
    "                F.sum(\"units_sold\").alias(\"campaign_units_sold\")\n",
    "            )\n",
    "            .fillna({\"campaign_avg_product_margin\": 0.0, \"campaign_products_revenue\": 0.0, \"campaign_units_sold\": 0})\n",
    "            .orderBy(F.col(\"campaign_products_revenue\").desc_nulls_last())\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61f808",
   "metadata": {},
   "source": [
    "# Inventory Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9249df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"current_stock\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"available_stock\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"minimum_stock_level\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"reorder_point_breach_count\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"avg_daily_sales\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"days_of_supply\")\n",
    "):\n",
    "    inv_src = dataframes[\"agg_product_inventory_health\"]\n",
    "    inv = inv_src.fillna(\n",
    "        {\n",
    "            \"current_stock\": 0,\n",
    "            \"available_stock\": 0,\n",
    "            \"minimum_stock_level\": 0,\n",
    "            \"reorder_point_breach_count\": 0,\n",
    "            \"avg_daily_sales\": 0.0,\n",
    "            # \"days_of_supply\": None,  # REMOVE this line\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # If days_of_supply missing, approximate from avg_daily_sales\n",
    "    if \"days_of_supply\" not in inv.columns:\n",
    "        inv = inv.withColumn(\n",
    "            \"days_of_supply\",\n",
    "            F.when(F.col(\"avg_daily_sales\") > 0,\n",
    "                   F.col(\"available_stock\") / F.col(\"avg_daily_sales\"))\n",
    "             .otherwise(F.lit(None)),\n",
    "        )\n",
    "\n",
    "    # Compute stock_status_computed using business rules:\n",
    "   \n",
    "    inv = inv.withColumn(\n",
    "        \"stock_status_computed\",\n",
    "        F.when(\n",
    "            (F.col(\"available_stock\") <= 0)\n",
    "            | (F.col(\"reorder_point_breach_count\") > 0)\n",
    "            | (F.col(\"days_of_supply\").isNotNull() & (F.col(\"days_of_supply\") < 7)),\n",
    "            F.lit(\"critical\"),\n",
    "        )\n",
    "        .when(\n",
    "            (F.col(\"days_of_supply\").isNotNull() & (F.col(\"days_of_supply\") < 30))\n",
    "            | (\n",
    "                F.col(\"minimum_stock_level\").isNotNull()\n",
    "                & (F.col(\"available_stock\") <= F.col(\"minimum_stock_level\") * 1.5)\n",
    "            ),\n",
    "            F.lit(\"low\"),\n",
    "        )\n",
    "        .otherwise(F.lit(\"healthy\")),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"inventory_stock_status\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"supplier_id\"] if c in inv.columns],\n",
    "            *[c for c in [\"available_stock\", \"current_stock\", \"minimum_stock_level\"] if c in inv.columns],\n",
    "            \"days_of_supply\",\n",
    "            \"stock_status_computed\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"stock_status_computed\").asc(),\n",
    "            F.col(\"available_stock\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    inv_src = dataframes[\"agg_inventory\"]\n",
    "    inv = inv_src.fillna(\n",
    "        {\n",
    "            \"stock_quantity\": 0,\n",
    "            \"available_stock\": 0,\n",
    "            \"minimum_stock_level\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # No days_of_supply here, so we classify purely on levels and minimum_stock_level\n",
    "    inv = inv.withColumn(\n",
    "        \"stock_status_computed\",\n",
    "        F.when(\n",
    "            (F.col(\"available_stock\") <= 0)\n",
    "            | (F.col(\"reorder_point_breach\") == 1),\n",
    "            F.lit(\"critical\"),\n",
    "        )\n",
    "        .when(\n",
    "            F.col(\"available_stock\") <= F.col(\"minimum_stock_level\") * 1.5,\n",
    "            F.lit(\"low\"),\n",
    "        )\n",
    "        .otherwise(F.lit(\"healthy\")),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"inventory_stock_status\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            \"available_stock\",\n",
    "            \"stock_quantity\",\n",
    "            \"minimum_stock_level\",\n",
    "            \"reorder_point_breach\",\n",
    "            \"stock_status_computed\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"stock_status_computed\").asc(),\n",
    "            F.col(\"available_stock\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping stock status analysis.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4c286",
   "metadata": {},
   "source": [
    "Days of supply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f9d0e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days of supply\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"current_stock\": 0,\n",
    "            \"available_stock\": 0,\n",
    "            \"avg_daily_sales\": 0.0,\n",
    "            # \"days_of_supply\": None,  # REMOVE this line\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # If days_of_supply already exists in the table, respect it.\n",
    "    if \"days_of_supply\" not in inv.columns:\n",
    "        inv = inv.withColumn(\n",
    "            \"days_of_supply\",\n",
    "            F.when(\n",
    "                F.col(\"avg_daily_sales\") > 0,\n",
    "                F.col(\"available_stock\") / F.col(\"avg_daily_sales\"),\n",
    "            ).otherwise(F.lit(None)),\n",
    "        )\n",
    "\n",
    "    product_analysis[\"days_of_supply\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"supplier_id\"] if c in inv.columns],\n",
    "            \"available_stock\",\n",
    "            \"avg_daily_sales\",\n",
    "            \"days_of_supply\",\n",
    "        )\n",
    "        .orderBy(F.col(\"days_of_supply\").asc_nulls_last())\n",
    "    )\n",
    "\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    # Fallback: approximate avg_daily_sales from total_sold and last_restocked_date (or stock_turnover_ratio)\n",
    "    inv = dataframes[\"agg_inventory\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"stock_quantity\": 0,\n",
    "            \"total_sold\": 0,\n",
    "            \"avg_inventory\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Try to derive avg_daily_sales:\n",
    "    # 1) If stock_turnover_ratio exists: approx annual_turnover / 365 * avg_inventory -> daily_sales\n",
    "    # 2) Else, if last_restocked_date exists: total_sold / days_since_restock\n",
    "    if \"avg_daily_sales\" in inv.columns and not is_column_all_null_or_zero(inv, \"avg_daily_sales\"):\n",
    "        inv = inv\n",
    "    else:\n",
    "        if \"stock_turnover_ratio\" in inv.columns and not is_column_all_null_or_zero(inv, \"stock_turnover_ratio\"):\n",
    "            inv = inv.withColumn(\n",
    "                \"avg_daily_sales\",\n",
    "                F.col(\"stock_turnover_ratio\") * F.col(\"avg_inventory\") / F.lit(365.0),\n",
    "            )\n",
    "        elif \"last_restocked_date\" in inv.columns and not is_column_all_null_or_zero(inv, \"last_restocked_date\"):\n",
    "            inv = inv.withColumn(\n",
    "                \"days_since_restock\",\n",
    "                F.datediff(F.current_date(), F.col(\"last_restocked_date\")),\n",
    "            ).withColumn(\n",
    "                \"avg_daily_sales\",\n",
    "                F.when(\n",
    "                    F.col(\"days_since_restock\") > 0,\n",
    "                    F.col(\"total_sold\") / F.col(\"days_since_restock\"),\n",
    "                ).otherwise(F.lit(0.0)),\n",
    "            )\n",
    "        else:\n",
    "            inv = inv.withColumn(\"avg_daily_sales\", F.lit(0.0))\n",
    "\n",
    "    inv = inv.withColumn(\n",
    "        \"days_of_supply\",\n",
    "        F.when(\n",
    "            F.col(\"avg_daily_sales\") > 0,\n",
    "            F.col(\"available_stock\") / F.col(\"avg_daily_sales\"),\n",
    "        ).otherwise(F.lit(None)),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"days_of_supply\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            \"available_stock\",\n",
    "            \"stock_quantity\",\n",
    "            \"avg_daily_sales\",\n",
    "            \"days_of_supply\",\n",
    "        )\n",
    "        .orderBy(F.col(\"days_of_supply\").asc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping days of supply analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81f813",
   "metadata": {},
   "source": [
    "SKU-level reorder urgency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "63d3d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKU-level reorder urgency\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(\n",
    "        dataframes[\"agg_product_inventory_health\"], \"product_id\"\n",
    "    )\n",
    "):\n",
    "    # Do NOT use None in fillna; only numeric defaults\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"current_stock\": 0,\n",
    "            \"minimum_stock_level\": 0,\n",
    "            \"reorder_point_breach_count\": 0,\n",
    "            \"avg_daily_sales\": 0.0,\n",
    "            # \"days_of_supply\": None,  # <-- remove this, Spark can't handle None here\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Ensure days_of_supply exists / is populated\n",
    "    if (\n",
    "        \"days_of_supply\" not in inv.columns\n",
    "        or is_column_all_null_or_zero(inv, \"days_of_supply\")\n",
    "    ):\n",
    "        inv = inv.withColumn(\n",
    "            \"days_of_supply\",\n",
    "            F.when(\n",
    "                F.col(\"avg_daily_sales\") > 0,\n",
    "                F.col(\"available_stock\") / F.col(\"avg_daily_sales\"),\n",
    "            ).otherwise(F.lit(None)),\n",
    "        )\n",
    "\n",
    "    # Reorder urgency score\n",
    "    inv = inv.withColumn(\n",
    "        \"reorder_urgency_score\",\n",
    "        # Strong penalty for breach\n",
    "        F.when(F.col(\"reorder_point_breach_count\") > 0, 100).otherwise(0)\n",
    "        # Low days of supply\n",
    "        + F.when(\n",
    "            F.col(\"days_of_supply\").isNotNull(),\n",
    "            F.when(F.col(\"days_of_supply\") < 7, 50)\n",
    "            .when(F.col(\"days_of_supply\") < 30, 20)\n",
    "            .otherwise(0),\n",
    "        ).otherwise(10)\n",
    "        # Available stock below minimum\n",
    "        + F.when(\n",
    "            (F.col(\"minimum_stock_level\") > 0)\n",
    "            & (F.col(\"available_stock\") <= F.col(\"minimum_stock_level\")),\n",
    "            30,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    inv = inv.withColumn(\n",
    "        \"reorder_urgency_tier\",\n",
    "        F.when(F.col(\"reorder_urgency_score\") >= 100, \"urgent\")\n",
    "        .when(F.col(\"reorder_urgency_score\") >= 30, \"high\")\n",
    "        .when(F.col(\"reorder_urgency_score\") >= 10, \"medium\")\n",
    "        .otherwise(\"low\"),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"sku_reorder_urgency\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\"] if c in inv.columns],\n",
    "            \"available_stock\",\n",
    "            \"current_stock\",\n",
    "            \"minimum_stock_level\",\n",
    "            \"reorder_point_breach_count\",\n",
    "            \"days_of_supply\",\n",
    "            \"reorder_urgency_score\",\n",
    "            \"reorder_urgency_tier\",\n",
    "        ).orderBy(F.col(\"reorder_urgency_score\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_inventory\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"stock_quantity\": 0,\n",
    "            \"minimum_stock_level\": 0,\n",
    "            \"reorder_point_breach\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # No days_of_supply here, just level vs minimum + breach\n",
    "    inv = inv.withColumn(\n",
    "        \"reorder_urgency_score\",\n",
    "        F.when(F.col(\"reorder_point_breach\") == 1, 100).otherwise(0)\n",
    "        + F.when(\n",
    "            (F.col(\"minimum_stock_level\") > 0)\n",
    "            & (F.col(\"available_stock\") <= F.col(\"minimum_stock_level\")),\n",
    "            40,\n",
    "        ).otherwise(0)\n",
    "        + F.when(\n",
    "            (F.col(\"minimum_stock_level\") > 0)\n",
    "            & (F.col(\"available_stock\") <= F.col(\"minimum_stock_level\") * 1.5),\n",
    "            20,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    inv = inv.withColumn(\n",
    "        \"reorder_urgency_tier\",\n",
    "        F.when(F.col(\"reorder_urgency_score\") >= 100, \"urgent\")\n",
    "        .when(F.col(\"reorder_urgency_score\") >= 30, \"high\")\n",
    "        .when(F.col(\"reorder_urgency_score\") >= 10, \"medium\")\n",
    "        .otherwise(\"low\"),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"sku_reorder_urgency\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            \"available_stock\",\n",
    "            \"stock_quantity\",\n",
    "            \"minimum_stock_level\",\n",
    "            \"reorder_point_breach\",\n",
    "            \"reorder_urgency_score\",\n",
    "            \"reorder_urgency_tier\",\n",
    "        ).orderBy(F.col(\"reorder_urgency_score\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping SKU-level reorder urgency analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16dc8",
   "metadata": {},
   "source": [
    "Reorder Point Breach Frequency: Products frequently hitting reorder_point_breach_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3708713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Reorder Point Breach Frequency: products frequently breaching reorder point\n",
    "# -------------------------------------------------------\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    # 1) Base DF: DO NOT use None in fillna, only scalars\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"reorder_point_breach_count\": 0,\n",
    "            \"current_stock\": 0,\n",
    "            \"available_stock\": 0,\n",
    "            \"stock_health_score\": 0,\n",
    "            # don't touch days_of_supply / reorder_urgency here; leave them as-is\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2) Optional enrich with product metadata (NO stock_status to avoid ambiguity)\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\"\n",
    "            # skip agg_products.stock_status on purpose\n",
    "        )\n",
    "        inv = inv.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # 3) Threshold for \"frequent\" breaches\n",
    "    BREACH_THRESHOLD = 3\n",
    "\n",
    "    # 4) Final selection – simple, explicit\n",
    "    #    (use stock_status from agg_product_inventory_health only)\n",
    "    product_analysis[\"reorder_point_breach_frequency\"] = (\n",
    "        inv.filter(F.col(\"reorder_point_breach_count\") >= BREACH_THRESHOLD)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"supplier_id\",\n",
    "            \"reorder_point_breach_count\",\n",
    "            \"stock_health_score\",\n",
    "            \"reorder_urgency\",\n",
    "            \"current_stock\",\n",
    "            \"available_stock\",\n",
    "            \"days_of_supply\",\n",
    "            \"stock_status\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"reorder_point_breach_count\").desc_nulls_last(),\n",
    "            F.col(\"stock_health_score\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Fallback: use agg_inventory if agg_product_inventory_health not available\n",
    "# -------------------------------------------------------\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_inventory\"].fillna(\n",
    "        {\n",
    "            \"reorder_point_breach\": 0,\n",
    "            \"stock_quantity\": 0,\n",
    "            \"available_stock\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Aggregate breaches per product/supplier\n",
    "    breaches_agg = (\n",
    "        inv.groupBy(\"product_id\", \"supplier_id\")\n",
    "        .agg(\n",
    "            F.sum(\"reorder_point_breach\").alias(\"reorder_point_breach_count\"),\n",
    "            F.max(\"stock_quantity\").alias(\"latest_stock_quantity\"),\n",
    "            F.max(\"available_stock\").alias(\"latest_available_stock\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional enrich with product metadata (again, skip stock_status to keep it simple)\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        breaches_agg = breaches_agg.alias(\"b\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    BREACH_THRESHOLD = 3\n",
    "\n",
    "    product_analysis[\"reorder_point_breach_frequency\"] = (\n",
    "        breaches_agg.filter(F.col(\"reorder_point_breach_count\") >= BREACH_THRESHOLD)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"supplier_id\",\n",
    "            \"reorder_point_breach_count\",\n",
    "            \"latest_stock_quantity\",\n",
    "            \"latest_available_stock\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .orderBy(F.col(\"reorder_point_breach_count\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping reorder point breach frequency analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db039e",
   "metadata": {},
   "source": [
    "Overstock Analysis: Products with stock_health_score < 40 and high storage_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "21bab18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "\n",
    "# Overstock Analysis: low stock_health_score & high storage cost\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    # 1) Base inventory DF – do NOT use None in fillna\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"stock_health_score\": 100,\n",
    "            \"storage_cost\": 0.0,\n",
    "            \"storage_cost_per_unit\": 0.0,\n",
    "            \"current_stock\": 0,\n",
    "            \"available_stock\": 0,\n",
    "            # leave days_of_supply as-is (don’t fill with None)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 2) Optional enrich with product metadata\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\"\n",
    "            # skip agg_products.stock_status to avoid ambiguity\n",
    "        )\n",
    "        inv = inv.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # 3) Thresholds\n",
    "    LOW_HEALTH_THRESHOLD = 40          # stock_health_score < 40\n",
    "    HIGH_STORAGE_COST = 1000.0         # absolute storage cost\n",
    "    HIGH_STORAGE_COST_PER_UNIT = 1.0   # per-unit storage cost\n",
    "\n",
    "    # 4) Flag overstock\n",
    "    overstock = (\n",
    "        inv.withColumn(\n",
    "            \"is_overstock\",\n",
    "            F.when(\n",
    "                (F.col(\"stock_health_score\") < LOW_HEALTH_THRESHOLD)\n",
    "                & (\n",
    "                    (F.col(\"storage_cost\") >= HIGH_STORAGE_COST)\n",
    "                    | (F.col(\"storage_cost_per_unit\") >= HIGH_STORAGE_COST_PER_UNIT)\n",
    "                ),\n",
    "                1,\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .filter(F.col(\"is_overstock\") == 1)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"supplier_id\",\n",
    "            \"stock_health_score\",\n",
    "            \"storage_cost\",\n",
    "            \"storage_cost_per_unit\",\n",
    "            \"current_stock\",\n",
    "            \"available_stock\",\n",
    "            \"days_of_supply\",\n",
    "            \"stock_status\",   # from agg_product_inventory_health\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"storage_cost\").desc_nulls_last(),\n",
    "            F.col(\"storage_cost_per_unit\").desc_nulls_last(),\n",
    "            F.col(\"stock_health_score\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"overstock_analysis\"] = overstock\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping overstock analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c019f1",
   "metadata": {},
   "source": [
    "Storage Cost Efficiency: total_storage_cost / total_available_stock by supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3c532876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"supplier_analysis\" not in locals():\n",
    "    supplier_analysis = {}\n",
    "\n",
    "# Storage Cost Efficiency: total_storage_cost / total_available_stock by supplier\n",
    "if (\n",
    "    \"agg_supplier_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_supplier_inventory_health\"], \"supplier_id\")\n",
    "):\n",
    "    sup_inv = dataframes[\"agg_supplier_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"total_storage_cost\": 0.0,\n",
    "            \"total_available_stock\": 0,\n",
    "            \"total_current_stock\": 0,\n",
    "            \"avg_storage_cost_per_unit\": 0.0,\n",
    "            \"supplier_inventory_health_score\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    efficiency = (\n",
    "        sup_inv.withColumn(\n",
    "            \"storage_cost_efficiency\",\n",
    "            F.when(\n",
    "                F.col(\"total_available_stock\") > 0,\n",
    "                F.col(\"total_storage_cost\") / F.col(\"total_available_stock\")\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .select(\n",
    "            \"supplier_id\",\n",
    "            \"total_storage_cost\",\n",
    "            \"total_available_stock\",\n",
    "            \"total_current_stock\",\n",
    "            \"avg_storage_cost_per_unit\",\n",
    "            \"supplier_inventory_health_score\",\n",
    "            \"storage_cost_efficiency\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"storage_cost_efficiency\").desc_nulls_last(),\n",
    "            F.col(\"total_storage_cost\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    supplier_analysis[\"storage_cost_efficiency_by_supplier\"] = efficiency\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_supplier_inventory_health not available, or supplier_id column is all NULL/zero; \"\n",
    "        \"skipping storage cost efficiency by supplier analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3c431",
   "metadata": {},
   "source": [
    "Inventory Carrying Cost: Total storage_cost across all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "73ca5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "if \"supplier_analysis\" not in locals():\n",
    "    supplier_analysis = {}\n",
    "\n",
    "# Primary: product-level storage_cost in agg_product_inventory_health\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"storage_cost\": 0.0,\n",
    "            \"storage_cost_per_unit\": 0.0,\n",
    "            \"available_stock\": 0,\n",
    "            \"current_stock\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional enrich with product/category\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "        )\n",
    "        inv = inv.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # 1) Overall inventory carrying cost (total storage_cost)\n",
    "    analysis[\"inventory_carrying_cost_overall\"] = inv.agg(\n",
    "        F.sum(\"storage_cost\").alias(\"total_inventory_carrying_cost\")\n",
    "    )\n",
    "\n",
    "    # 2) By supplier\n",
    "    supplier_analysis[\"inventory_carrying_cost_by_supplier\"] = (\n",
    "        inv.groupBy(\"supplier_id\")\n",
    "        .agg(\n",
    "            F.sum(\"storage_cost\").alias(\"total_storage_cost\"),\n",
    "            F.sum(\"available_stock\").alias(\"total_available_stock\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"total_storage_cost\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # 3) By product/category\n",
    "    product_cols = [\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"category\",\n",
    "        \"supplier_id\",\n",
    "        \"storage_cost\",\n",
    "        \"available_stock\",\n",
    "        \"current_stock\",\n",
    "        \"storage_cost_per_unit\",\n",
    "    ]\n",
    "    product_analysis[\"inventory_carrying_cost_by_product\"] = (\n",
    "        inv.select(*[c for c in product_cols if c in inv.columns])\n",
    "        .orderBy(F.col(\"storage_cost\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "# Fallback: supplier-level total_storage_cost in agg_supplier_inventory_health\n",
    "elif (\n",
    "    \"agg_supplier_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_supplier_inventory_health\"], \"supplier_id\")\n",
    "):\n",
    "    sup_inv = dataframes[\"agg_supplier_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"total_storage_cost\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    analysis[\"inventory_carrying_cost_overall\"] = sup_inv.agg(\n",
    "        F.sum(\"total_storage_cost\").alias(\"total_inventory_carrying_cost\")\n",
    "    )\n",
    "\n",
    "    supplier_analysis[\"inventory_carrying_cost_by_supplier\"] = (\n",
    "        sup_inv.select(\n",
    "            \"supplier_id\",\n",
    "            \"total_storage_cost\",\n",
    "            \"total_available_stock\",\n",
    "            \"total_current_stock\",\n",
    "        )\n",
    "        .orderBy(F.col(\"total_storage_cost\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_supplier_inventory_health not available, \"\n",
    "        \"or key id columns all NULL/zero; skipping inventory carrying cost analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcb664",
   "metadata": {},
   "source": [
    "Margin Erosion Risk: Products with increasing storage_cost relative to sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "83d2aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "\n",
    "# Margin Erosion Risk: products with high storage cost relative to sales\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"storage_cost\": 0.0,\n",
    "            \"storage_cost_per_unit\": 0.0,\n",
    "            \"available_stock\": 0,\n",
    "            \"current_stock\": 0,\n",
    "            \"stock_health_score\": 100,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional enrich with product sales metrics\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_order_value_product\",\n",
    "        )\n",
    "        inv = inv.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "    else:\n",
    "        prod = None\n",
    "\n",
    "    # Ratios indicating margin pressure\n",
    "    inv = (\n",
    "        inv\n",
    "        # storage_cost / total_revenue (how much of revenue is being eaten by storage)\n",
    "        .withColumn(\n",
    "            \"storage_cost_to_revenue\",\n",
    "            F.when(\n",
    "                F.col(\"total_revenue\") > 0,\n",
    "                F.col(\"storage_cost\") / F.col(\"total_revenue\"),\n",
    "            ).otherwise(F.lit(None)),\n",
    "        )\n",
    "        # storage_cost_per_unit / avg_order_value_product (per-unit storage vs price)\n",
    "        .withColumn(\n",
    "            \"storage_cost_per_unit_to_price\",\n",
    "            F.when(\n",
    "                F.col(\"avg_order_value_product\") > 0,\n",
    "                F.col(\"storage_cost_per_unit\") / F.col(\"avg_order_value_product\"),\n",
    "            ).otherwise(F.lit(None)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Thresholds for \"at risk\" (tune to your business):\n",
    "    # - storage cost > 10% of revenue OR\n",
    "    # - per-unit storage > 20% of unit selling price\n",
    "    STORAGE_TO_REV_THRESHOLD = 0.10\n",
    "    STORAGE_PER_UNIT_TO_PRICE_THRESHOLD = 0.20\n",
    "\n",
    "    risk_df = (\n",
    "        inv.withColumn(\n",
    "            \"margin_erosion_risk\",\n",
    "            F.when(\n",
    "                (\n",
    "                    (F.col(\"storage_cost_to_revenue\") >= STORAGE_TO_REV_THRESHOLD)\n",
    "                    | (F.col(\"storage_cost_per_unit_to_price\") >= STORAGE_PER_UNIT_TO_PRICE_THRESHOLD)\n",
    "                )\n",
    "                # And stock is non-trivial (otherwise cost is negligible)\n",
    "                & (F.col(\"available_stock\") > 0),\n",
    "                1,\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .filter(F.col(\"margin_erosion_risk\") == 1)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\"] if c in inv.columns],\n",
    "            \"supplier_id\",\n",
    "            \"storage_cost\",\n",
    "            \"storage_cost_per_unit\",\n",
    "            \"available_stock\",\n",
    "            \"current_stock\",\n",
    "            \"stock_health_score\",\n",
    "            *[c for c in [\"total_units_sold\", \"total_revenue\", \"avg_order_value_product\"] if c in inv.columns],\n",
    "            \"storage_cost_to_revenue\",\n",
    "            \"storage_cost_per_unit_to_price\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"storage_cost_to_revenue\").desc_nulls_last(),\n",
    "            F.col(\"storage_cost_per_unit_to_price\").desc_nulls_last(),\n",
    "            F.col(\"storage_cost\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"margin_erosion_risk\"] = risk_df\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping margin erosion risk analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de87a8",
   "metadata": {},
   "source": [
    "Reserved vs available stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b63aae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved vs available stock\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(\n",
    "        dataframes[\"agg_product_inventory_health\"], \"product_id\"\n",
    "    )\n",
    "):\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"reserved_quantity\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    product_analysis[\"reserved_vs_available\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\", \"supplier_id\"] if c in inv.columns],\n",
    "            \"available_stock\",\n",
    "            \"reserved_quantity\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"total_stock\",\n",
    "            F.col(\"available_stock\") + F.col(\"reserved_quantity\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"reserved_share\",\n",
    "            F.when(\n",
    "                F.col(\"total_stock\") > 0,\n",
    "                F.col(\"reserved_quantity\") / F.col(\"total_stock\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(F.col(\"reserved_share\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_inventory\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"reserved_quantity\": 0,\n",
    "            \"stock_quantity\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    product_analysis[\"reserved_vs_available\"] = (\n",
    "        inv.select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"supplier_id\"] if c in inv.columns],\n",
    "            \"available_stock\",\n",
    "            \"reserved_quantity\",\n",
    "            \"stock_quantity\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"total_stock\",\n",
    "            F.when(\n",
    "                F.col(\"stock_quantity\") > 0,\n",
    "                F.col(\"stock_quantity\"),\n",
    "            ).otherwise(F.col(\"available_stock\") + F.col(\"reserved_quantity\")),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"reserved_share\",\n",
    "            F.when(\n",
    "                F.col(\"total_stock\") > 0,\n",
    "                F.col(\"reserved_quantity\") / F.col(\"total_stock\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(F.col(\"reserved_share\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id \"\n",
    "        \"column is all NULL/zero; skipping reserved vs available stock analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e9733",
   "metadata": {},
   "source": [
    "Excess inventory → items not selling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9619cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "\n",
    "# Excess inventory: items not selling\n",
    "# Preferred source: agg_product_inventory_health\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"current_stock\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Try to enrich with product-level sales if agg_products exists\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "            \"avg_order_value_product\",\n",
    "            \"days_since_launch\",\n",
    "        )\n",
    "\n",
    "        inv = (\n",
    "            inv.alias(\"i\")\n",
    "            .join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "        )\n",
    "    else:\n",
    "        prod = None\n",
    "\n",
    "    # Fallback for avg_daily_sales if missing: use product total_units_sold over a 180-day window\n",
    "    if \"avg_daily_sales\" in inv.columns:\n",
    "        inv = inv.withColumn(\n",
    "            \"avg_daily_sales_effective\",\n",
    "            F.when(F.col(\"avg_daily_sales\").isNotNull(), F.col(\"avg_daily_sales\"))\n",
    "            .otherwise(\n",
    "                F.when(\n",
    "                    F.col(\"total_units_sold\").isNotNull() & (F.col(\"total_units_sold\") > 0),\n",
    "                    F.col(\"total_units_sold\") / F.lit(180.0),\n",
    "                ).otherwise(F.lit(0.0))\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        inv = inv.withColumn(\n",
    "            \"avg_daily_sales_effective\",\n",
    "            F.when(\n",
    "                F.col(\"total_units_sold\").isNotNull() & (F.col(\"total_units_sold\") > 0),\n",
    "                F.col(\"total_units_sold\") / F.lit(180.0),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "\n",
    "    # Ensure days_of_supply exists: prefer existing, else derive from available_stock / avg_daily_sales_effective\n",
    "    if \"days_of_supply\" in inv.columns and not is_column_all_null_or_zero(inv, \"days_of_supply\"):\n",
    "        inv = inv.withColumn(\n",
    "            \"days_of_supply_effective\",\n",
    "            F.col(\"days_of_supply\"),\n",
    "        )\n",
    "    else:\n",
    "        inv = inv.withColumn(\n",
    "            \"days_of_supply_effective\",\n",
    "            F.when(\n",
    "                F.col(\"avg_daily_sales_effective\") > 0,\n",
    "                F.col(\"available_stock\") / F.col(\"avg_daily_sales_effective\"),\n",
    "            ).otherwise(F.lit(None)),\n",
    "        )\n",
    "\n",
    "    # Thresholds (you can tune these):\n",
    "    EXCESS_DOS = 180.0     # very high days of supply\n",
    "    LOW_SALES_RATE = 0.1   # <= 0.1 units/day ~ < 3 units/month\n",
    "\n",
    "    excess = (\n",
    "        inv.withColumn(\n",
    "            \"is_excess_inventory\",\n",
    "            F.when(\n",
    "                (F.col(\"available_stock\") > 0)\n",
    "                & (F.col(\"days_of_supply_effective\").isNotNull())\n",
    "                & (F.col(\"days_of_supply_effective\") >= F.lit(EXCESS_DOS))\n",
    "                & (F.col(\"avg_daily_sales_effective\") <= F.lit(LOW_SALES_RATE)),\n",
    "                1,\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .filter(F.col(\"is_excess_inventory\") == 1)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\"] if c in inv.columns],\n",
    "            \"supplier_id\",\n",
    "            \"available_stock\",\n",
    "            \"current_stock\",\n",
    "            \"avg_daily_sales_effective\",\n",
    "            \"days_of_supply_effective\",\n",
    "            *[c for c in [\"total_units_sold\", \"total_revenue\", \"days_since_launch\"] if c in inv.columns],\n",
    "            \"stock_health_score\",\n",
    "            \"reorder_urgency\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"days_of_supply_effective\").desc_nulls_last(),\n",
    "            F.col(\"available_stock\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"excess_inventory_not_selling\"] = excess\n",
    "\n",
    "# Fallback: use agg_inventory + agg_products if product_inventory_health not usable\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_inventory\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"stock_quantity\": 0,\n",
    "            \"total_sold\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    base = inv\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "            \"days_since_launch\",\n",
    "        )\n",
    "        base = base.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # Approx avg_daily_sales from total_sold or total_units_sold over 180 days\n",
    "    base = base.withColumn(\n",
    "        \"total_units_sold_effective\",\n",
    "        F.coalesce(F.col(\"total_sold\"), F.col(\"total_units_sold\")),\n",
    "    ).withColumn(\n",
    "        \"avg_daily_sales_effective\",\n",
    "        F.when(\n",
    "            F.col(\"total_units_sold_effective\") > 0,\n",
    "            F.col(\"total_units_sold_effective\") / F.lit(180.0),\n",
    "        ).otherwise(F.lit(0.0)),\n",
    "    )\n",
    "\n",
    "    base = base.withColumn(\n",
    "        \"days_of_supply_effective\",\n",
    "        F.when(\n",
    "            F.col(\"avg_daily_sales_effective\") > 0,\n",
    "            F.col(\"available_stock\") / F.col(\"avg_daily_sales_effective\"),\n",
    "        ).otherwise(F.lit(None)),\n",
    "    )\n",
    "\n",
    "    EXCESS_DOS = 180.0\n",
    "    LOW_SALES_RATE = 0.1\n",
    "\n",
    "    excess = (\n",
    "        base.withColumn(\n",
    "            \"is_excess_inventory\",\n",
    "            F.when(\n",
    "                (F.col(\"available_stock\") > 0)\n",
    "                & (F.col(\"days_of_supply_effective\").isNotNull())\n",
    "                & (F.col(\"days_of_supply_effective\") >= F.lit(EXCESS_DOS))\n",
    "                & (F.col(\"avg_daily_sales_effective\") <= F.lit(LOW_SALES_RATE)),\n",
    "                1,\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .filter(F.col(\"is_excess_inventory\") == 1)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\"] if c in base.columns],\n",
    "            \"supplier_id\",\n",
    "            \"available_stock\",\n",
    "            \"stock_quantity\",\n",
    "            \"avg_daily_sales_effective\",\n",
    "            \"days_of_supply_effective\",\n",
    "            *[c for c in [\"total_units_sold_effective\", \"total_revenue\", \"days_since_launch\"] if c in base.columns],\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"days_of_supply_effective\").desc_nulls_last(),\n",
    "            F.col(\"available_stock\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"excess_inventory_not_selling\"] = excess\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping excess inventory (items not selling) analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a1542",
   "metadata": {},
   "source": [
    "Aging inventory → identify slow movers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f2b0562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"product_analysis\" not in locals():\n",
    "    product_analysis = {}\n",
    "\n",
    "# Aging inventory: slow movers\n",
    "# Preferred: agg_product_inventory_health (+ optional agg_products enrichment)\n",
    "\n",
    "if (\n",
    "    \"agg_product_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_product_inventory_health\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_product_inventory_health\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"current_stock\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional enrichment with product-level context\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "            \"days_since_launch\",\n",
    "        )\n",
    "\n",
    "        inv = inv.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # Effective avg_daily_sales\n",
    "    inv = inv.withColumn(\n",
    "        \"avg_daily_sales_effective\",\n",
    "        F.when(F.col(\"avg_daily_sales\").isNotNull(), F.col(\"avg_daily_sales\"))\n",
    "        .otherwise(\n",
    "            F.when(\n",
    "                F.col(\"total_units_sold\").isNotNull() & (F.col(\"total_units_sold\") > 0),\n",
    "                F.col(\"total_units_sold\") / F.lit(180.0),  # simple proxy: last ~6 months\n",
    "            ).otherwise(F.lit(0.0))\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Effective days_since_restock: prefer days_since_restock, else days_since_launch\n",
    "    inv = inv.withColumn(\n",
    "        \"days_since_restock_effective\",\n",
    "        F.when(F.col(\"days_since_restock\").isNotNull(), F.col(\"days_since_restock\"))\n",
    "        .otherwise(F.col(\"days_since_launch\")),\n",
    "    )\n",
    "\n",
    "    # Effective days_of_supply: prefer existing, else derive from stock / avg_daily_sales_effective\n",
    "    if \"days_of_supply\" in inv.columns and not is_column_all_null_or_zero(inv, \"days_of_supply\"):\n",
    "        inv = inv.withColumn(\"days_of_supply_effective\", F.col(\"days_of_supply\"))\n",
    "    else:\n",
    "        inv = inv.withColumn(\n",
    "            \"days_of_supply_effective\",\n",
    "            F.when(\n",
    "                F.col(\"avg_daily_sales_effective\") > 0,\n",
    "                F.col(\"available_stock\") / F.col(\"avg_daily_sales_effective\"),\n",
    "            ).otherwise(F.lit(None)),\n",
    "        )\n",
    "\n",
    "    # Thresholds for \"slow mover\" (tune as needed)\n",
    "    SLOW_MIN_DAYS_IN_STOCK = 60      # at least 60 days in stock\n",
    "    SLOW_MAX_SALES_RATE = 0.5        # <= 0.5 units/day\n",
    "    SLOW_MIN_DOS = 30.0              # optional: at least 30 days of supply\n",
    "\n",
    "    slow = (\n",
    "        inv.withColumn(\n",
    "            \"is_slow_mover\",\n",
    "            F.when(\n",
    "                (F.col(\"available_stock\") > 0)\n",
    "                & (F.col(\"avg_daily_sales_effective\") <= F.lit(SLOW_MAX_SALES_RATE))\n",
    "                & (F.col(\"days_since_restock_effective\").isNotNull())\n",
    "                & (F.col(\"days_since_restock_effective\") >= F.lit(SLOW_MIN_DAYS_IN_STOCK))\n",
    "                & (\n",
    "                    F.col(\"days_of_supply_effective\").isNull()\n",
    "                    | (F.col(\"days_of_supply_effective\") >= F.lit(SLOW_MIN_DOS))\n",
    "                ),\n",
    "                1,\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .filter(F.col(\"is_slow_mover\") == 1)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\"] if c in inv.columns],\n",
    "            \"supplier_id\",\n",
    "            \"available_stock\",\n",
    "            \"current_stock\",\n",
    "            \"avg_daily_sales_effective\",\n",
    "            \"days_of_supply_effective\",\n",
    "            \"days_since_restock_effective\",\n",
    "            *[c for c in [\"total_units_sold\", \"total_revenue\", \"days_since_launch\"] if c in inv.columns],\n",
    "            \"stock_health_score\",\n",
    "            \"reorder_urgency\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"days_since_restock_effective\").desc_nulls_last(),\n",
    "            F.col(\"avg_daily_sales_effective\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"aging_inventory_slow_movers\"] = slow\n",
    "\n",
    "# Fallback: agg_inventory + agg_products if product_inventory_health not usable\n",
    "elif (\n",
    "    \"agg_inventory\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_inventory\"], \"product_id\")\n",
    "):\n",
    "    inv = dataframes[\"agg_inventory\"].fillna(\n",
    "        {\n",
    "            \"available_stock\": 0,\n",
    "            \"stock_quantity\": 0,\n",
    "            \"total_sold\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    base = inv\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "            \"days_since_launch\",\n",
    "        )\n",
    "        base = base.alias(\"i\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # days_since_restock from last_restocked_date\n",
    "    base = base.withColumn(\n",
    "        \"days_since_restock_effective\",\n",
    "        F.when(\n",
    "            F.col(\"last_restocked_date\").isNotNull(),\n",
    "            F.datediff(F.current_date(), F.col(\"last_restocked_date\")),\n",
    "        ).otherwise(F.col(\"days_since_launch\")),\n",
    "    )\n",
    "\n",
    "    # Sales velocity proxy\n",
    "    base = base.withColumn(\n",
    "        \"total_units_sold_effective\",\n",
    "        F.coalesce(F.col(\"total_sold\"), F.col(\"total_units_sold\")),\n",
    "    ).withColumn(\n",
    "        \"avg_daily_sales_effective\",\n",
    "        F.when(\n",
    "            F.col(\"total_units_sold_effective\") > 0,\n",
    "            F.col(\"total_units_sold_effective\") / F.lit(180.0),\n",
    "        ).otherwise(F.lit(0.0)),\n",
    "    )\n",
    "\n",
    "    base = base.withColumn(\n",
    "        \"days_of_supply_effective\",\n",
    "        F.when(\n",
    "            F.col(\"avg_daily_sales_effective\") > 0,\n",
    "            F.col(\"available_stock\") / F.col(\"avg_daily_sales_effective\"),\n",
    "        ).otherwise(F.lit(None)),\n",
    "    )\n",
    "\n",
    "    SLOW_MIN_DAYS_IN_STOCK = 60\n",
    "    SLOW_MAX_SALES_RATE = 0.5\n",
    "    SLOW_MIN_DOS = 30.0\n",
    "\n",
    "    slow = (\n",
    "        base.withColumn(\n",
    "            \"is_slow_mover\",\n",
    "            F.when(\n",
    "                (F.col(\"available_stock\") > 0)\n",
    "                & (F.col(\"avg_daily_sales_effective\") <= F.lit(SLOW_MAX_SALES_RATE))\n",
    "                & (F.col(\"days_since_restock_effective\").isNotNull())\n",
    "                & (F.col(\"days_since_restock_effective\") >= F.lit(SLOW_MIN_DAYS_IN_STOCK))\n",
    "                & (\n",
    "                    F.col(\"days_of_supply_effective\").isNull()\n",
    "                    | (F.col(\"days_of_supply_effective\") >= F.lit(SLOW_MIN_DOS))\n",
    "                ),\n",
    "                1,\n",
    "            ).otherwise(0),\n",
    "        )\n",
    "        .filter(F.col(\"is_slow_mover\") == 1)\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\"] if c in base.columns],\n",
    "            \"supplier_id\",\n",
    "            \"available_stock\",\n",
    "            \"stock_quantity\",\n",
    "            \"avg_daily_sales_effective\",\n",
    "            \"days_of_supply_effective\",\n",
    "            \"days_since_restock_effective\",\n",
    "            *[c for c in [\"total_units_sold_effective\", \"total_revenue\", \"days_since_launch\"] if c in base.columns],\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"days_since_restock_effective\").desc_nulls_last(),\n",
    "            F.col(\"avg_daily_sales_effective\").asc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    product_analysis[\"aging_inventory_slow_movers\"] = slow\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_product_inventory_health or agg_inventory not available, or product_id column is all NULL/zero; \"\n",
    "        \"skipping aging inventory / slow movers analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41a77f",
   "metadata": {},
   "source": [
    "# Cart, Checkout & Funnel Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85bfbd7",
   "metadata": {},
   "source": [
    "High-Value Funnel: Sessions → Views → Cart → Checkout → Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e2ebe196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_customer_sessions missing or session_id is all NULL/zero; skipping high-value funnel analysis.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# High-Value Funnel:  Sessions → Views → Cart → Order\n",
    "if (\n",
    "    \"agg_customer_sessions\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"session_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"customer_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"session_start\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"device_type\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"referrer_source\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"products_viewed\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"total_products_viewed\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"items_added_to_cart\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"orders_from_session\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"converted\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"abandoned\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"cart_value\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"session_engagement_score\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"session_type\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"conversion_flag\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"cart_abandonment_flag\")\n",
    "\n",
    "):\n",
    "    \n",
    "    funnel = dataframes[\"agg_customer_sessions\"]. select(\n",
    "        \"session_id\",\n",
    "        \"customer_id\",\n",
    "        \"session_start\",\n",
    "        \"device_type\",\n",
    "        \"referrer_source\",\n",
    "        \"products_viewed\",\n",
    "        \"total_products_viewed\",\n",
    "        \"items_added_to_cart\",\n",
    "        \"orders_from_session\",\n",
    "        \"converted\",\n",
    "        \"abandoned\",\n",
    "        \"cart_value\",\n",
    "        \"session_engagement_score\",\n",
    "        \"session_type\",\n",
    "        \"conversion_flag\",\n",
    "        \"cart_abandonment_flag\"\n",
    "    )\n",
    "    \n",
    "    # Create funnel step flags\n",
    "    funnel = (\n",
    "        funnel\n",
    "        .withColumn(\"reached_view\", F. when(F.col(\"total_products_viewed\") > 0, 1).otherwise(0))\n",
    "        .withColumn(\"reached_cart\", F.when(F. col(\"items_added_to_cart\") > 0, 1).otherwise(0))\n",
    "        .withColumn(\"reached_order\", F.when(F.col(\"orders_from_session\") > 0, 1).otherwise(0))\n",
    "    )\n",
    "    \n",
    "    # Calculate session monetary value\n",
    "    funnel = funnel.withColumn(\n",
    "        \"session_monetary_value\",\n",
    "        F.coalesce(F.col(\"cart_value\"), F.lit(0.0))\n",
    "    )\n",
    "    \n",
    "    # Mark high-value sessions\n",
    "    HIGH_VALUE_REVENUE_THRESHOLD = 200.0\n",
    "    \n",
    "    funnel = funnel.withColumn(\n",
    "        \"is_high_value_session\",\n",
    "        F.when(F.col(\"session_monetary_value\") >= HIGH_VALUE_REVENUE_THRESHOLD, 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Calculate conversion metrics\n",
    "    funnel = funnel. withColumn(\n",
    "        \"view_to_cart_rate\",\n",
    "        F.when(\n",
    "            F.col(\"total_products_viewed\") > 0,\n",
    "            F.col(\"items_added_to_cart\") / F.col(\"total_products_viewed\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    funnel = funnel.withColumn(\n",
    "        \"cart_to_order_rate\",\n",
    "        F.when(\n",
    "            F.col(\"items_added_to_cart\") > 0,\n",
    "            F.col(\"orders_from_session\") / F.col(\"items_added_to_cart\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    funnel = funnel.withColumn(\n",
    "        \"view_to_order_rate\",\n",
    "        F.when(\n",
    "            F.col(\"total_products_viewed\") > 0,\n",
    "            F.col(\"orders_from_session\") / F.col(\"total_products_viewed\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    analysis[\"high_value_funnel\"] = funnel. orderBy(F.col(\"session_monetary_value\").desc_nulls_last())\n",
    "    \n",
    "    # Aggregate funnel metrics\n",
    "    analysis[\"funnel_summary\"] = funnel.agg(\n",
    "        F.count(\"*\").alias(\"total_sessions\"),\n",
    "        F.sum(\"reached_view\").alias(\"sessions_with_views\"),\n",
    "        F.sum(\"reached_cart\").alias(\"sessions_with_cart\"),\n",
    "        F.sum(\"reached_order\").alias(\"sessions_with_orders\"),\n",
    "        F.sum(\"is_high_value_session\").alias(\"high_value_sessions\"),\n",
    "        F.avg(\"total_products_viewed\").alias(\"avg_products_viewed\"),\n",
    "        F.avg(\"items_added_to_cart\").alias(\"avg_items_added_to_cart\"),\n",
    "        F.avg(\"orders_from_session\").alias(\"avg_orders_per_session\"),\n",
    "        F.avg(\"session_monetary_value\").alias(\"avg_session_value\"),\n",
    "        F.sum(\"session_monetary_value\").alias(\"total_session_value\")\n",
    "    ).withColumn(\n",
    "        \"view_to_cart_conversion\",\n",
    "        F.when(\n",
    "            F.col(\"sessions_with_views\") > 0,\n",
    "            F.col(\"sessions_with_cart\") / F.col(\"sessions_with_views\")\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"cart_to_order_conversion\",\n",
    "        F.when(\n",
    "            F.col(\"sessions_with_cart\") > 0,\n",
    "            F.col(\"sessions_with_orders\") / F.col(\"sessions_with_cart\")\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"overall_conversion_rate\",\n",
    "        F.when(\n",
    "            F.col(\"total_sessions\") > 0,\n",
    "            F.col(\"sessions_with_orders\") / F.col(\"total_sessions\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    # High-value session analysis\n",
    "    if not is_column_all_null_or_zero(funnel, \"is_high_value_session\"):\n",
    "        analysis[\"high_value_vs_regular\"] = funnel.groupBy(\"is_high_value_session\").agg(\n",
    "            F.count(\"*\").alias(\"session_count\"),\n",
    "            F.avg(\"total_products_viewed\").alias(\"avg_products_viewed\"),\n",
    "            F.avg(\"items_added_to_cart\").alias(\"avg_items_in_cart\"),\n",
    "            F.avg(\"orders_from_session\").alias(\"avg_orders\"),\n",
    "            F.avg(\"session_monetary_value\").alias(\"avg_session_value\"),\n",
    "            F.sum(\"session_monetary_value\").alias(\"total_value\"),\n",
    "            F.avg(\"view_to_cart_rate\").alias(\"avg_view_to_cart_rate\"),\n",
    "            F.avg(\"cart_to_order_rate\").alias(\"avg_cart_to_order_rate\"),\n",
    "            F.avg(\"view_to_order_rate\").alias(\"avg_view_to_order_rate\")\n",
    "        ).orderBy(F.col(\"is_high_value_session\").desc())\n",
    "    \n",
    "    # Funnel by device type\n",
    "    if not is_column_all_null_or_zero(funnel, \"device_type\"):\n",
    "        analysis[\"funnel_by_device\"] = funnel.groupBy(\"device_type\").agg(\n",
    "            F. count(\"*\").alias(\"total_sessions\"),\n",
    "            F.sum(\"reached_view\").alias(\"sessions_with_views\"),\n",
    "            F.sum(\"reached_cart\").alias(\"sessions_with_cart\"),\n",
    "            F.sum(\"reached_order\").alias(\"sessions_with_orders\"),\n",
    "            F.avg(\"session_monetary_value\").alias(\"avg_session_value\"),\n",
    "            F.sum(\"is_high_value_session\").alias(\"high_value_sessions\")\n",
    "        ).withColumn(\n",
    "            \"conversion_rate\",\n",
    "            F.when(\n",
    "                F.col(\"total_sessions\") > 0,\n",
    "                F.col(\"sessions_with_orders\") / F.col(\"total_sessions\")\n",
    "            ).otherwise(0.0)\n",
    "        ).orderBy(F.col(\"avg_session_value\").desc_nulls_last())\n",
    "    \n",
    "    # Funnel by referrer source\n",
    "    if not is_column_all_null_or_zero(funnel, \"referrer_source\"):\n",
    "        analysis[\"funnel_by_referrer\"] = funnel.groupBy(\"referrer_source\").agg(\n",
    "            F.count(\"*\").alias(\"total_sessions\"),\n",
    "            F.sum(\"reached_view\").alias(\"sessions_with_views\"),\n",
    "            F.sum(\"reached_cart\").alias(\"sessions_with_cart\"),\n",
    "            F.sum(\"reached_order\").alias(\"sessions_with_orders\"),\n",
    "            F.avg(\"session_monetary_value\").alias(\"avg_session_value\"),\n",
    "            F.sum(\"is_high_value_session\").alias(\"high_value_sessions\")\n",
    "        ).withColumn(\n",
    "            \"conversion_rate\",\n",
    "            F.when(\n",
    "                F.col(\"total_sessions\") > 0,\n",
    "                F. col(\"sessions_with_orders\") / F.col(\"total_sessions\")\n",
    "            ).otherwise(0.0)\n",
    "        ).orderBy(F.col(\"avg_session_value\").desc_nulls_last())\n",
    "    \n",
    "    # Abandoned vs Converted sessions comparison\n",
    "    analysis[\"abandoned_vs_converted\"] = funnel.groupBy(\"converted\").agg(\n",
    "        F.count(\"*\").alias(\"session_count\"),\n",
    "        F.avg(\"total_products_viewed\").alias(\"avg_products_viewed\"),\n",
    "        F.avg(\"items_added_to_cart\").alias(\"avg_items_in_cart\"),\n",
    "        F.avg(\"session_monetary_value\").alias(\"avg_cart_value\"),\n",
    "        F.sum(\"session_monetary_value\").alias(\"total_cart_value\")\n",
    "    ).orderBy(F.col(\"converted\").desc())\n",
    "\n",
    "else:\n",
    "    print(\"agg_customer_sessions missing or session_id is all NULL/zero; skipping high-value funnel analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d88680",
   "metadata": {},
   "source": [
    "Checkout drop-off reasons (incomplete payment, missing info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "da2ccf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Checkout drop-off reasons (incomplete payment, missing info, etc.)\n",
    "# 1) Prefer agg_cart_abandonment_analysis from agg-only schema\n",
    "\n",
    "if (\n",
    "    \"agg_cart_abandonment_analysis\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_id\")\n",
    "):\n",
    "\n",
    "    carts = dataframes[\"agg_cart_abandonment_analysis\"].fillna(\n",
    "        {\n",
    "            \"cart_status\": \"unknown\",\n",
    "            \"cart_status_derived\": \"unknown\",\n",
    "            \"cart_abandonment_reason\": \"unknown\",\n",
    "            \"session_converted\": 0,\n",
    "            \"abandonment_risk_score\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # We treat \"abandoned\" or equivalent derived status as drop-off\n",
    "    # Adjust if you use different labels in cart_status_derived\n",
    "    dropped_carts = carts.filter(\n",
    "        F.lower(F.coalesce(F.col(\"cart_status_derived\"), F.col(\"cart_status\"))).isin(\n",
    "            \"abandoned\", \"lost\", \"expired\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 1) Overall abandonment reasons\n",
    "    product_analysis[\"checkout_dropoff_reasons\"] = (\n",
    "        dropped_carts.groupBy(\"cart_abandonment_reason\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"dropoff_count\"),\n",
    "            F.avg(\"abandonment_risk_score\").alias(\"avg_abandonment_risk_score\"),\n",
    "            F.avg(\"session_converted\").alias(\"conversion_after_abandonment_rate\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"dropoff_count\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # 2) Bucket reasons into higher-level categories: payment vs info vs other\n",
    "    dropped_bucketed = dropped_carts.withColumn(\n",
    "        \"dropoff_bucket\",\n",
    "        F.when(\n",
    "            F.lower(F.col(\"cart_abandonment_reason\")).like(\"%payment%\"),\n",
    "            F.lit(\"payment_issues\"),\n",
    "        )\n",
    "        .when(\n",
    "            F.lower(F.col(\"cart_abandonment_reason\")).like(\"%missing%\")\n",
    "            | F.lower(F.col(\"cart_abandonment_reason\")).like(\"%info%\")\n",
    "            | F.lower(F.col(\"cart_abandonment_reason\")).like(\"%address%\"),\n",
    "            F.lit(\"missing_or_incomplete_info\"),\n",
    "        )\n",
    "        .when(\n",
    "            F.lower(F.col(\"cart_abandonment_reason\")).like(\"%technical%\")\n",
    "            | F.lower(F.col(\"cart_abandonment_reason\")).like(\"%error%\")\n",
    "            | F.lower(F.col(\"cart_abandonment_reason\")).like(\"%timeout%\"),\n",
    "            F.lit(\"technical_errors\"),\n",
    "        )\n",
    "        .otherwise(F.lit(\"other_or_unknown\")),\n",
    "    )\n",
    "\n",
    "    product_analysis[\"checkout_dropoff_buckets\"] = (\n",
    "        dropped_bucketed.groupBy(\"dropoff_bucket\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"dropoff_count\"),\n",
    "            F.avg(\"abandonment_risk_score\").alias(\"avg_abandonment_risk_score\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"dropoff_count\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # 3) Optional enrichment with session/device if agg_customer_sessions exists\n",
    "    if (\n",
    "        \"agg_customer_sessions\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_customer_sessions\"], \"session_id\")\n",
    "    ):\n",
    "        sessions = dataframes[\"agg_customer_sessions\"].select(\n",
    "            \"session_id\",\n",
    "            \"device_type\",\n",
    "            \"referrer_source\",\n",
    "            \"session_engagement_score\",\n",
    "        )\n",
    "\n",
    "        carts_with_sessions = (\n",
    "            dropped_bucketed.join(sessions, on=\"session_id\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        # Drop-off by device and bucket\n",
    "        product_analysis[\"checkout_dropoff_by_device_and_reason\"] = (\n",
    "            carts_with_sessions.groupBy(\"device_type\", \"dropoff_bucket\")\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"dropoff_count\"),\n",
    "                F.avg(\"abandonment_risk_score\").alias(\"avg_abandonment_risk_score\"),\n",
    "                F.avg(\"session_engagement_score\").alias(\"avg_session_engagement_score\"),\n",
    "            )\n",
    "            .orderBy(\n",
    "                F.col(\"dropoff_count\").desc_nulls_last(),\n",
    "                F.col(\"device_type\").asc_nulls_last(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "else:\n",
    "    # 2) Fallback to global-schema checkout events if you later have one (e.g., agg_checkout_events)\n",
    "    if (\n",
    "        \"agg_checkout_events\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_checkout_events\"], \"checkout_id\")\n",
    "    ):\n",
    "        chk = dataframes[\"agg_checkout_events\"].fillna(\n",
    "            {\n",
    "                \"checkout_status\": \"unknown\",\n",
    "                \"dropoff_reason\": \"unknown\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Non-completed = dropped\n",
    "        dropped_chk = chk.filter(\n",
    "            F.lower(F.col(\"checkout_status\")).isin(\"abandoned\", \"failed\", \"incomplete\")\n",
    "        )\n",
    "\n",
    "        product_analysis[\"checkout_dropoff_reasons\"] = (\n",
    "            dropped_chk.groupBy(\"dropoff_reason\")\n",
    "            .agg(F.count(\"*\").alias(\"dropoff_count\"))\n",
    "            .orderBy(F.col(\"dropoff_count\").desc_nulls_last())\n",
    "        )\n",
    "\n",
    "        dropped_bucketed = dropped_chk.withColumn(\n",
    "            \"dropoff_bucket\",\n",
    "            F.when(\n",
    "                F.lower(F.col(\"dropoff_reason\")).like(\"%payment%\"),\n",
    "                F.lit(\"payment_issues\"),\n",
    "            )\n",
    "            .when(\n",
    "                F.lower(F.col(\"dropoff_reason\")).like(\"%missing%\")\n",
    "                | F.lower(F.col(\"dropoff_reason\")).like(\"%info%\")\n",
    "                | F.lower(F.col(\"dropoff_reason\")).like(\"%address%\"),\n",
    "                F.lit(\"missing_or_incomplete_info\"),\n",
    "            )\n",
    "            .when(\n",
    "                F.lower(F.col(\"dropoff_reason\")).like(\"%technical%\")\n",
    "                | F.lower(F.col(\"dropoff_reason\")).like(\"%error%\")\n",
    "                | F.lower(F.col(\"dropoff_reason\")).like(\"%timeout%\"),\n",
    "                F.lit(\"technical_errors\"),\n",
    "            )\n",
    "            .otherwise(F.lit(\"other_or_unknown\")),\n",
    "        )\n",
    "\n",
    "        product_analysis[\"checkout_dropoff_buckets\"] = (\n",
    "            dropped_bucketed.groupBy(\"dropoff_bucket\")\n",
    "            .agg(F.count(\"*\").alias(\"dropoff_count\"))\n",
    "            .orderBy(F.col(\"dropoff_count\").desc_nulls_last())\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            \"Neither agg_cart_abandonment_analysis (agg-only) nor a checkout events table \"\n",
    "            \"with reasons is available; skipping checkout drop-off reasons analysis.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3cc711",
   "metadata": {},
   "source": [
    "# Supplier And procurement insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3a9a8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"supplier_analysis\" not in locals():\n",
    "    supplier_analysis = {}\n",
    "\n",
    "has_suppliers = (\n",
    "    \"agg_suppliers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_suppliers\"], \"supplier_id\")\n",
    ")\n",
    "\n",
    "has_sup_inv = (\n",
    "    \"agg_supplier_inventory_health\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_supplier_inventory_health\"], \"supplier_id\")\n",
    ")\n",
    "\n",
    "if not has_suppliers and not has_sup_inv:\n",
    "    print(\n",
    "        \"agg_suppliers and agg_supplier_inventory_health not available, or supplier_id all NULL/zero; \"\n",
    "        \"skipping supplier & procurement insights.\"\n",
    "    )\n",
    "else:\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Build a non-ambiguous base supplier frame from agg_suppliers\n",
    "    #    Use ONLY these names; do NOT bring in overlapping metrics from inv table.\n",
    "    # ------------------------------------------------------------------\n",
    "    if has_suppliers:\n",
    "        s = dataframes[\"agg_suppliers\"].select(\n",
    "            \"supplier_id\",\n",
    "            \"supplier_status\",\n",
    "            \"total_products_supplied\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_orders_fulfilled\",\n",
    "            # keep this as the authoritative stockouts at supplier level\n",
    "            \"total_stockouts\",\n",
    "            \"total_revenue_generated\",\n",
    "            \"avg_profit_margin\",\n",
    "            \"total_stock_value\",\n",
    "            \"avg_stock_quantity\",\n",
    "            \"avg_restock_lead_time\",\n",
    "            \"supplier_performance_score\",\n",
    "            \"stock_efficiency_ratio\",\n",
    "            \"supplier_reliability_score\",\n",
    "            \"stockout_rate\",                     # supplier-level stockout rate\n",
    "            \"supplier_inventory_health_score\",   # supplier-level health score\n",
    "            \"contract_start_date\",\n",
    "            \"contract_end_date\",\n",
    "            \"days_until_contract_expiry\",\n",
    "            \"contract_status_flag\",\n",
    "            \"avg_order_value\",\n",
    "        )\n",
    "    else:\n",
    "        # minimal shell when agg_suppliers missing\n",
    "        s = dataframes[\"agg_supplier_inventory_health\"].select(\"supplier_id\").distinct()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Build inventory-health frame with RENAMED columns to avoid clashes\n",
    "    #    We DO NOT bring in columns that already exist in 's' with same name.\n",
    "    # ------------------------------------------------------------------\n",
    "    if has_sup_inv:\n",
    "        i = dataframes[\"agg_supplier_inventory_health\"].select(\n",
    "            \"supplier_id\",\n",
    "            F.col(\"total_products\").alias(\"inv_total_products\"),\n",
    "            F.col(\"total_current_stock\").alias(\"inv_total_current_stock\"),\n",
    "            F.col(\"total_available_stock\").alias(\"inv_total_available_stock\"),\n",
    "            # this \"total_stockouts\" would clash, so we rename it:\n",
    "            F.col(\"total_stockouts\").alias(\"inv_total_stockouts\"),\n",
    "            F.col(\"total_reorder_breaches\").alias(\"inv_total_reorder_breaches\"),\n",
    "            F.col(\"total_storage_cost\").alias(\"inv_total_storage_cost\"),\n",
    "            F.col(\"avg_stock_per_product\").alias(\"inv_avg_stock_per_product\"),\n",
    "            F.col(\"stockout_rate\").alias(\"inv_stockout_rate\"),  # separate from supplier stockout_rate\n",
    "            F.col(\"breach_rate\").alias(\"inv_breach_rate\"),\n",
    "            F.col(\"avg_storage_cost_per_unit\").alias(\"inv_avg_storage_cost_per_unit\"),\n",
    "            F.col(\"days_since_last_restock\").alias(\"inv_days_since_last_restock\"),\n",
    "            F.col(\"supplier_inventory_health_score\").alias(\"inv_health_score\"),\n",
    "        )\n",
    "\n",
    "        joined = (\n",
    "            s.alias(\"s\")\n",
    "             .join(i.alias(\"i\"), on=\"supplier_id\", how=\"left\")\n",
    "        )\n",
    "    else:\n",
    "        joined = s\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Supplier reliability score (no ambiguities)\n",
    "    #    Prefer: agg_suppliers.supplier_reliability_score,\n",
    "    #    else use inv_health_score from inventory health.\n",
    "    # ------------------------------------------------------------------\n",
    "    joined = joined.withColumn(\n",
    "        \"supplier_reliability_score_effective\",\n",
    "        F.coalesce(\n",
    "            F.col(\"supplier_reliability_score\"),\n",
    "            F.col(\"inv_health_score\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    supplier_analysis[\"supplier_reliability\"] = (\n",
    "        joined.select(\n",
    "            \"supplier_id\",\n",
    "            \"supplier_status\",\n",
    "            \"supplier_reliability_score_effective\",\n",
    "            \"supplier_performance_score\",\n",
    "            \"stock_efficiency_ratio\",\n",
    "        )\n",
    "        .orderBy(F.col(\"supplier_reliability_score_effective\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Supplier stockout rate (no ambiguous total_stockouts)\n",
    "    #    Use supplier-level metrics; keep inventory-level as separate columns.\n",
    "    # ------------------------------------------------------------------\n",
    "    supplier_analysis[\"supplier_stockouts\"] = (\n",
    "        joined.select(\n",
    "            \"supplier_id\",\n",
    "            F.coalesce(F.col(\"total_stockouts\"), F.lit(0)).alias(\"total_stockouts\"),\n",
    "            F.coalesce(F.col(\"stockout_rate\"), F.lit(0.0)).alias(\"supplier_stockout_rate\"),\n",
    "            # inventory-health side for context\n",
    "            F.coalesce(F.col(\"inv_total_stockouts\"), F.lit(0)).alias(\"inv_total_stockouts\"),\n",
    "            F.coalesce(F.col(\"inv_stockout_rate\"), F.lit(0.0)).alias(\"inv_stockout_rate\"),\n",
    "            F.coalesce(F.col(\"inv_total_products\"), F.lit(0)).alias(\"inv_total_products\"),\n",
    "            F.coalesce(F.col(\"inv_total_current_stock\"), F.lit(0)).alias(\"inv_total_current_stock\"),\n",
    "            F.coalesce(F.col(\"inv_total_available_stock\"), F.lit(0)).alias(\"inv_total_available_stock\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"supplier_stockout_rate\").desc_nulls_last(),\n",
    "            F.col(\"total_stockouts\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Supplier fulfillment performance\n",
    "    # ------------------------------------------------------------------\n",
    "    supplier_analysis[\"supplier_fulfillment_performance\"] = (\n",
    "        joined.select(\n",
    "            \"supplier_id\",\n",
    "            \"supplier_status\",\n",
    "            F.coalesce(F.col(\"total_orders_fulfilled\"), F.lit(0)).alias(\"total_orders_fulfilled\"),\n",
    "            F.coalesce(F.col(\"total_units_sold\"), F.lit(0)).alias(\"total_units_sold\"),\n",
    "            F.coalesce(F.col(\"avg_restock_lead_time\"), F.lit(0.0)).alias(\"avg_restock_lead_time\"),\n",
    "            F.coalesce(F.col(\"supplier_performance_score\"), F.lit(0.0)).alias(\"supplier_performance_score\"),\n",
    "            F.coalesce(F.col(\"stock_efficiency_ratio\"), F.lit(0.0)).alias(\"stock_efficiency_ratio\"),\n",
    "            F.col(\"supplier_reliability_score_effective\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"supplier_performance_score\").desc_nulls_last(),\n",
    "            F.col(\"supplier_reliability_score_effective\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6) Supplier revenue contribution\n",
    "    # ------------------------------------------------------------------\n",
    "    if \"total_revenue_generated\" in joined.columns:\n",
    "        total_rev_all = joined.agg(F.sum(\"total_revenue_generated\")).collect()[0][0] or 0.0\n",
    "\n",
    "        supplier_revenue = joined.withColumn(\n",
    "            \"revenue_contribution_share\",\n",
    "            F.when(\n",
    "                F.lit(total_rev_all) > 0,\n",
    "                F.col(\"total_revenue_generated\") / F.lit(total_rev_all),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "\n",
    "        supplier_analysis[\"supplier_revenue_contribution\"] = (\n",
    "            supplier_revenue.select(\n",
    "                \"supplier_id\",\n",
    "                \"total_revenue_generated\",\n",
    "                F.coalesce(F.col(\"total_orders_fulfilled\"), F.lit(0)).alias(\"total_orders_fulfilled\"),\n",
    "                \"avg_order_value\",\n",
    "                \"revenue_contribution_share\",\n",
    "            )\n",
    "            .orderBy(F.col(\"total_revenue_generated\").desc_nulls_last())\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 7) Supplier’s avg profit margin\n",
    "    # ------------------------------------------------------------------\n",
    "    if \"avg_profit_margin\" in joined.columns:\n",
    "        supplier_analysis[\"supplier_profit_margin\"] = (\n",
    "            joined.select(\n",
    "                \"supplier_id\",\n",
    "                \"avg_profit_margin\",\n",
    "                F.coalesce(F.col(\"total_revenue_generated\"), F.lit(0.0)).alias(\"total_revenue_generated\"),\n",
    "                F.coalesce(F.col(\"total_orders_fulfilled\"), F.lit(0)).alias(\"total_orders_fulfilled\"),\n",
    "            )\n",
    "            .orderBy(\n",
    "                F.col(\"avg_profit_margin\").desc_nulls_last(),\n",
    "                F.col(\"total_revenue_generated\").desc_nulls_last(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 8) Days since last restock (supplier-level, from inventory health)\n",
    "    # ------------------------------------------------------------------\n",
    "    if \"inv_days_since_last_restock\" in joined.columns:\n",
    "        supplier_analysis[\"supplier_days_since_last_restock\"] = (\n",
    "            joined.select(\n",
    "                \"supplier_id\",\n",
    "                F.col(\"inv_days_since_last_restock\").alias(\"days_since_last_restock\"),\n",
    "                F.coalesce(F.col(\"inv_total_products\"), F.lit(0)).alias(\"inv_total_products\"),\n",
    "                F.coalesce(F.col(\"inv_total_current_stock\"), F.lit(0)).alias(\"inv_total_current_stock\"),\n",
    "                F.coalesce(F.col(\"inv_total_available_stock\"), F.lit(0)).alias(\"inv_total_available_stock\"),\n",
    "            )\n",
    "            .orderBy(F.col(\"days_since_last_restock\").desc_nulls_last())\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 9) Supplier contract expiry prediction\n",
    "    # ------------------------------------------------------------------\n",
    "    if \"contract_end_date\" in joined.columns or \"days_until_contract_expiry\" in joined.columns:\n",
    "        supplier_analysis[\"supplier_contract_expiry\"] = (\n",
    "            joined.select(\n",
    "                \"supplier_id\",\n",
    "                \"supplier_status\",\n",
    "                \"contract_start_date\",\n",
    "                \"contract_end_date\",\n",
    "                \"days_until_contract_expiry\",\n",
    "                \"contract_status_flag\",\n",
    "                \"supplier_reliability_score_effective\",\n",
    "                F.coalesce(F.col(\"supplier_performance_score\"), F.lit(0.0)).alias(\"supplier_performance_score\"),\n",
    "            )\n",
    "            .orderBy(\n",
    "                F.col(\"days_until_contract_expiry\").asc_nulls_last(),\n",
    "                F.col(\"supplier_reliability_score_effective\").desc_nulls_last(),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71223c7",
   "metadata": {},
   "source": [
    "# Wishlist usage and conversion rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7d599",
   "metadata": {},
   "source": [
    "Overall wishlist usage and conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "27052d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"product_id\") and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"purchased_date\") and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"customer_id\"):\n",
    "    analysis[\"wishlist_overall_summary\"] = dataframes[\"agg_wishlist\"].agg(\n",
    "        F.count(\"*\").alias(\"total_wishlist_items\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"customers_using_wishlist\"),\n",
    "        F.countDistinct(\"product_id\").alias(\"products_in_wishlist\"),\n",
    "        F.sum(F.when(F.col(\"purchased_date\").isNotNull(), 1).otherwise(0)).alias(\"wishlist_purchased_items\")\n",
    "    ).withColumn(\n",
    "        \"wishlist_conversion_rate\",\n",
    "        F.col(\"wishlist_purchased_items\") / F.col(\"total_wishlist_items\")\n",
    "    ).fillna({\n",
    "        \"total_wishlist_items\": 0,\n",
    "        \"customers_using_wishlist\": 0,\n",
    "        \"products_in_wishlist\": 0,\n",
    "        \"wishlist_purchased_items\": 0,\n",
    "        \"wishlist_conversion_rate\": 0.0\n",
    "    })\n",
    "else:\n",
    "    print(\"One of the required columns in agg_wishlist is all NULL or zero; skipping wishlist overall analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904dbb4",
   "metadata": {},
   "source": [
    "Wishlist usage & conversion by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c8860a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"product_id\") and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"purchased_date\"):\n",
    "    analysis[\"wishlist_by_product\"] = (\n",
    "        dataframes[\"agg_wishlist\"]  \n",
    "        .groupBy(\"product_id\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"wishlist_adds\"),\n",
    "            F.sum(F.when(F.col(\"purchased_date\").isNotNull(), 1).otherwise(0)).alias(\"wishlist_purchases\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"wishlist_conversion_rate\",\n",
    "            F.col(\"wishlist_purchases\") / F.col(\"wishlist_adds\")\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"One of the required columns in agg_wishlist is all NULL or zero; skipping wishlist by product analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5689a",
   "metadata": {},
   "source": [
    "Wishlist usage & conversion by customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "374ecaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"customer_id\") and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"purchased_date\"):\n",
    "    analysis[\"wishlist_by_customer\"] = (\n",
    "        dataframes[\"agg_wishlist\"] \n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"wishlist_adds\"),\n",
    "            F.sum(F.when(F.col(\"purchased_date\").isNotNull(), 1).otherwise(0)).alias(\"wishlist_purchases\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"wishlist_conversion_rate\",\n",
    "            F.when(F.col(\"wishlist_adds\") > 0,\n",
    "                F.col(\"wishlist_purchases\") / F.col(\"wishlist_adds\"))\n",
    "            .otherwise(F.lit(0.0))\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"customer_id or purchased_date column is all NULL or zero; skipping wishlist by customer analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98b2d6",
   "metadata": {},
   "source": [
    "Wishlist Time-to-Purchase: wishlist_to_purchase_time distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0617c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Wishlist Time-to-Purchase: distribution\n",
    "if \"agg_wishlist\" in dataframes and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"wishlist_id\"):\n",
    "    wl = dataframes[\"agg_wishlist\"].select(\n",
    "        \"wishlist_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"wishlist_to_purchase_time\"\n",
    "    ).filter(F.col(\"wishlist_to_purchase_time\").isNotNull())\n",
    "\n",
    "    # 1) Overall stats\n",
    "    analysis[\"wishlist_time_to_purchase_stats\"] = wl.agg(\n",
    "        F.count(\"*\").alias(\"records\"),\n",
    "        F.avg(\"wishlist_to_purchase_time\").alias(\"avg_time\"),\n",
    "        F.expr(\"percentile_approx(wishlist_to_purchase_time, 0.5)\").alias(\"median_time\"),\n",
    "        F.expr(\"percentile_approx(wishlist_to_purchase_time, 0.9)\").alias(\"p90_time\"),\n",
    "        F.max(\"wishlist_to_purchase_time\").alias(\"max_time\"),\n",
    "    )\n",
    "\n",
    "    # 2) Buckets (adjust edges to your unit: days/hours)\n",
    "    wl_buckets = wl.withColumn(\n",
    "        \"time_bucket\",\n",
    "        F.when(F.col(\"wishlist_to_purchase_time\") <= 1, \"≤ 1\")\n",
    "         .when((F.col(\"wishlist_to_purchase_time\") > 1) & (F.col(\"wishlist_to_purchase_time\") <= 7), \"1–7\")\n",
    "         .when((F.col(\"wishlist_to_purchase_time\") > 7) & (F.col(\"wishlist_to_purchase_time\") <= 30), \"7–30\")\n",
    "         .otherwise(\"> 30\")\n",
    "    )\n",
    "\n",
    "    # 3) Counts per bucket\n",
    "    bucket_counts = (\n",
    "        wl_buckets.groupBy(\"time_bucket\")\n",
    "        .agg(F.count(\"*\").alias(\"count\"))\n",
    "    )\n",
    "\n",
    "    # 4) Total count (for share calculation)\n",
    "    total_count = bucket_counts.agg(F.sum(\"count\").alias(\"total_count\"))\n",
    "\n",
    "    # 5) Cross join to get total_count on every row, then compute share\n",
    "    analysis[\"wishlist_time_to_purchase_distribution\"] = (\n",
    "        bucket_counts.crossJoin(total_count)\n",
    "        .withColumn(\"share\", F.col(\"count\") / F.col(\"total_count\"))\n",
    "        .select(\"time_bucket\", \"count\", \"share\")\n",
    "        .orderBy(\"time_bucket\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_wishlist not available, or wishlist_id all NULL/zero; \"\n",
    "        \"skipping wishlist time-to-purchase analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9b8cb",
   "metadata": {},
   "source": [
    "Abandoned Wishlist Analysis: Items added but never purchased or removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3e1078fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "if \"agg_wishlist\" in dataframes and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"wishlist_id\"):\n",
    "    wl = dataframes[\"agg_wishlist\"].select(\n",
    "        \"wishlist_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"added_date\",\n",
    "        \"purchased_date\",\n",
    "        \"removed_date\"\n",
    "    )\n",
    "\n",
    "    # Abandoned = no purchased_date and no removed_date\n",
    "    abandoned = wl.filter(\n",
    "        F.col(\"purchased_date\").isNull() & F.col(\"removed_date\").isNull()\n",
    "    )\n",
    "\n",
    "    analysis[\"abandoned_wishlist_items\"] = abandoned\n",
    "else:\n",
    "    print(\n",
    "        \"agg_wishlist not available, or wishlist_id all NULL/zero; \"\n",
    "        \"skipping abandoned wishlist analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4446d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" in locals() and \"abandoned_wishlist_items\" in analysis:\n",
    "    abandoned = analysis[\"abandoned_wishlist_items\"]\n",
    "\n",
    "    # By customer\n",
    "    analysis[\"abandoned_wishlist_by_customer\"] = (\n",
    "        abandoned.groupBy(\"customer_id\")\n",
    "        .agg(F.count(\"*\").alias(\"abandoned_wishlist_count\"))\n",
    "        .orderBy(F.col(\"abandoned_wishlist_count\").desc())\n",
    "    )\n",
    "\n",
    "    # By product\n",
    "    analysis[\"abandoned_wishlist_by_product\"] = (\n",
    "        abandoned.groupBy(\"product_id\")\n",
    "        .agg(F.count(\"*\").alias(\"abandoned_wishlist_count\"))\n",
    "        .orderBy(F.col(\"abandoned_wishlist_count\").desc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f8d60",
   "metadata": {},
   "source": [
    "Seasonal Wishlist Patterns: Wishlist adds before holidays/sales events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8086f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "if \"agg_wishlist\" in dataframes and not is_column_all_null_or_zero(dataframes[\"agg_wishlist\"], \"wishlist_id\"):\n",
    "    wl = dataframes[\"agg_wishlist\"].select(\n",
    "        \"wishlist_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        \"added_date\"\n",
    "    ).filter(F.col(\"added_date\").isNotNull())\n",
    "\n",
    "    # Adds by year-month\n",
    "    wl_monthly = wl.withColumn(\"year_month\", F.date_format(\"added_date\", \"yyyy-MM\"))\n",
    "\n",
    "    analysis[\"wishlist_adds_by_month\"] = (\n",
    "        wl_monthly.groupBy(\"year_month\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"wishlist_adds\")\n",
    "        )\n",
    "        .orderBy(\"year_month\")\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"agg_wishlist not available, or wishlist_id all NULL/zero; \"\n",
    "        \"skipping wishlist seasonality by month.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c781e",
   "metadata": {},
   "source": [
    "# Cart creation, abandonment, and recovery statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b71a95",
   "metadata": {},
   "source": [
    "Basic cart creation & status distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cf8da874",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_shopping_cart\"], \"cart_id\") and not is_column_all_null_or_zero(dataframes[\"agg_shopping_cart\"], \"cart_status\"):\n",
    "    analysis[\"cart_overall_stats\"] = dataframes[\"agg_shopping_cart\"].agg(\n",
    "        F.countDistinct(\"cart_id\").alias(\"total_carts\"),\n",
    "        F.count(\"*\").alias(\"total_cart_lines\")\n",
    "    ).fillna({\n",
    "        \"total_carts\": 0,\n",
    "        \"total_cart_lines\": 0\n",
    "    })\n",
    "else:\n",
    "    print(\"cart_id or cart_status column is all NULL or zero; skipping overall cart statistics analysis.\")\n",
    "if not is_column_all_null_or_zero(dataframes[\"agg_shopping_cart\"], \"cart_status\"):\n",
    "    analysis[\"cart_status_distribution\"] = (\n",
    "        dataframes[\"agg_shopping_cart\"]\n",
    "        .groupBy(\"cart_status\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"cart_id\").alias(\"carts_count\"),\n",
    "        F.count(\"*\").alias(\"cart_lines_count\")\n",
    "    ).fillna({\n",
    "        \"carts_count\": 0,\n",
    "        \"cart_lines_count\": 0\n",
    "    })\n",
    "    .orderBy(\"cart_status\")\n",
    ")\n",
    "else:\n",
    "    print(\"cart_status column is all NULL or zero; skipping cart status distribution analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531172d",
   "metadata": {},
   "source": [
    "Abandonment & recovery (using agg_cart_abandonment_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "dd92fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+-----------+----------------+-----------------+-----------------+------------------+------------------------+------------------+-------------------+-----------+-----------------------+----------------+---------------+--------------------+-------------------+-----------------------+---------------+------------------+----------------------+\n",
      "|cart_id|cart_status|cart_added_date|customer_id|cart_items_count|session_converted|time_in_cart_days|time_in_cart_hours|recovery_potential_score|  cart_total_value|cart_avg_item_price|device_used|abandoned_cart_category|first_added_date|last_added_date|          session_id|cart_status_derived|cart_abandonment_reason|cart_value_tier|cart_size_category|abandonment_risk_score|\n",
      "+-------+-----------+---------------+-----------+----------------+-----------------+-----------------+------------------+------------------------+------------------+-------------------+-----------+-----------------------+----------------+---------------+--------------------+-------------------+-----------------------+---------------+------------------+----------------------+\n",
      "|   1001|       NULL|     2025-12-19|       1657|               1|             NULL|               21|               504|                      25|            1428.0|              476.0|       NULL|          Winter Sports|      2025-12-19|     2025-12-19|033ee339-efab-4e2...|          Abandoned|   High Value - Pric...|     High Value|       Single Item|     73.83999999999999|\n",
      "|   1002|       NULL|     2025-12-21|       1832|               1|             NULL|               19|               456|                      25|345.29999999999995|              115.1|       NULL|   Sports & Outdoors...|      2025-12-21|     2025-12-21|6dcd8f05-f046-446...|          Abandoned|   Long Time - Lost ...|      Low Value|       Single Item|     38.69233333333333|\n",
      "|   1003|       NULL|     2025-12-13|       1112|               1|             NULL|               27|               648|                      25|             50.91|              50.91|       NULL|            Non-Fiction|      2025-12-13|     2025-12-13|63b39ad-f753-4b12...|          Abandoned|   Long Time - Lost ...| Very Low Value|       Single Item|               40.5273|\n",
      "|   1004|       NULL|     2025-12-30|       1538|               1|             NULL|               10|               240|                      50|1167.3600000000001|             389.12|       NULL|            Team Sports|      2025-12-30|     2025-12-30|4d703158-c8b2-4d5...|          Abandoned|   High Value - Pric...|     High Value|       Single Item|     51.35413333333334|\n",
      "|   1005|       NULL|     2026-01-06|       1891|               1|             NULL|                3|                72|                      50|             93.43|              93.43|       NULL|            Team Sports|      2026-01-06|     2026-01-06|5adbe58d-8c99-434...|          Abandoned|                Unknown| Very Low Value|       Single Item|     9.802900000000001|\n",
      "+-------+-----------+---------------+-----------+----------------+-----------------+-----------------+------------------+------------------------+------------------+-------------------+-----------+-----------------------+----------------+---------------+--------------------+-------------------+-----------------------+---------------+------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframes[\"agg_cart_abandonment_analysis\"].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6f1ae286",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_id\") and not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_status\"):\n",
    "    analysis[\"cart_abandon_summary\"] = dataframes[\"agg_cart_abandonment_analysis\"].agg(\n",
    "        F.countDistinct(\"cart_id\").alias(\"total_carts_tracked\"),\n",
    "        F.countDistinct(F.when(F.col(\"cart_status\") == \"Abandoned\", F.col(\"cart_id\"))).alias(\"abandoned_carts\"),\n",
    "        F.countDistinct(F.when(F.col(\"cart_status\") == \"Converted\", F.col(\"cart_id\"))).alias(\"converted_carts\")\n",
    "    ).withColumn(\n",
    "        \"abandonment_rate\",\n",
    "        F.col(\"abandoned_carts\") / F.col(\"total_carts_tracked\")\n",
    "    ).withColumn(\n",
    "        \"purchase_rate\",\n",
    "        F.col(\"converted_carts\") / F.col(\"total_carts_tracked\")\n",
    "    ).fillna({\n",
    "        \"total_carts_tracked\": 0,\n",
    "        \"abandoned_carts\": 0,\n",
    "        \"converted_carts\": 0,\n",
    "        \"abandonment_rate\": 0.0,\n",
    "        \"purchase_rate\": 0.0\n",
    "    })\n",
    "else:\n",
    "    print(\"cart_id or cart_status column is all NULL or zero; skipping cart abandonment overall analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b15b6f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+---------------+----------------+-------------+\n",
      "|total_carts_tracked|abandoned_carts|converted_carts|abandonment_rate|purchase_rate|\n",
      "+-------------------+---------------+---------------+----------------+-------------+\n",
      "|               1776|              0|              0|             0.0|          0.0|\n",
      "+-------------------+---------------+---------------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysis[\"cart_abandon_summary\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c0219",
   "metadata": {},
   "source": [
    "Value and size characteristics of abandoned vs purchased carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "34490f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_status\"):\n",
    "    analysis[\"cart_value_stats\"] = (\n",
    "        dataframes[\"agg_cart_abandonment_analysis\"]\n",
    "        .groupBy(\"cart_status\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"cart_id\").alias(\"carts_count\"),\n",
    "            F.avg(\"cart_total_value\").alias(\"avg_cart_value\"),\n",
    "        F.avg(\"cart_items_count\").alias(\"avg_cart_items\"),\n",
    "        F.avg(\"time_in_cart_days\").alias(\"avg_time_in_cart_days\"),\n",
    "        F.avg(\"recovery_potential_score\").alias(\"avg_recovery_potential_score\")\n",
    "    ).fillna({\n",
    "        \"carts_count\": 0,\n",
    "        \"avg_cart_value\": 0.0,\n",
    "        \"avg_cart_items\": 0.0,\n",
    "        \"avg_time_in_cart_days\": 0.0,\n",
    "        \"avg_recovery_potential_score\": 0.0\n",
    "    })\n",
    "    .orderBy(\"cart_status\")\n",
    ")\n",
    "else:\n",
    "    print(\"cart_status column is all NULL or zero; skipping cart value statistics analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e73f44",
   "metadata": {},
   "source": [
    "Recovery opportunity: high-value abandoned carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e6cafa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_status\") and not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_total_value\"):\n",
    "    analysis[\"high_value_abandoned_carts\"] = (\n",
    "        dataframes[\"agg_cart_abandonment_analysis\"]\n",
    "        .filter(\n",
    "            (F.col(\"cart_status\") == \"abandoned\") &\n",
    "            (F.col(\"cart_total_value\") >= 100)  # threshold – adjust as needed\n",
    "        )\n",
    "        .select(\n",
    "            \"cart_id\",\n",
    "            \"customer_id\",\n",
    "            \"cart_total_value\",\n",
    "            \"cart_items_count\",\n",
    "            \"time_in_cart_days\",\n",
    "            \"recovery_potential_score\",\n",
    "            \"abandonment_risk_score\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"cart_status or cart_total_value column is all NULL or zero; skipping high-value abandoned carts analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e87de",
   "metadata": {},
   "source": [
    "Average Time-to-Purchase: Analyze time_in_cart_days/hours for completed orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4a2818f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Average Time-to-Purchase: time_in_cart_days/hours for completed carts\n",
    "if (\n",
    "    \"agg_cart_abandonment_analysis\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_id\")\n",
    "):\n",
    "    carts = dataframes[\"agg_cart_abandonment_analysis\"].fillna(\n",
    "        {\n",
    "            \"session_converted\": 0,\n",
    "            \"time_in_cart_days\": 0,\n",
    "            \"time_in_cart_hours\": 0,\n",
    "            \"cart_items_count\": 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define \"completed / purchased\" carts:\n",
    "    # 1) session_converted = 1\n",
    "    # 2) OR cart_status_derived looks like a purchased/completed status\n",
    "    completed_carts = carts.filter(\n",
    "        (F.col(\"session_converted\") == 1)\n",
    "        | (\n",
    "            F.lower(F.col(\"cart_status_derived\")).isin(\n",
    "                \"purchased\", \"completed\", \"converted\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 1) Overall average time-to-purchase (days & hours)\n",
    "    analysis[\"time_to_purchase_overall\"] = (\n",
    "        completed_carts.agg(\n",
    "            F.countDistinct(\"cart_id\").alias(\"completed_carts\"),\n",
    "            F.avg(\"time_in_cart_days\").alias(\"avg_time_in_cart_days\"),\n",
    "            F.avg(\"time_in_cart_hours\").alias(\"avg_time_in_cart_hours\"),\n",
    "            F.expr(\"percentile(time_in_cart_days, array(0.25, 0.5, 0.75))\").alias(\n",
    "                \"time_in_cart_days_percentiles\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2) Time-to-purchase by cart value tier (e.g., low/medium/high) if populated\n",
    "    group_cols = []\n",
    "    if \"cart_value_tier\" in carts.columns:\n",
    "        group_cols.append(\"cart_value_tier\")\n",
    "    if \"cart_size_category\" in carts.columns:\n",
    "        group_cols.append(\"cart_size_category\")\n",
    "\n",
    "    if group_cols:\n",
    "        analysis[\"time_to_purchase_by_tier\"] = (\n",
    "            completed_carts.groupBy(*group_cols)\n",
    "            .agg(\n",
    "                F.countDistinct(\"cart_id\").alias(\"completed_carts\"),\n",
    "                F.avg(\"time_in_cart_days\").alias(\"avg_time_in_cart_days\"),\n",
    "                F.avg(\"time_in_cart_hours\").alias(\"avg_time_in_cart_hours\"),\n",
    "                F.avg(\"cart_items_count\").alias(\"avg_cart_items_count\"),\n",
    "            )\n",
    "            .orderBy(F.col(\"avg_time_in_cart_days\").desc_nulls_last())\n",
    "        )\n",
    "\n",
    "    # 3) Optional: time-to-purchase distribution buckets (e.g., same day, 1–3 days, >3 days)\n",
    "    buckets = (\n",
    "        completed_carts.withColumn(\n",
    "            \"time_to_purchase_bucket\",\n",
    "            F.when(F.col(\"time_in_cart_days\") <= 0, \"same_day_or_less\")\n",
    "            .when(F.col(\"time_in_cart_days\") <= 1, \"1_day\")\n",
    "            .when(F.col(\"time_in_cart_days\") <= 3, \"2-3_days\")\n",
    "            .otherwise(\">3_days\"),\n",
    "        )\n",
    "        .groupBy(\"time_to_purchase_bucket\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"cart_id\").alias(\"completed_carts\"),\n",
    "            F.avg(\"time_in_cart_days\").alias(\"avg_time_in_cart_days\"),\n",
    "        )\n",
    "        .orderBy(\"time_to_purchase_bucket\")\n",
    "    )\n",
    "\n",
    "    analysis[\"time_to_purchase_buckets\"] = buckets\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_cart_abandonment_analysis not available, or cart_id all NULL/zero; \"\n",
    "        \"skipping average time-to-purchase analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f345be",
   "metadata": {},
   "source": [
    "# Campaign Performance\n",
    "\n",
    "Impressions → clicks → conversionsCTR ,ROAS ,ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6a99a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Campaign Performance: Impressions → Clicks → Conversions, CTR, ROAS, ROI\n",
    "\n",
    "if (\n",
    "    \"agg_marketing_campaigns\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_marketing_campaigns\"], \"campaign_id\")\n",
    "):\n",
    "    cm = dataframes[\"agg_marketing_campaigns\"].fillna(\n",
    "        {\n",
    "            \"impressions\": 0,\n",
    "            \"clicks\": 0,\n",
    "            \"conversions\": 0,\n",
    "            \"total_impressions\": 0,\n",
    "            \"total_clicks\": 0,\n",
    "            \"total_conversions\": 0,\n",
    "            \"revenue_generated\": 0.0,\n",
    "            \"spent_amount\": 0.0,\n",
    "            \"budget\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Prefer existing derived fields when present\n",
    "    cm_perf = (\n",
    "        cm\n",
    "        # Base funnel metrics\n",
    "        .withColumn(\n",
    "            \"effective_impressions\",\n",
    "            F.when(F.col(\"total_impressions\") > 0, F.col(\"total_impressions\")).otherwise(F.col(\"impressions\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"effective_clicks\",\n",
    "            F.when(F.col(\"total_clicks\") > 0, F.col(\"total_clicks\")).otherwise(F.col(\"clicks\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"effective_conversions\",\n",
    "            F.when(F.col(\"total_conversions\") > 0, F.col(\"total_conversions\")).otherwise(F.col(\"conversions\"))\n",
    "        )\n",
    "        # CTR: use existing ctr if not null, else compute\n",
    "        .withColumn(\n",
    "            \"ctr_effective\",\n",
    "            F.when(\n",
    "                F.col(\"ctr\").isNotNull(),\n",
    "                F.col(\"ctr\")\n",
    "            ).otherwise(\n",
    "                F.when(\n",
    "                    F.col(\"effective_impressions\") > 0,\n",
    "                    F.col(\"effective_clicks\") / F.col(\"effective_impressions\")\n",
    "                ).otherwise(F.lit(0.0))\n",
    "            )\n",
    "        )\n",
    "        # Conversion rate: use existing conversion_rate if present\n",
    "        .withColumn(\n",
    "            \"conversion_rate_effective\",\n",
    "            F.when(\n",
    "                F.col(\"conversion_rate\").isNotNull(),\n",
    "                F.col(\"conversion_rate\")\n",
    "            ).otherwise(\n",
    "                F.when(\n",
    "                    F.col(\"effective_clicks\") > 0,\n",
    "                    F.col(\"effective_conversions\") / F.col(\"effective_clicks\")\n",
    "                ).otherwise(F.lit(0.0))\n",
    "            )\n",
    "        )\n",
    "        # ROAS: use existing roas if present, else compute revenue_generated / spent_amount\n",
    "        .withColumn(\n",
    "            \"roas_effective\",\n",
    "            F.when(\n",
    "                F.col(\"roas\").isNotNull(),\n",
    "                F.col(\"roas\")\n",
    "            ).otherwise(\n",
    "                F.when(\n",
    "                    F.col(\"spent_amount\") > 0,\n",
    "                    F.col(\"revenue_generated\") / F.col(\"spent_amount\")\n",
    "                ).otherwise(F.lit(None))\n",
    "            )\n",
    "        )\n",
    "        # ROI: use existing roi if present, else (revenue - spend) / spend\n",
    "        .withColumn(\n",
    "            \"roi_effective\",\n",
    "            F.when(\n",
    "                F.col(\"roi\").isNotNull(),\n",
    "                F.col(\"roi\")\n",
    "            ).otherwise(\n",
    "                F.when(\n",
    "                    F.col(\"spent_amount\") > 0,\n",
    "                    (F.col(\"revenue_generated\") - F.col(\"spent_amount\")) / F.col(\"spent_amount\")\n",
    "                ).otherwise(F.lit(None))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Store concise performance view\n",
    "    analysis[\"campaign_performance\"] = (\n",
    "        cm_perf.select(\n",
    "            \"campaign_id\",\n",
    "            \"campaign_name\",\n",
    "            \"campaign_type\",\n",
    "            \"campaign_status\",\n",
    "            \"performance_tier\",\n",
    "            \"budget\",\n",
    "            \"spent_amount\",\n",
    "            \"effective_impressions\",\n",
    "            \"effective_clicks\",\n",
    "            \"effective_conversions\",\n",
    "            \"revenue_generated\",\n",
    "            \"avg_order_value\",\n",
    "            \"ctr_effective\",\n",
    "            \"conversion_rate_effective\",\n",
    "            \"roas_effective\",\n",
    "            \"roi_effective\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"revenue_generated\").desc_nulls_last(),\n",
    "            F.col(\"roas_effective\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_marketing_campaigns not available, or campaign_id column is all NULL/zero; \"\n",
    "        \"skipping campaign performance analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1240d",
   "metadata": {},
   "source": [
    "Device-Based Conversion Rates: Mobile vs Desktop performance from device_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2661bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Device-Based Conversion Rates: Mobile vs Desktop from device_used\n",
    "if (\n",
    "    \"agg_cart_abandonment_analysis\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_cart_abandonment_analysis\"], \"cart_id\")\n",
    "):\n",
    "    carts = dataframes[\"agg_cart_abandonment_analysis\"].fillna(\n",
    "        {\n",
    "            \"device_used\": \"unknown\",\n",
    "            \"session_converted\": 0,\n",
    "            \"cart_status_derived\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Normalize a simple converted flag:\n",
    "    # - primary: session_converted\n",
    "    # - backup: status-derived indicates purchased/completed\n",
    "    carts = carts.withColumn(\n",
    "        \"converted_flag\",\n",
    "        F.when(F.col(\"session_converted\") == 1, 1)\n",
    "        .when(\n",
    "            F.lower(F.col(\"cart_status_derived\")).isin(\"purchased\", \"completed\", \"converted\"),\n",
    "            1,\n",
    "        )\n",
    "        .otherwise(0),\n",
    "    )\n",
    "\n",
    "    # Abandoned flag from status\n",
    "    carts = carts.withColumn(\n",
    "        \"abandoned_flag\",\n",
    "        F.when(\n",
    "            F.lower(F.col(\"cart_status_derived\")).isin(\"abandoned\", \"lost\", \"expired\"),\n",
    "            1,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    # Aggregate by device_used\n",
    "    device_stats = (\n",
    "        carts.groupBy(\"device_used\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"cart_id\").alias(\"carts\"),\n",
    "            F.sum(\"converted_flag\").alias(\"converted_carts\"),\n",
    "            F.sum(\"abandoned_flag\").alias(\"abandoned_carts\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"conversion_rate\",\n",
    "            F.when(F.col(\"carts\") > 0, F.col(\"converted_carts\") / F.col(\"carts\")).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"abandonment_rate\",\n",
    "            F.when(F.col(\"carts\") > 0, F.col(\"abandoned_carts\") / F.col(\"carts\")).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(F.col(\"conversion_rate\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    analysis[\"device_conversion_rates\"] = device_stats\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_cart_abandonment_analysis not available, or cart_id all NULL/zero; \"\n",
    "        \"skipping device-based conversion rates analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a70c26",
   "metadata": {},
   "source": [
    "# Pyament patterns analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad8dcf2",
   "metadata": {},
   "source": [
    "Payment Method Preference by Geography: Map payment_method adoption by country/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f49673cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "has_payments = \"agg_payments\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_payments\"], \"payment_id\"\n",
    ")\n",
    "has_customers = \"agg_customers\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_customers\"], \"customer_id\"\n",
    ")\n",
    "has_orders = \"agg_orders\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_orders\"], \"order_id\"\n",
    ")\n",
    "\n",
    "if not (has_payments and has_customers and has_orders):\n",
    "    print(\n",
    "        \"agg_payments, agg_orders, or agg_customers missing/invalid; \"\n",
    "        \"cannot compute payment counts by country & method.\"\n",
    "    )\n",
    "else:\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"payment_method\",\n",
    "        \"payment_status\",\n",
    "    )\n",
    "\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "    )\n",
    "\n",
    "    customers = dataframes[\"agg_customers\"].select(\n",
    "        \"customer_id\",\n",
    "        \"country\",\n",
    "        \"state_province\",\n",
    "    )\n",
    "\n",
    "    pay_geo = (\n",
    "        payments.alias(\"p\")\n",
    "        .join(orders.alias(\"o\"), on=\"order_id\", how=\"left\")\n",
    "        .join(customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    "        .fillna(\n",
    "            {\n",
    "                \"payment_method\": \"unknown\",\n",
    "                \"country\": \"unknown\",\n",
    "                \"state_province\": \"unknown\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional: keep only non-failed payments\n",
    "    pay_geo = pay_geo.filter(\n",
    "        ~F.lower(F.col(\"payment_status\")).isin(\"failed\", \"declined\", \"cancelled\",\"incomplete\",\"unknown\")\n",
    "    )\n",
    "\n",
    "    # 1) By country & payment_method\n",
    "    analysis[\"payment_counts_by_country_method\"] = (\n",
    "        pay_geo.groupBy(\"country\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"payment_count\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"order_count\"),\n",
    "        )\n",
    "        .orderBy(\"country\", F.col(\"payment_count\").desc())\n",
    "    )\n",
    "\n",
    "    # 2) By state & payment_method (optional)\n",
    "    analysis[\"payment_counts_by_state_method\"] = (\n",
    "        pay_geo.groupBy(\"country\", \"state_province\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"payment_count\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"order_count\"),\n",
    "        )\n",
    "        .orderBy(\"country\", \"state_province\", F.col(\"payment_count\").desc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1bb89b",
   "metadata": {},
   "source": [
    "Payment Method Success Rates: payment_status = 'completed' by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "472ad4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "has_payments = \"agg_payments\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_payments\"], \"payment_id\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Overall success rate by payment_method\n",
    "# -------------------------------------------------------------------\n",
    "if has_payments:\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"payment_method\",\n",
    "        \"payment_status\",\n",
    "    ).fillna(\n",
    "        {\n",
    "            \"payment_method\": \"unknown\",\n",
    "            \"payment_status\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Flag completed payments\n",
    "    \n",
    "    payments = payments.withColumn(\n",
    "        \"is_completed\",\n",
    "        F.when(\n",
    "            F.lower(F.col(\"payment_status\")).isin(\"completed\", \"successful\",\"success\",\"done\",\"transfered\",\"was_successful\"),\n",
    "            1,\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "\n",
    "    analysis[\"payment_method_success_rates\"] = (\n",
    "        payments.groupBy(\"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"total_payments\"),\n",
    "            F.sum(\"is_completed\").alias(\"completed_payments\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"distinct_orders\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"success_rate\",\n",
    "            F.when(\n",
    "                F.col(\"total_payments\") > 0,\n",
    "                F.col(\"completed_payments\") / F.col(\"total_payments\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(F.col(\"success_rate\").desc_nulls_last())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"agg_payments not available, or payment_id all NULL/zero; \"\n",
    "        \"skipping payment method success rates.\"\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) OPTIONAL: Success rate by payment_method + country\n",
    "# -------------------------------------------------------------------\n",
    "has_customers = \"agg_customers\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_customers\"], \"customer_id\"\n",
    ")\n",
    "has_orders = \"agg_orders\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_orders\"], \"order_id\"\n",
    ")\n",
    "\n",
    "if has_payments and has_customers and has_orders:\n",
    "    orders = dataframes[\"agg_orders\"].select(\"order_id\", \"customer_id\")\n",
    "    customers = dataframes[\"agg_customers\"].select(\"customer_id\", \"country\")\n",
    "\n",
    "    pay_geo = (\n",
    "        payments.alias(\"p\")          # reuse 'payments' with is_completed flag\n",
    "        .join(orders.alias(\"o\"), on=\"order_id\", how=\"left\")\n",
    "        .join(customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    "        .fillna({\"country\": \"unknown\"})\n",
    "    )\n",
    "\n",
    "    analysis[\"payment_method_success_rates_by_country\"] = (\n",
    "        pay_geo.groupBy(\"country\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"total_payments\"),\n",
    "            F.sum(\"is_completed\").alias(\"completed_payments\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"distinct_orders\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"success_rate\",\n",
    "            F.when(\n",
    "                F.col(\"total_payments\") > 0,\n",
    "                F.col(\"completed_payments\") / F.col(\"total_payments\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(\"country\", F.col(\"success_rate\").desc_nulls_last())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c68c0a",
   "metadata": {},
   "source": [
    "Payment Method AOV Correlation: Do certain methods correlate with higher order values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "89f8c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "has_payments = \"agg_payments\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_payments\"], \"payment_id\"\n",
    ")\n",
    "has_orders = \"agg_orders\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_orders\"], \"order_id\"\n",
    ")\n",
    "\n",
    "if not (has_payments and has_orders):\n",
    "    print(\n",
    "        \"agg_payments or agg_orders missing/invalid; \"\n",
    "        \"cannot compute payment method AOV correlation.\"\n",
    "    )\n",
    "else:\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"payment_method\",\n",
    "        \"payment_status\",\n",
    "    ).fillna(\n",
    "        {\n",
    "            \"payment_method\": \"unknown\",\n",
    "            \"payment_status\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"total_amount\",\n",
    "    ).fillna({\"total_amount\": 0.0})\n",
    "\n",
    "    # Join payments to orders to get order value per payment\n",
    "    pay_orders = (\n",
    "        payments.alias(\"p\")\n",
    "        .join(orders.alias(\"o\"), on=\"order_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Optional: focus only on completed payments\n",
    "    pay_orders = pay_orders.filter(F.lower(F.col(\"payment_status\")) == \"completed\")\n",
    "\n",
    "    analysis[\"payment_method_aov\"] = (\n",
    "        pay_orders.groupBy(\"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"payment_count\"),\n",
    "            F.countDistinct(\"order_id\").alias(\"order_count\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_order_value_method\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"avg_order_value_method\").desc_nulls_last())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84f677",
   "metadata": {},
   "source": [
    "Refund Rate by Payment Method: refund_amount / total payments by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "11ba7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "has_payments = \"agg_payments\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_payments\"], \"payment_id\"\n",
    ")\n",
    "\n",
    "if not has_payments:\n",
    "    print(\n",
    "        \"agg_payments not available, or payment_id all NULL/zero; \"\n",
    "        \"skipping refund rate by payment method.\"\n",
    "    )\n",
    "else:\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"payment_method\",\n",
    "        \"payment_status\",\n",
    "        \"processing_fee\",\n",
    "        \"refund_amount\",\n",
    "    ).fillna(\n",
    "        {\n",
    "            \"payment_method\": \"unknown\",\n",
    "            \"payment_status\": \"unknown\",\n",
    "            \"processing_fee\": 0.0,\n",
    "            \"refund_amount\": 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Flag payments that had any refund\n",
    "    payments = payments.withColumn(\n",
    "        \"has_refund\",\n",
    "        F.when(F.col(\"refund_amount\") > 0, 1).otherwise(0),\n",
    "    )\n",
    "\n",
    "    # Aggregate by payment_method\n",
    "    analysis[\"refund_rate_by_payment_method\"] = (\n",
    "        payments.groupBy(\"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"total_payments\"),\n",
    "            F.sum(\"refund_amount\").alias(\"total_refund_amount\"),\n",
    "            F.sum(\"has_refund\").alias(\"payments_with_refund\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            # 1) Share of payments that had a refund (count-based rate)\n",
    "            \"refund_rate_payments\",\n",
    "            F.when(\n",
    "                F.col(\"total_payments\") > 0,\n",
    "                F.col(\"payments_with_refund\") / F.col(\"total_payments\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .withColumn(\n",
    "            # 2) Avg refund amount per payment (including 0s)\n",
    "            \"avg_refund_per_payment\",\n",
    "            F.when(\n",
    "                F.col(\"total_payments\") > 0,\n",
    "                F.col(\"total_refund_amount\") / F.col(\"total_payments\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(F.col(\"refund_rate_payments\").desc_nulls_last())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d67b5",
   "metadata": {},
   "source": [
    "Refund rate by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "40f994e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "has_payments = \"agg_payments\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_payments\"], \"payment_id\"\n",
    ")\n",
    "has_order_items = \"agg_order_items\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_order_items\"], \"order_item_id\"\n",
    ")\n",
    "has_products = \"agg_products\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_products\"], \"product_id\"\n",
    ")\n",
    "\n",
    "if not (has_payments and has_order_items):\n",
    "    print(\n",
    "        \"agg_payments or agg_order_items missing/invalid; \"\n",
    "        \"cannot compute refund rate by product.\"\n",
    "    )\n",
    "else:\n",
    "    # Payments: which orders had refunds?\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"refund_amount\",\n",
    "    ).fillna({\"refund_amount\": 0.0})\n",
    "\n",
    "    # Flag refunded orders\n",
    "    refunded_orders = (\n",
    "        payments.withColumn(\n",
    "            \"has_refund\",\n",
    "            F.when(F.col(\"refund_amount\") > 0, 1).otherwise(0),\n",
    "        )\n",
    "        .groupBy(\"order_id\")\n",
    "        .agg(\n",
    "            F.max(\"has_refund\").alias(\"order_has_refund\"),\n",
    "            F.sum(\"refund_amount\").alias(\"order_total_refund_amount\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Order items: link orders to products\n",
    "    order_items = dataframes[\"agg_order_items\"].select(\n",
    "        \"order_id\",\n",
    "        \"order_item_id\",\n",
    "        \"product_id\",\n",
    "        \"quantity\",\n",
    "    )\n",
    "\n",
    "    # Attach refund flags to each product in the order\n",
    "    prod_refunds = (\n",
    "        order_items.alias(\"oi\")\n",
    "        .join(refunded_orders.alias(\"r\"), on=\"order_id\", how=\"left\")\n",
    "        .fillna({\"order_has_refund\": 0, \"order_total_refund_amount\": 0.0})\n",
    "    )\n",
    "\n",
    "    # Optional: bring in product names\n",
    "    if has_products:\n",
    "        prod_meta = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "        )\n",
    "        prod_refunds = prod_refunds.join(prod_meta, on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # Aggregate by product\n",
    "    analysis[\"refund_rate_by_product\"] = (\n",
    "        prod_refunds.groupBy(\"product_id\", *[c for c in [\"product_name\", \"category\"] if c in prod_refunds.columns])\n",
    "        .agg(\n",
    "            F.countDistinct(\"order_id\").alias(\"orders_for_product\"),\n",
    "            F.sum(\"order_has_refund\").alias(\"orders_with_refund\"),\n",
    "            F.sum(\"order_total_refund_amount\").alias(\"total_refund_amount\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"refund_rate_orders\",\n",
    "            F.when(\n",
    "                F.col(\"orders_for_product\") > 0,\n",
    "                F.col(\"orders_with_refund\") / F.col(\"orders_for_product\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(F.col(\"refund_rate_orders\").desc_nulls_last())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62dcb88",
   "metadata": {},
   "source": [
    "Refund rate by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5360913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "has_orders = \"agg_orders\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_orders\"], \"order_id\"\n",
    ")\n",
    "\n",
    "if not (has_payments and has_orders):\n",
    "    print(\n",
    "        \"agg_payments or agg_orders missing/invalid; \"\n",
    "        \"cannot compute refund rate by month.\"\n",
    "    )\n",
    "else:\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"refund_amount\",\n",
    "    ).fillna({\"refund_amount\": 0.0})\n",
    "\n",
    "    # Order-level refund flags\n",
    "    refunded_orders = (\n",
    "        payments.withColumn(\n",
    "            \"has_refund\",\n",
    "            F.when(F.col(\"refund_amount\") > 0, 1).otherwise(0),\n",
    "        )\n",
    "        .groupBy(\"order_id\")\n",
    "        .agg(\n",
    "            F.max(\"has_refund\").alias(\"order_has_refund\"),\n",
    "            F.sum(\"refund_amount\").alias(\"order_total_refund_amount\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"order_placed_year\",\n",
    "        \"order_placed_month\",\n",
    "        \"total_amount\",\n",
    "    ).fillna({\"total_amount\": 0.0})\n",
    "\n",
    "    # Join refund info to orders with date parts\n",
    "    orders_with_refund = (\n",
    "        orders.alias(\"o\")\n",
    "        .join(refunded_orders.alias(\"r\"), on=\"order_id\", how=\"left\")\n",
    "        .fillna({\"order_has_refund\": 0, \"order_total_refund_amount\": 0.0})\n",
    "    )\n",
    "\n",
    "    # Aggregate by year-month\n",
    "    analysis[\"refund_rate_by_month\"] = (\n",
    "        orders_with_refund.groupBy(\"order_placed_year\", \"order_placed_month\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"order_id\").alias(\"orders\"),\n",
    "            F.sum(\"order_has_refund\").alias(\"orders_with_refund\"),\n",
    "            F.sum(\"order_total_refund_amount\").alias(\"total_refund_amount\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_order_amount\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"refund_rate_orders\",\n",
    "            F.when(\n",
    "                F.col(\"orders\") > 0,\n",
    "                F.col(\"orders_with_refund\") / F.col(\"orders\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"refund_rate_amount\",\n",
    "            F.when(\n",
    "                F.col(\"total_order_amount\") > 0,\n",
    "                F.col(\"total_refund_amount\") / F.col(\"total_order_amount\"),\n",
    "            ).otherwise(F.lit(0.0)),\n",
    "        )\n",
    "        .orderBy(\"order_placed_year\", \"order_placed_month\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0144a73",
   "metadata": {},
   "source": [
    "Time to Refund Analysis: refund_date - payment_date by payment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "692294d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "has_payments = \"agg_payments\" in dataframes and not is_column_all_null_or_zero(\n",
    "    dataframes[\"agg_payments\"], \"payment_id\"\n",
    ")\n",
    "\n",
    "if not has_payments:\n",
    "    print(\n",
    "        \"agg_payments not available, or payment_id all NULL/zero; \"\n",
    "        \"skipping time to refund analysis.\"\n",
    "    )\n",
    "else:\n",
    "    payments = dataframes[\"agg_payments\"].select(\n",
    "        \"payment_id\",\n",
    "        \"order_id\",\n",
    "        \"payment_method\",\n",
    "        \"payment_status\",\n",
    "        \"payment_date\",\n",
    "        \"refund_date\",\n",
    "        \"refund_amount\",\n",
    "    ).fillna(\n",
    "        {\n",
    "            \"payment_method\": \"unknown\",\n",
    "            \"payment_status\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Only keep rows that actually have a refund date (i.e., a refund happened)\n",
    "    refunded = payments.filter(F.col(\"refund_date\").isNotNull())\n",
    "\n",
    "    # Compute time to refund in days (you can also compute hours if these were TIMESTAMPs)\n",
    "    refunded = refunded.withColumn(\n",
    "        \"days_to_refund\",\n",
    "        F.datediff(F.col(\"refund_date\"), F.col(\"payment_date\"))\n",
    "    )\n",
    "\n",
    "    # Basic distribution by payment method\n",
    "    analysis[\"time_to_refund_by_payment_method\"] = (\n",
    "        refunded.groupBy(\"payment_method\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"payment_id\").alias(\"refunded_payments\"),\n",
    "            F.sum(\"refund_amount\").alias(\"total_refund_amount\"),\n",
    "            F.avg(\"days_to_refund\").alias(\"avg_days_to_refund\"),\n",
    "            F.expr(\"percentile(days_to_refund, array(0.25, 0.5, 0.75))\").alias(\n",
    "                \"days_to_refund_percentiles\"\n",
    "            ),\n",
    "            F.min(\"days_to_refund\").alias(\"min_days_to_refund\"),\n",
    "            F.max(\"days_to_refund\").alias(\"max_days_to_refund\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"avg_days_to_refund\").asc_nulls_last())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8003d",
   "metadata": {},
   "source": [
    "# sentiment analysis by reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a9928",
   "metadata": {},
   "source": [
    "Review Velocity: Reviews per product over time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f2cf5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Base reviews table: agg_reviews\n",
    "# -------------------------------------------------------------------\n",
    "if (\n",
    "    \"agg_reviews\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"product_id\")\n",
    "):\n",
    "    reviews = dataframes[\"agg_reviews\"].select(\n",
    "        \"product_id\",\n",
    "        F.col(\"review_date\").cast(\"date\").alias(\"review_date\"),\n",
    "        \"rating\",\n",
    "    ).filter(F.col(\"review_date\").isNotNull())\n",
    "\n",
    "    # Optional enrichment with product metadata\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "        reviews = reviews.alias(\"r\").join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 1) Daily review velocity per product\n",
    "    # ----------------------------------------------------------------\n",
    "    analysis[\"review_velocity_daily\"] = (\n",
    "        reviews.groupBy(\"product_id\", \"review_date\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"daily_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating_daily\"),\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"review_date\")\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 2) Weekly review velocity per product\n",
    "    # ----------------------------------------------------------------\n",
    "    reviews_weekly = (\n",
    "        reviews\n",
    "        .withColumn(\"review_year\", F.year(\"review_date\"))\n",
    "        .withColumn(\"review_week\", F.weekofyear(\"review_date\"))\n",
    "        .withColumn(\"year_week\", F.date_format(F.col(\"review_date\"), \"yyyy-MM-dd\"))\n",
    "    )\n",
    "\n",
    "    analysis[\"review_velocity_weekly\"] = (\n",
    "        reviews_weekly.groupBy(\"product_id\", \"year_week\", \"review_year\", \"review_week\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"weekly_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating_weekly\"),\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"review_year\", \"review_week\")\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # 3) Monthly review velocity per product\n",
    "    # ----------------------------------------------------------------\n",
    "    reviews_monthly = (\n",
    "        reviews\n",
    "        .withColumn(\"review_year\", F.year(\"review_date\"))\n",
    "        .withColumn(\"review_month\", F.month(\"review_date\"))\n",
    "        .withColumn(\"year_month\", F.date_format(F.col(\"review_date\"), \"yyyy-MM\"))\n",
    "    )\n",
    "\n",
    "    analysis[\"review_velocity_monthly\"] = (\n",
    "        reviews_monthly.groupBy(\"product_id\", \"year_month\", \"review_year\", \"review_month\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"monthly_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating_monthly\"),\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"review_year\", \"review_month\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_reviews not available, or product_id all NULL/zero; \"\n",
    "        \"skipping review velocity analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026d8640",
   "metadata": {},
   "source": [
    "Sentiment by Product Category: Which categories get best/worst sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "aa57fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Sentiment by Product Category\n",
    "if (\n",
    "    \"agg_reviews\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"product_id\")\n",
    "    and \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "):\n",
    "    reviews = dataframes[\"agg_reviews\"].select(\n",
    "        \"product_id\",\n",
    "        \"rating\",\n",
    "        \"review_sentiment\"\n",
    "    )\n",
    "\n",
    "    products = dataframes[\"agg_products\"].select(\n",
    "        \"product_id\",\n",
    "        \"category\"\n",
    "    )\n",
    "\n",
    "    rev_prod = (\n",
    "        reviews.alias(\"r\")\n",
    "        .join(products.alias(\"p\"), on=\"product_id\", how=\"inner\")\n",
    "    )\n",
    "\n",
    "    # Optional: turn sentiment into numeric for scoring (simple mapping)\n",
    "    rev_prod = rev_prod.withColumn(\n",
    "        \"sentiment_score\",\n",
    "        F.when(F.lower(F.col(\"review_sentiment\")) == \"positive\", 1.0)\n",
    "         .when(F.lower(F.col(\"review_sentiment\")) == \"negative\", -1.0)\n",
    "         .when(F.lower(F.col(\"review_sentiment\")) == \"neutral\", 0.0)\n",
    "         .otherwise(F.lit(0.0))\n",
    "    )\n",
    "\n",
    "    analysis[\"sentiment_by_category\"] = (\n",
    "        rev_prod.groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "            F.expr(\"sum(CASE WHEN lower(review_sentiment) = 'positive' THEN 1 ELSE 0 END)\").alias(\"positive_reviews\"),\n",
    "            F.expr(\"sum(CASE WHEN lower(review_sentiment) = 'negative' THEN 1 ELSE 0 END)\").alias(\"negative_reviews\"),\n",
    "            F.expr(\"sum(CASE WHEN lower(review_sentiment) = 'neutral' THEN 1 ELSE 0 END)\").alias(\"neutral_reviews\"),\n",
    "            F.avg(\"sentiment_score\").alias(\"avg_sentiment_score\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"positive_share\",\n",
    "            F.col(\"positive_reviews\") / F.col(\"total_reviews\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"negative_share\",\n",
    "            F.col(\"negative_reviews\") / F.col(\"total_reviews\")\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"avg_sentiment_score\").desc_nulls_last(),\n",
    "            F.col(\"avg_rating\").desc_nulls_last()\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_reviews or agg_products not available, or product_id all NULL/zero; \"\n",
    "        \"skipping sentiment by product category analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774c0c7",
   "metadata": {},
   "source": [
    "Low-Rated Product Alert System: Products with declining sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8359b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Low-rated product monthly trends (rating only)\n",
    "if (\n",
    "    \"agg_reviews\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"rating\")\n",
    "    and \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "):\n",
    "    # 1) Base reviews with date + rating\n",
    "    reviews = dataframes[\"agg_reviews\"].select(\n",
    "        \"product_id\",\n",
    "        F.col(\"review_date\").cast(\"date\").alias(\"review_date\"),\n",
    "        \"rating\",\n",
    "    ).filter(F.col(\"review_date\").isNotNull())\n",
    "\n",
    "    # 2) Derive year/month keys\n",
    "    reviews_monthly = (\n",
    "        reviews\n",
    "        .withColumn(\"review_year\", F.year(\"review_date\"))\n",
    "        .withColumn(\"review_month\", F.month(\"review_date\"))\n",
    "        .withColumn(\"year_month\", F.date_format(F.col(\"review_date\"), \"yyyy-MM\"))\n",
    "    )\n",
    "\n",
    "    # 3) Monthly rating per product\n",
    "    monthly_ratings = (\n",
    "        reviews_monthly.groupBy(\"product_id\", \"year_month\", \"review_year\", \"review_month\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"monthly_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating_month\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 4) Optional: join with product metadata (only unique columns)\n",
    "    if (\n",
    "        \"agg_products\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    ):\n",
    "        prod = dataframes[\"agg_products\"].select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"category\",\n",
    "            \"total_units_sold\",\n",
    "            \"total_revenue\",\n",
    "        )\n",
    "\n",
    "        monthly_with_meta = (\n",
    "            monthly_ratings.alias(\"m\")\n",
    "            .join(prod.alias(\"p\"), on=\"product_id\", how=\"left\")\n",
    "        )\n",
    "    else:\n",
    "        monthly_with_meta = monthly_ratings\n",
    "\n",
    "    # 5) Full monthly rating trends\n",
    "    analysis[\"product_monthly_rating_trends\"] = (\n",
    "        monthly_with_meta.select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\", \"total_units_sold\", \"total_revenue\"]\n",
    "              if c in monthly_with_meta.columns],\n",
    "            \"year_month\",\n",
    "            \"review_year\",\n",
    "            \"review_month\",\n",
    "            \"monthly_reviews\",\n",
    "            \"avg_rating_month\",\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"review_year\", \"review_month\")\n",
    "    )\n",
    "\n",
    "    # 6) Low-rated monthly periods (rating-only thresholds)\n",
    "    MIN_REVIEWS_MONTH = 5\n",
    "    RATING_THRESHOLD = 3.5  # <= 3.5 stars is considered low\n",
    "\n",
    "    low_rated_months = (\n",
    "        monthly_with_meta\n",
    "        .filter(\n",
    "            (F.col(\"monthly_reviews\") >= MIN_REVIEWS_MONTH)\n",
    "            & (F.col(\"avg_rating_month\") <= RATING_THRESHOLD)\n",
    "        )\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            *[c for c in [\"product_name\", \"category\", \"total_units_sold\", \"total_revenue\"]\n",
    "              if c in monthly_with_meta.columns],\n",
    "            \"year_month\",\n",
    "            \"review_year\",\n",
    "            \"review_month\",\n",
    "            \"monthly_reviews\",\n",
    "            \"avg_rating_month\",\n",
    "        )\n",
    "        .orderBy(\"product_id\", \"review_year\", \"review_month\")\n",
    "    )\n",
    "\n",
    "    analysis[\"low_rated_product_monthly_trends_rating_only\"] = low_rated_months\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_reviews not available, or product_id all NULL/zero; \"\n",
    "        \"skipping rating-only low-rated product monthly trends.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef0e4b",
   "metadata": {},
   "source": [
    "Review Impact on Business\n",
    "Rating Threshold Analysis: Sales velocity at different rating tiers (4. 5+, 4.0-4. 5, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7b02013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Rating Threshold Analysis: Sales velocity at different rating tiers\n",
    "if (\n",
    "    \"agg_reviews\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"product_id\")\n",
    "    and \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_reviews\"], \"rating\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_units_sold\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"total_revenue\")\n",
    "):\n",
    "    # 1) Per-product average rating from reviews\n",
    "    reviews = dataframes[\"agg_reviews\"].select(\n",
    "        \"product_id\",\n",
    "        \"rating\",\n",
    "    )\n",
    "\n",
    "    per_product_rating = (\n",
    "        reviews.groupBy(\"product_id\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_reviews\"),\n",
    "            F.avg(\"rating\").alias(\"avg_rating_product\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2) Join with product sales metrics\n",
    "    products = dataframes[\"agg_products\"].select(\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"category\",\n",
    "        \"total_units_sold\",\n",
    "        \"total_revenue\",\n",
    "    )\n",
    "\n",
    "    rating_sales = (\n",
    "        per_product_rating.alias(\"r\")\n",
    "        .join(products.alias(\"p\"), on=\"product_id\", how=\"inner\")\n",
    "    )\n",
    "\n",
    "    # Optional filter: ignore products with too few reviews\n",
    "    MIN_REVIEWS = 5\n",
    "    rating_sales = rating_sales.filter(F.col(\"total_reviews\") >= MIN_REVIEWS)\n",
    "\n",
    "    # 3) Define rating tiers (you can tweak ranges / labels)\n",
    "    rating_sales = rating_sales.withColumn(\n",
    "        \"rating_tier\",\n",
    "        F.when(F.col(\"avg_rating_product\") >= 4.5, \"4.5 - 5.0\")\n",
    "         .when((F.col(\"avg_rating_product\") >= 4.0) & (F.col(\"avg_rating_product\") < 4.5), \"4.0 - 4.5\")\n",
    "         .when((F.col(\"avg_rating_product\") >= 3.5) & (F.col(\"avg_rating_product\") < 4.0), \"3.5 - 4.0\")\n",
    "         .when((F.col(\"avg_rating_product\") >= 3.0) & (F.col(\"avg_rating_product\") < 3.5), \"3.0 - 3.5\")\n",
    "         .otherwise(\"< 3.0\")\n",
    "    )\n",
    "\n",
    "    # 4) Aggregate sales velocity per rating tier\n",
    "    # (velocity here = total_units_sold; if you have time-normalized fields, plug them in)\n",
    "    tier_stats = (\n",
    "        rating_sales.groupBy(\"rating_tier\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"products_in_tier\"),\n",
    "            F.sum(\"total_units_sold\").alias(\"total_units_sold_tier\"),\n",
    "            F.sum(\"total_revenue\").alias(\"total_revenue_tier\"),\n",
    "            F.avg(\"total_units_sold\").alias(\"avg_units_sold_per_product\"),\n",
    "            F.avg(\"total_revenue\").alias(\"avg_revenue_per_product\"),\n",
    "            F.avg(\"avg_rating_product\").alias(\"avg_rating_in_tier\"),\n",
    "            F.avg(\"total_reviews\").alias(\"avg_reviews_per_product\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"rating_tier\").desc()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 5) Save detailed per-product mapping and tier summary\n",
    "    analysis[\"rating_tier_per_product\"] = rating_sales.select(\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"category\",\n",
    "        \"total_reviews\",\n",
    "        \"avg_rating_product\",\n",
    "        \"total_units_sold\",\n",
    "        \"total_revenue\",\n",
    "        \"rating_tier\",\n",
    "    )\n",
    "\n",
    "    analysis[\"rating_tier_sales_velocity\"] = tier_stats\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_reviews or agg_products not available, or product_id all NULL/zero; \"\n",
    "        \"skipping rating threshold sales velocity analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f0572",
   "metadata": {},
   "source": [
    "# ORDER FULFILLMENT & LOGISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d8bf1",
   "metadata": {},
   "source": [
    "Order Processing Time Analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "66215352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Order Processing Time by Product Category\n",
    "# =============================================================================\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and \"agg_order_items\" in dataframes\n",
    "    and \"agg_products\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_products\"], \"product_id\")\n",
    "):\n",
    "    orders_with_cat = (\n",
    "        dataframes[\"agg_order_items\"]. select(\"order_id\", \"product_id\", \"quantity\")\n",
    "        .join(dataframes[\"agg_products\"]. select(\"product_id\", \"category\", \"sub_category\"), on=\"product_id\", how=\"left\")\n",
    "        .join(\n",
    "            dataframes[\"agg_orders\"].select(\n",
    "                \"order_id\", \"order_processing_days_diff\", \"delivery_days_diff\", \"total_order_fulfillment_time_days\"\n",
    "            ),\n",
    "            on=\"order_id\", how=\"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Category-level aggregation\n",
    "    analysis[\"processing_by_category\"] = (\n",
    "        orders_with_cat\n",
    "        .groupBy(\"category\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"order_id\").alias(\"orders\"),\n",
    "            F.sum(\"quantity\").alias(\"total_units\"),\n",
    "            F.avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.avg(\"total_order_fulfillment_time_days\").alias(\"avg_total_fulfillment_days\"),\n",
    "            F.expr(\"percentile_approx(order_processing_days_diff, 0.5)\").alias(\"median_processing_days\"),\n",
    "            F.max(\"order_processing_days_diff\").alias(\"max_processing_days\")\n",
    "        )\n",
    "        .orderBy(F.col(\"avg_processing_days\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # Sub-category drill-down\n",
    "    if not is_column_all_null_or_zero(orders_with_cat, \"sub_category\"):\n",
    "        analysis[\"processing_by_subcategory\"] = (\n",
    "            orders_with_cat\n",
    "            .filter(F.col(\"sub_category\").isNotNull())\n",
    "            .groupBy(\"category\", \"sub_category\")\n",
    "            .agg(\n",
    "                F. countDistinct(\"order_id\").alias(\"orders\"),\n",
    "                F. avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "                F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "                F.expr(\"percentile_approx(order_processing_days_diff, 0.5)\").alias(\"median_processing_days\")\n",
    "            )\n",
    "            .orderBy(F.col(\"avg_processing_days\").desc_nulls_last())\n",
    "        )\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Peak Processing Times Analysis\n",
    "# =============================================================================\n",
    "if \"agg_orders\" in dataframes and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\"):\n",
    "    \n",
    "    orders_time = (\n",
    "        dataframes[\"agg_orders\"]\n",
    "        .select(\"order_id\", \"order_placed_at\", \"order_processing_days_diff\", \"delivery_days_diff\", \"total_amount\")\n",
    "        .filter(F.col(\"order_placed_at\").isNotNull())\n",
    "        .withColumn(\"order_hour\", F.hour(\"order_placed_at\"))\n",
    "        .withColumn(\"order_dow\", F.dayofweek(\"order_placed_at\"))\n",
    "        .withColumn(\"order_dow_name\", F.date_format(\"order_placed_at\", \"E\"))\n",
    "        .withColumn(\"is_weekend\", F.when(F.col(\"order_dow\").isin([1, 7]), 1).otherwise(0))\n",
    "    )\n",
    "\n",
    "    # Processing time by hour of day\n",
    "    analysis[\"processing_by_hour\"] = (\n",
    "        orders_time\n",
    "        .groupBy(\"order_hour\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"orders\"),\n",
    "            F.avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .orderBy(F.col(\"order_hour\").asc())\n",
    "    )\n",
    "\n",
    "    # Processing time by day of week\n",
    "    analysis[\"processing_by_day_of_week\"] = (\n",
    "        orders_time\n",
    "        .groupBy(\"order_dow\", \"order_dow_name\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"orders\"),\n",
    "            F.avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .orderBy(F.col(\"order_dow\").asc())\n",
    "    )\n",
    "\n",
    "    # Weekend vs Weekday comparison\n",
    "    analysis[\"weekend_vs_weekday\"] = (\n",
    "        orders_time\n",
    "        .groupBy(\"is_weekend\")\n",
    "        .agg(\n",
    "            F. count(\"*\").alias(\"orders\"),\n",
    "            F.avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .withColumn(\"day_type\", F.when(F.col(\"is_weekend\") == 1, \"Weekend\").otherwise(\"Weekday\"))\n",
    "        .select(\"day_type\", \"orders\", \"avg_processing_days\", \"avg_delivery_days\", \"total_revenue\")\n",
    "        .orderBy(F.col(\"is_weekend\").asc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb13c2",
   "metadata": {},
   "source": [
    "Delivery Time Performance:\n",
    "delivery_days_diff by geography (country/state/city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "42e2edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "# Base orders with delivery info\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "    and \"agg_customers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "):\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_delivered_at\",\n",
    "        \"delivery_days_diff\",\n",
    "    ).filter(F.col(\"order_delivered_at\").isNotNull())\n",
    "\n",
    "    customers = dataframes[\"agg_customers\"].select(\n",
    "        \"customer_id\",\n",
    "        \"country\",\n",
    "        \"state_province\",\n",
    "        \"city\",\n",
    "    )\n",
    "\n",
    "    # Join orders -> customers to bring geography in\n",
    "    orders_geo = (\n",
    "        orders.alias(\"o\")\n",
    "        .join(customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # --- By country ---\n",
    "    analysis[\"delivery_days_by_country\"] = (\n",
    "        orders_geo.groupBy(\"country\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"delivered_orders\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.expr(\"percentile_approx(delivery_days_diff, 0.5)\").alias(\"median_delivery_days\"),\n",
    "            F.max(\"delivery_days_diff\").alias(\"max_delivery_days\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"avg_delivery_days\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # --- By state (within country) ---\n",
    "    analysis[\"delivery_days_by_state\"] = (\n",
    "        orders_geo.groupBy(\"country\", \"state_province\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"delivered_orders\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.expr(\"percentile_approx(delivery_days_diff, 0.5)\").alias(\"median_delivery_days\"),\n",
    "            F.max(\"delivery_days_diff\").alias(\"max_delivery_days\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"avg_delivery_days\").desc_nulls_last(),\n",
    "            \"country\",\n",
    "            \"state_province\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- By city (within state/country) ---\n",
    "    analysis[\"delivery_days_by_city\"] = (\n",
    "        orders_geo.groupBy(\"country\", \"state_province\", \"city\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"delivered_orders\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "            F.expr(\"percentile_approx(delivery_days_diff, 0.5)\").alias(\"median_delivery_days\"),\n",
    "            F.max(\"delivery_days_diff\").alias(\"max_delivery_days\"),\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"avg_delivery_days\").desc_nulls_last(),\n",
    "            \"country\",\n",
    "            \"state_province\",\n",
    "            \"city\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_orders or agg_customers not available, or key ID columns NULL/zero; \"\n",
    "        \"skipping delivery_days_diff by geography.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a33a20",
   "metadata": {},
   "source": [
    "On‑time delivery %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "25ee96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Approximate on-time delivery using a simple SLA threshold\n",
    "SLA_DAYS = 3  # adjust to your promised standard (e.g., 2, 3, 5)\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "    and \"agg_customers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "):\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_delivered_at\",\n",
    "        \"delivery_days_diff\",\n",
    "    ).filter(F.col(\"order_delivered_at\").isNotNull())\n",
    "\n",
    "    customers = dataframes[\"agg_customers\"].select(\n",
    "        \"customer_id\",\n",
    "        \"country\",\n",
    "        \"state_province\",\n",
    "        \"city\",\n",
    "    )\n",
    "\n",
    "    orders_geo = (\n",
    "        orders.alias(\"o\")\n",
    "        .join(customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    orders_geo = orders_geo.withColumn(\n",
    "        \"is_on_time\",\n",
    "        F.when(F.col(\"delivery_days_diff\") <= SLA_DAYS, F.lit(1)).otherwise(F.lit(0)),\n",
    "    )\n",
    "\n",
    "    # --- On-time delivery % by country ---\n",
    "    analysis[\"ontime_delivery_by_country\"] = (\n",
    "        orders_geo.groupBy(\"country\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"delivered_orders\"),\n",
    "            F.sum(\"is_on_time\").alias(\"on_time_orders\"),\n",
    "            (F.sum(\"is_on_time\") / F.count(\"*\")).alias(\"on_time_rate\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"on_time_rate\").asc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # --- On-time delivery % by state ---\n",
    "    analysis[\"ontime_delivery_by_state\"] = (\n",
    "        orders_geo.groupBy(\"country\", \"state_province\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"delivered_orders\"),\n",
    "            F.sum(\"is_on_time\").alias(\"on_time_orders\"),\n",
    "            (F.sum(\"is_on_time\") / F.count(\"*\")).alias(\"on_time_rate\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"on_time_rate\").asc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # --- On-time delivery % by city ---\n",
    "    analysis[\"ontime_delivery_by_city\"] = (\n",
    "        orders_geo.groupBy(\"country\", \"state_province\", \"city\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"delivered_orders\"),\n",
    "            F.sum(\"is_on_time\").alias(\"on_time_orders\"),\n",
    "            (F.sum(\"is_on_time\") / F.count(\"*\")).alias(\"on_time_rate\"),\n",
    "            F.avg(\"delivery_days_diff\").alias(\"avg_delivery_days\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"on_time_rate\").asc_nulls_last())\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_orders or agg_customers not available, or key ID columns NULL/zero; \"\n",
    "        \"skipping on-time delivery approximation.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d48dac",
   "metadata": {},
   "source": [
    "Shipping Cost Efficiency: shipping_cost as % of subtotal by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "de34962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "if (\n",
    "    \"agg_orders\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\")\n",
    "    and \"agg_customers\" in dataframes\n",
    "    and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "):\n",
    "    # 1) Base orders: keep only fields needed\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"subtotal\",\n",
    "        \"shipping_cost\",\n",
    "    )\n",
    "\n",
    "    # Avoid div-by-zero: ship% only where subtotal > 0\n",
    "    orders = orders.withColumn(\n",
    "        \"shipping_pct_of_subtotal\",\n",
    "        F.when(\n",
    "            F.col(\"subtotal\") > 0,\n",
    "            F.col(\"shipping_cost\") / F.col(\"subtotal\")\n",
    "        ).otherwise(F.lit(None)),\n",
    "    )\n",
    "\n",
    "    # 2) Customers: region info\n",
    "    customers = dataframes[\"agg_customers\"].select(\n",
    "        \"customer_id\",\n",
    "        \"country\",\n",
    "        \"state_province\",\n",
    "        \"city\",\n",
    "    )\n",
    "\n",
    "    # 3) Join orders -> customers\n",
    "    orders_geo = (\n",
    "        orders.alias(\"o\")\n",
    "        .join(customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Common aggregations used for each region level\n",
    "    def region_agg(df, group_cols):\n",
    "        return (\n",
    "            df.groupBy(*group_cols)\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"orders\"),\n",
    "                F.sum(\"shipping_cost\").alias(\"total_shipping_cost\"),\n",
    "                F.sum(\"subtotal\").alias(\"total_subtotal\"),\n",
    "                F.avg(\"shipping_pct_of_subtotal\").alias(\"avg_shipping_pct_of_subtotal\"),\n",
    "                F.expr(\n",
    "                    \"percentile_approx(shipping_pct_of_subtotal, 0.5)\"\n",
    "                ).alias(\"median_shipping_pct_of_subtotal\"),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"shipping_pct_of_subtotal_overall\",\n",
    "                F.when(\n",
    "                    F.col(\"total_subtotal\") > 0,\n",
    "                    F.col(\"total_shipping_cost\") / F.col(\"total_subtotal\"),\n",
    "                ).otherwise(F.lit(None)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4) By country\n",
    "    analysis[\"shipping_efficiency_by_country\"] = (\n",
    "        region_agg(orders_geo, [\"country\"])\n",
    "        .orderBy(\n",
    "            F.col(\"avg_shipping_pct_of_subtotal\").desc_nulls_last(),\n",
    "            \"country\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 5) By state (within country)\n",
    "    analysis[\"shipping_efficiency_by_state\"] = (\n",
    "        region_agg(orders_geo, [\"country\", \"state_province\"])\n",
    "        .orderBy(\n",
    "            F.col(\"avg_shipping_pct_of_subtotal\").desc_nulls_last(),\n",
    "            \"country\",\n",
    "            \"state_province\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 6) By city (within state/country)\n",
    "    analysis[\"shipping_efficiency_by_city\"] = (\n",
    "        region_agg(orders_geo, [\"country\", \"state_province\", \"city\"])\n",
    "        .orderBy(\n",
    "            F.col(\"avg_shipping_pct_of_subtotal\").desc_nulls_last(),\n",
    "            \"country\",\n",
    "            \"state_province\",\n",
    "            \"city\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_orders or agg_customers not available, or key ID columns NULL/zero; \"\n",
    "        \"skipping shipping cost efficiency by region.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328d58d",
   "metadata": {},
   "source": [
    "Processing Delays by Season: season impact on order_processing_days_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ddf8546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "if \"agg_orders\" in dataframes and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\"):\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"order_status\",\n",
    "        \"season\",\n",
    "        \"order_processing_days_diff\",\n",
    "    ).filter(F.col(\"season\").isNotNull())\n",
    "\n",
    "    # 1) Processing delays by season\n",
    "    analysis[\"processing_by_season\"] = (\n",
    "        orders.groupBy(\"season\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"orders\"),\n",
    "            F.avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "            F.expr(\n",
    "                \"percentile_approx(order_processing_days_diff, 0.5)\"\n",
    "            ).alias(\"median_processing_days\"),\n",
    "            F.max(\"order_processing_days_diff\").alias(\"max_processing_days\"),\n",
    "        )\n",
    "        .orderBy(F.col(\"avg_processing_days\").desc_nulls_last())\n",
    "    )\n",
    "\n",
    "    # 2) Optional: breakdown by season + order_status (e.g., shipped, delivered)\n",
    "    analysis[\"processing_by_season_and_status\"] = (\n",
    "        orders.groupBy(\"season\", \"order_status\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"orders\"),\n",
    "            F.avg(\"order_processing_days_diff\").alias(\"avg_processing_days\"),\n",
    "            F.expr(\n",
    "                \"percentile_approx(order_processing_days_diff, 0.5)\"\n",
    "            ).alias(\"median_processing_days\"),\n",
    "        )\n",
    "        .orderBy(\"season\", F.col(\"avg_processing_days\").desc_nulls_last())\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"agg_orders not available, or order_id all NULL/zero; \"\n",
    "        \"skipping processing delays by season analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d14c2",
   "metadata": {},
   "source": [
    "Shipping Cost Outliers: Orders with unusually high shipping costs for investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c0e48893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "if \"analysis\" not in locals():\n",
    "    analysis = {}\n",
    "\n",
    "if \"agg_orders\" in dataframes and not is_column_all_null_or_zero(dataframes[\"agg_orders\"], \"order_id\"):\n",
    "    # 1) Base orders\n",
    "    orders = dataframes[\"agg_orders\"].select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"order_placed_at\",\n",
    "        \"subtotal\",\n",
    "        \"shipping_cost\",\n",
    "        \"order_status\",\n",
    "    )\n",
    "\n",
    "    # 2) Shipping as % of subtotal (avoid div-by-zero)\n",
    "    orders = orders.withColumn(\n",
    "        \"shipping_pct_of_subtotal\",\n",
    "        F.when(\n",
    "            F.col(\"subtotal\") > 0,\n",
    "            F.col(\"shipping_cost\") / F.col(\"subtotal\"),\n",
    "        ).otherwise(F.lit(None)),\n",
    "    )\n",
    "\n",
    "    # 3) Compute global stats for shipping_pct_of_subtotal\n",
    "    stats = orders.select(\n",
    "        F.avg(\"shipping_pct_of_subtotal\").alias(\"mean_pct\"),\n",
    "        F.stddev(\"shipping_pct_of_subtotal\").alias(\"std_pct\"),\n",
    "    ).collect()[0]\n",
    "\n",
    "    mean_pct = stats[\"mean_pct\"] or 0.0\n",
    "    std_pct = stats[\"std_pct\"] or 0.0\n",
    "\n",
    "    # Configure thresholds\n",
    "    # - MIN_SUBTOTAL: ignore tiny orders where shipping naturally dominates\n",
    "    # - Z_THRESHOLD: how many std devs above mean to call \"outlier\"\n",
    "    # - PCT_ABS_THRESHOLD: also require pct itself be above an absolute floor\n",
    "    MIN_SUBTOTAL = 10.0       # currency units\n",
    "    Z_THRESHOLD = 3.0         # > 3 standard deviations\n",
    "    PCT_ABS_THRESHOLD = 0.5   # > 50% of subtotal\n",
    "\n",
    "    if std_pct is None:\n",
    "        std_pct = 0.0\n",
    "\n",
    "    # 4) Add z-score column (handle std = 0)\n",
    "    orders_with_z = orders.withColumn(\n",
    "        \"shipping_pct_zscore\",\n",
    "        F.when(\n",
    "            F.lit(std_pct) > 0,\n",
    "            (F.col(\"shipping_pct_of_subtotal\") - F.lit(mean_pct)) / F.lit(std_pct),\n",
    "        ).otherwise(F.lit(0.0)),\n",
    "    )\n",
    "\n",
    "    # 5) Filter outliers\n",
    "    outliers = (\n",
    "        orders_with_z\n",
    "        .filter(\n",
    "            (F.col(\"subtotal\") >= MIN_SUBTOTAL)\n",
    "            & (F.col(\"shipping_pct_of_subtotal\").isNotNull())\n",
    "            & (F.col(\"shipping_pct_of_subtotal\") >= PCT_ABS_THRESHOLD)\n",
    "            & (F.col(\"shipping_pct_zscore\") >= Z_THRESHOLD)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 6) Optional: bring in customer geography for investigation\n",
    "    if (\n",
    "        \"agg_customers\" in dataframes\n",
    "        and not is_column_all_null_or_zero(dataframes[\"agg_customers\"], \"customer_id\")\n",
    "    ):\n",
    "        customers = dataframes[\"agg_customers\"].select(\n",
    "            \"customer_id\",\n",
    "            \"country\",\n",
    "            \"state_province\",\n",
    "            \"city\",\n",
    "        )\n",
    "\n",
    "        outliers = (\n",
    "            outliers.alias(\"o\")\n",
    "            .join(customers.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    "        )\n",
    "\n",
    "    # 7) Final outlier table (explicit, unique columns)\n",
    "    analysis[\"shipping_cost_outliers\"] = (\n",
    "        outliers.select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            *[c for c in [\"country\", \"state_province\", \"city\"] if c in outliers.columns],\n",
    "            \"order_placed_at\",\n",
    "            \"order_status\",\n",
    "            \"subtotal\",\n",
    "            \"shipping_cost\",\n",
    "            \"shipping_pct_of_subtotal\",\n",
    "            \"shipping_pct_zscore\",\n",
    "        )\n",
    "        .orderBy(\n",
    "            F.col(\"shipping_pct_zscore\").desc_nulls_last(),\n",
    "            F.col(\"shipping_pct_of_subtotal\").desc_nulls_last(),\n",
    "            F.col(\"shipping_cost\").desc_nulls_last(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"agg_orders not available, or order_id all NULL/zero; \"\n",
    "        \"skipping shipping cost outlier analysis.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "cfbdff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "Damn Thats a lot of analyses!🧑🏿\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key in analysis.keys():\n",
    "    count+=1\n",
    "print(count)\n",
    "print(\"Damn Thats a lot of analyses!🧑🏿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5b33b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Damn Thats a lot of analyses!🧑🏿\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key in product_analysis.keys():\n",
    "    count+=1\n",
    "print(count)\n",
    "print(\"Damn Thats a lot of analyses!🧑🏿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c0769b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Damn Thats a lot of analyses!🧑🏿\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key in supplier_analysis.keys():\n",
    "    count+=1\n",
    "print(count)\n",
    "print(\"Damn Thats a lot of analyses!🧑🏿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4982a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|         grain_date|total_orders|total_units_sold|total_revenue|       gross_profit|         net_profit|               aov|        margin_pct|\n",
      "+-------------------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|1900-01-01 00:00:00|          86|            8171|  89514.41972|-4260099.7420000015|-2424411.9420000007|1040.8653455813953|-47.59121217928397|\n",
      "+-------------------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|grain_year|grain_week|total_orders|total_units_sold|total_revenue|       gross_profit|         net_profit|               aov|        margin_pct|\n",
      "+----------+----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|      1900|         1|          86|            8171|  89514.41972|-4260099.7420000015|-2424411.9420000007|1040.8653455813953|-47.59121217928397|\n",
      "+----------+----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|grain_year|grain_month|total_orders|total_units_sold|total_revenue|       gross_profit|         net_profit|               aov|        margin_pct|\n",
      "+----------+-----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "|      1900|          1|          86|            8171|  89514.41972|-4260099.7420000015|-2424411.9420000007|1040.8653455813953|-47.59121217928397|\n",
      "+----------+-----------+------------+----------------+-------------+-------------------+-------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----------------+---------------------+----------------------+----------+\n",
      "|   category|avg_profit_margin|total_category_profit|total_category_revenue|units_sold|\n",
      "+-----------+-----------------+---------------------+----------------------+----------+\n",
      "|Kitchen 964|              0.0|   48876.299999999996|    103817.12000000001|       631|\n",
      "+-----------+-----------------+---------------------+----------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+--------------+--------------+\n",
      "|grain_date|account_status|customer_count|\n",
      "+----------+--------------+--------------+\n",
      "|1900-01-01|          NULL|             4|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+--------------+--------------+\n",
      "|grain_date|account_status|customer_count|\n",
      "+----------+--------------+--------------+\n",
      "|1900-01-01|          NULL|             4|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+--------------+--------------+\n",
      "|grain_date|account_status|customer_count|\n",
      "+----------+--------------+--------------+\n",
      "|1900-01-01|          NULL|             4|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------+\n",
      "|grain_date|new_customers|\n",
      "+----------+-------------+\n",
      "|1900-01-01|           13|\n",
      "+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+-------------+\n",
      "|grain_year|grain_week|new_customers|\n",
      "+----------+----------+-------------+\n",
      "|      1900|         1|           13|\n",
      "+----------+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+-------------+\n",
      "|grain_year|grain_month|new_customers|\n",
      "+----------+-----------+-------------+\n",
      "|      1900|          1|           13|\n",
      "+----------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------+--------------------+\n",
      "|grain_date|new_customers|cumulative_customers|\n",
      "+----------+-------------+--------------------+\n",
      "|1900-01-01|           13|                  13|\n",
      "+----------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+-------------+--------------------+\n",
      "|grain_year|grain_week|new_customers|cumulative_customers|\n",
      "+----------+----------+-------------+--------------------+\n",
      "|      1900|         1|           13|                  13|\n",
      "+----------+----------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+-------------+--------------------+\n",
      "|grain_year|grain_month|new_customers|cumulative_customers|\n",
      "+----------+-----------+-------------+--------------------+\n",
      "|      1900|          1|           13|                  13|\n",
      "+----------+-----------+-------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------+------------------+----------+-------------+\n",
      "|grain_date|country|    state_province|      city|new_customers|\n",
      "+----------+-------+------------------+----------+-------------+\n",
      "|1900-01-01|Burundi|Northern Territory|Harveystad|            1|\n",
      "+----------+-------+------------------+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+-------+------------------+----------+-------------+\n",
      "|grain_year|grain_month|country|    state_province|      city|new_customers|\n",
      "+----------+-----------+-------+------------------+----------+-------------+\n",
      "|      1900|          1|Burundi|Northern Territory|Harveystad|            1|\n",
      "+----------+-----------+-------+------------------+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------+--------------+\n",
      "|customer_age_group|customer_count|\n",
      "+------------------+--------------+\n",
      "|             18-24|           107|\n",
      "+------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+-----------+--------------+\n",
      "|country|state_province|       city|customer_count|\n",
      "+-------+--------------+-----------+--------------+\n",
      "|   NULL|       Arizona|East Samuel|             1|\n",
      "+-------+--------------+-----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+--------------+\n",
      "|country|state_province|customer_count|\n",
      "+-------+--------------+--------------+\n",
      "|   NULL|       Arizona|             2|\n",
      "+-------+--------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+\n",
      "|country|customer_count|\n",
      "+-------+--------------+\n",
      "|   NULL|            20|\n",
      "+-------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------+--------------+---------------------+-------+-----------+-----------------------+\n",
      "|customer_age_group|customer_count|avg_order_total_spent|avg_clv|total_spent|total_revenue_age_group|\n",
      "+------------------+--------------+---------------------+-------+-----------+-----------------------+\n",
      "|             18-24|           107|                  0.0|    0.0|        0.0|                    0.0|\n",
      "+------------------+--------------+---------------------+-------+-----------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+-----------------+-----------+-----------------+------------+\n",
      "|gender|         category|total_units|distinct_products|orders_count|\n",
      "+------+-----------------+-----------+-----------------+------------+\n",
      "|  Male|Sports & Outdoors|         66|                3|           2|\n",
      "+------+-----------------+-----------+-----------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+----------+--------------------+-----------------+-----------+------------+\n",
      "|gender|product_id|        product_name|         category|total_units|orders_count|\n",
      "+------+----------+--------------------+-----------------+-----------+------------+\n",
      "|  Male|      1014|Microsoft Smartph...|Sports & Outdoors|         35|           1|\n",
      "+------+----------+--------------------+-----------------+-----------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+-------------+--------------+\n",
      "|country|customer_type|customer_count|\n",
      "+-------+-------------+--------------+\n",
      "|   NULL|          new|            20|\n",
      "+-------+-------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+-----------+-------------+--------------+\n",
      "|country|state_province|       city|customer_type|customer_count|\n",
      "+-------+--------------+-----------+-------------+--------------+\n",
      "|   NULL|       Arizona|East Samuel|          new|             1|\n",
      "+-------+--------------+-----------+-------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+--------------+------------------+---------------------+\n",
      "|customer_id|total_sessions|total_pages_viewed|total_products_viewed|\n",
      "+-----------+--------------+------------------+---------------------+\n",
      "|          1|             1|                 2|                 NULL|\n",
      "+-----------+--------------+------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------------+-------------------------+--------------------------------+-----------------------------+-----------------------------------+--------------------------------+\n",
      "|total_sessions_all_customers|avg_sessions_per_customer|total_pages_viewed_all_customers|avg_pages_viewed_per_customer|total_products_viewed_all_customers|avg_products_viewed_per_customer|\n",
      "+----------------------------+-------------------------+--------------------------------+-----------------------------+-----------------------------------+--------------------------------+\n",
      "|                         352|                    0.352|                            2294|            6.747058823529412|                                751|              3.0160642570281126|\n",
      "+----------------------------+-------------------------+--------------------------------+-----------------------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "+---------------------------+-------------------------+\n",
      "|avg_session_conversion_rate|avg_cart_abandonment_rate|\n",
      "+---------------------------+-------------------------+\n",
      "|        0.10571428571428572|      0.22952380952380952|\n",
      "+---------------------------+-------------------------+\n",
      "\n",
      "+-----------+-------------+-----------------------+----------------+-----------+\n",
      "|customer_id|total_revenue|customer_lifetime_value|customer_segment|rfm_segment|\n",
      "+-----------+-------------+-----------------------+----------------+-----------+\n",
      "|       1000|      9114.86|                9114.86|            Lost|        355|\n",
      "+-----------+-------------+-----------------------+----------------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+------------------+----------------+------------+----------------+-----------+-------------+-----------------------+\n",
      "|customer_id|total_order_profit|total_net_profit|orders_count|customer_segment|rfm_segment|total_revenue|customer_lifetime_value|\n",
      "+-----------+------------------+----------------+------------+----------------+-----------+-------------+-----------------------+\n",
      "|       1010|           1813.92|         1804.39|           1|            NULL|       NULL|         NULL|                   NULL|\n",
      "+-----------+------------------+----------------+------------+----------------+-----------+-------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------------------+--------------+\n",
      "|session_conversion_percentage|customer_count|\n",
      "+-----------------------------+--------------+\n",
      "|                         75%+|           687|\n",
      "+-----------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------------------+--------------+\n",
      "|cart_abandonment_percentage|customer_count|\n",
      "+---------------------------+--------------+\n",
      "|                     25–50%|             1|\n",
      "+---------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+-------+---------------+-----------------+-----------------------+---------------------------+\n",
      "|customers|avg_clv|clv_percentiles|avg_total_revenue|avg_order_value_overall|total_revenue_all_customers|\n",
      "+---------+-------+---------------+-----------------+-----------------------+---------------------------+\n",
      "|     1000|9.11486|[0.0, 0.0, 0.0]|          9.11486|     1519.1433333333334|                    9114.86|\n",
      "+---------+-------+---------------+-----------------+-----------------------+---------------------------+\n",
      "\n",
      "+-------+-----------+--------------+---------------+--------------------+-------------+\n",
      "|country|       city|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+-------+-----------+--------------+---------------+--------------------+-------------+\n",
      "|Ecuador|Wintersport|             1|        9114.86|             9114.86|          1.0|\n",
      "+-------+-----------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------+--------------+---------------+--------------------+-------------+\n",
      "|customer_segment|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+----------------+--------------+---------------+--------------------+-------------+\n",
      "|            Lost|             1|        9114.86|             9114.86|          1.0|\n",
      "+----------------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+--------------+---------------+--------------------+-------------+\n",
      "|rfm_segment|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+-----------+--------------+---------------+--------------------+-------------+\n",
      "|        355|             1|        9114.86|             9114.86|          1.0|\n",
      "+-----------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------+--------------+---------------+--------------------+-------------+\n",
      "|customer_segment_label|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+----------------------+--------------+---------------+--------------------+-------------+\n",
      "|       Loyal Customers|             1|        9114.86|             9114.86|          1.0|\n",
      "+----------------------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------------+--------------+---------------+--------------------+-------------+\n",
      "|preferred_referrer_source|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+-------------------------+--------------+---------------+--------------------+-------------+\n",
      "|                    Email|            43|        9114.86|  211.97348837209304|          1.0|\n",
      "+-------------------------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------------+--------------+---------------+--------------------+-------------+\n",
      "|preferred_device_type|customer_count|segment_revenue|revenue_per_customer|revenue_share|\n",
      "+---------------------+--------------+---------------+--------------------+-------------+\n",
      "|               Mobile|           148|        9114.86|  61.586891891891895|          1.0|\n",
      "+---------------------+--------------+---------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+-----------+------------+-----------------+------------------+\n",
      "|order_date|order_year|order_month|total_orders|    total_revenue|   avg_order_value|\n",
      "+----------+----------+-----------+------------+-----------------+------------------+\n",
      "|1900-01-01|      1900|          1|          86|89514.41971999999|1065.6478538095237|\n",
      "+----------+----------+-----------+------------+-----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+----------+----------+------------+-----------------+------------------+\n",
      "|year_week|order_year|order_week|total_orders|    total_revenue|   avg_order_value|\n",
      "+---------+----------+----------+------------+-----------------+------------------+\n",
      "| 1900-W01|      1900|         1|          86|89514.41971999999|1065.6478538095237|\n",
      "+---------+----------+----------+------------+-----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+-----------+------------+-----------------+------------------+\n",
      "|year_month|order_year|order_month|total_orders|    total_revenue|   avg_order_value|\n",
      "+----------+----------+-----------+------------+-----------------+------------------+\n",
      "|   1900-01|      1900|          1|          86|89514.41971999999|1065.6478538095237|\n",
      "+----------+----------+-----------+------------+-----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-------------------+--------------+---------+------------------+------------+---------------------+----------------------+---------------------+--------------+------------------+---------------------+--------------------+-------------------+---------------------+---------------------+---------------+------+-------------+---------+--------------+-----------+-------+---------------+------------------+-----------------+------------+--------------------+---------------------+------------------+------------------------+----------------+-----------------------+-------------+---------------+-------------------+-----------------------+----------------------+----------------+---------------+-----------------------+-----------------+--------------------+-----------------------+---------------------+---------------------+-------------------------+------------------------+------------------------+------------------------+-----------------+-----------------------+---------------------+---------------------+-------------------------+----------------------+-------------+---------------+--------------+-----------+----------------------+-----------------+------------+----------+--------------------+-------------+-------------------------+------------------+\n",
      "|customer_id| account_created_at|account_status|is_active|is_repeat_customer|total_orders|total_items_purchased|total_cancelled_orders|total_reviews_written|total_sessions|total_pages_viewed|total_products_viewed|wishlist_items_count|total_carts_created|total_abandoned_carts|total_purchased_carts|order_frequency|gender|date_of_birth|     city|state_province|postal_code|country|last_login_date|order_recency_days|order_total_spent|customer_age|customer_tenure_days|days_since_last_login|customer_age_group|customer_activity_status|customer_segment|customer_lifetime_value|total_revenue|avg_order_value|avg_items_per_order|total_discount_received|avg_discount_per_order|first_order_date|last_order_date|avg_days_between_orders|avg_review_rating|avg_session_duration|session_conversion_rate|cart_abandonment_rate|preferred_device_type|preferred_referrer_source|wishlist_conversion_rate|preferred_payment_method|days_since_last_purchase|cancellation_rate|customer_activity_score|total_abandoned_value|avg_time_in_cart_days|customer_abandonment_rate|customer_purchase_rate|recency_score|frequency_score|monetary_score|rfm_segment|customer_segment_label|rfm_overall_score|rfm_category|churn_risk|account_created_date|customer_type|discount_share_of_revenue|is_discount_hunter|\n",
      "+-----------+-------------------+--------------+---------+------------------+------------+---------------------+----------------------+---------------------+--------------+------------------+---------------------+--------------------+-------------------+---------------------+---------------------+---------------+------+-------------+---------+--------------+-----------+-------+---------------+------------------+-----------------+------------+--------------------+---------------------+------------------+------------------------+----------------+-----------------------+-------------+---------------+-------------------+-----------------------+----------------------+----------------+---------------+-----------------------+-----------------+--------------------+-----------------------+---------------------+---------------------+-------------------------+------------------------+------------------------+------------------------+-----------------+-----------------------+---------------------+---------------------+-------------------------+----------------------+-------------+---------------+--------------+-----------+----------------------+-----------------+------------+----------+--------------------+-------------+-------------------------+------------------+\n",
      "|          1|2022-04-01 21:23:41|        Active|     true|                 0|           0|                    0|                  NULL|                    0|             1|                 2|                 NULL|                   0|                  0|                    0|                 NULL|           NULL|  Male|   1946-08-07|New Mandy|      Oklahoma|   SA8P 7YL|Senegal|     2024-05-14|              NULL|             NULL|          79|                1379|                  605|       65 and over|                 Dormant|            NULL|                    0.0|          0.0|           NULL|               NULL|                    0.0|                   0.0|            NULL|           NULL|                   NULL|             NULL|                 1.0|                    0.0|                  0.0|               Mobile|                 Referral|                    NULL|                    NULL|                    NULL|             NULL|                   NULL|                  0.0|                 NULL|                     NULL|                  NULL|         NULL|           NULL|          NULL|       NULL|                  NULL|             NULL|        NULL|      NULL|          2022-04-01|          new|                      0.0|                 0|\n",
      "+-----------+-------------------+--------------+---------+------------------+------------+---------------------+----------------------+---------------------+--------------+------------------+---------------------+--------------------+-------------------+---------------------+---------------------+---------------+------+-------------+---------+--------------+-----------+-------+---------------+------------------+-----------------+------------+--------------------+---------------------+------------------+------------------------+----------------+-----------------------+-------------+---------------+-------------------+-----------------------+----------------------+----------------+---------------+-----------------------+-----------------+--------------------+-----------------------+---------------------+---------------------+-------------------------+------------------------+------------------------+------------------------+-----------------+-----------------------+---------------------+---------------------+-------------------------+----------------------+-------------+---------------+--------------+-----------+----------------------+-----------------+------------+----------+--------------------+-------------+-------------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------+--------------+--------------------+----------------------+-------+-----------+\n",
      "|is_discount_hunter|customer_count|  avg_discount_share|avg_discount_per_order|avg_clv|avg_revenue|\n",
      "+------------------+--------------+--------------------+----------------------+-------+-----------+\n",
      "|                 0|          1000|7.347562112857465E-5|   0.11162000000000001|9.11486|    9.11486|\n",
      "+------------------+--------------+--------------------+----------------------+-------+-----------+\n",
      "\n",
      "+-----------------------+---------------------+\n",
      "|corr_discount_share_clv|corr_avg_discount_clv|\n",
      "+-----------------------+---------------------+\n",
      "|                    1.0|                  1.0|\n",
      "+-----------------------+---------------------+\n",
      "\n",
      "+-----------+-------------+-----------------------+-------------------------+----------------------+-----------------------+\n",
      "|customer_id|total_revenue|total_discount_received|discount_to_revenue_ratio|avg_discount_per_order|customer_lifetime_value|\n",
      "+-----------+-------------+-----------------------+-------------------------+----------------------+-----------------------+\n",
      "+-----------+-------------+-----------------------+-------------------------+----------------------+-----------------------+\n",
      "\n",
      "+----------+-------------+-------------+-------+----------------+\n",
      "|churn_risk|num_customers|total_revenue|avg_clv|avg_recency_days|\n",
      "+----------+-------------+-------------+-------+----------------+\n",
      "|      NULL|          999|          0.0|    0.0|            NULL|\n",
      "+----------+-------------+-------------+-------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----------------------+----------+-----------------------+------------------+------------------------+---------------------+----------------------+-----------+------------+\n",
      "|customer_id|customer_lifetime_value|churn_risk|customer_activity_score|order_recency_days|days_since_last_purchase|days_since_last_login|customer_segment_label|rfm_segment|rfm_category|\n",
      "+-----------+-----------------------+----------+-----------------------+------------------+------------------------+---------------------+----------------------+-----------+------------+\n",
      "+-----------+-----------------------+----------+-----------------------+------------------+------------------------+---------------------+----------------------+-----------+------------+\n",
      "\n",
      "+-----------+-------------+-------------+-------+----------------+----------------+------------------+\n",
      "|rfm_segment|num_customers|total_revenue|avg_clv|avg_recency_days|avg_total_orders|           avg_aov|\n",
      "+-----------+-------------+-------------+-------+----------------+----------------+------------------+\n",
      "|        355|            1|      9114.86|9114.86|           280.0|             2.0|1519.1433333333334|\n",
      "+-----------+-------------+-------------+-------+----------------+----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+---------------------+--------------------+-------------------+---------------------+---------------------+-----------------------+\n",
      "|customer_id|total_products_viewed|wishlist_items_count|total_carts_created|total_purchased_carts|cart_abandonment_rate|session_conversion_rate|\n",
      "+-----------+---------------------+--------------------+-------------------+---------------------+---------------------+-----------------------+\n",
      "|        350|                   15|                   0|                  0|                 NULL|                  0.0|                    0.0|\n",
      "+-----------+---------------------+--------------------+-------------------+---------------------+---------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----------+----------------+-------------------+-----------------+\n",
      "|customer_id|signup_date|first_order_date|signup_cohort_month|first_order_month|\n",
      "+-----------+-----------+----------------+-------------------+-----------------+\n",
      "|       1000| 2022-10-17|      2024-10-22|         2022-10-01|       2024-10-01|\n",
      "+-----------+-----------+----------------+-------------------+-----------------+\n",
      "\n",
      "+-------------------+----------------+\n",
      "|signup_cohort_month|cohort_customers|\n",
      "+-------------------+----------------+\n",
      "|         2022-10-01|               1|\n",
      "+-------------------+----------------+\n",
      "\n",
      "+-----------+-------------------+--------------+---------+------------------+------------+---------------------+----------------------+---------------------+--------------+------------------+---------------------+--------------------+-------------------+---------------------+---------------------+---------------+------+------------------+---------+--------------+-------+------------------+--------------------+---------------------+------------------------+----------------+----------------------+-----------+------------+----------+-----------------------+-------------+---------------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+---------------------+-------------------------+------------------------+------------------------+------------------------+-----------------+-----------------------+---------------------+---------------------+-------------------------+----------------------+\n",
      "|customer_id| account_created_at|account_status|is_active|is_repeat_customer|total_orders|total_items_purchased|total_cancelled_orders|total_reviews_written|total_sessions|total_pages_viewed|total_products_viewed|wishlist_items_count|total_carts_created|total_abandoned_carts|total_purchased_carts|order_frequency|gender|customer_age_group|     city|state_province|country|order_recency_days|customer_tenure_days|days_since_last_login|customer_activity_status|customer_segment|customer_segment_label|rfm_segment|rfm_category|churn_risk|customer_lifetime_value|total_revenue|avg_order_value|avg_items_per_order|total_discount_received|avg_discount_per_order|avg_session_duration|session_conversion_rate|cart_abandonment_rate|preferred_device_type|preferred_referrer_source|preferred_payment_method|wishlist_conversion_rate|days_since_last_purchase|cancellation_rate|customer_activity_score|total_abandoned_value|avg_time_in_cart_days|customer_abandonment_rate|customer_purchase_rate|\n",
      "+-----------+-------------------+--------------+---------+------------------+------------+---------------------+----------------------+---------------------+--------------+------------------+---------------------+--------------------+-------------------+---------------------+---------------------+---------------+------+------------------+---------+--------------+-------+------------------+--------------------+---------------------+------------------------+----------------+----------------------+-----------+------------+----------+-----------------------+-------------+---------------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+---------------------+-------------------------+------------------------+------------------------+------------------------+-----------------+-----------------------+---------------------+---------------------+-------------------------+----------------------+\n",
      "|          1|2022-04-01 21:23:41|        Active|     true|                 0|           0|                    0|                  NULL|                    0|             1|                 2|                 NULL|                   0|                  0|                    0|                 NULL|           NULL|  Male|       65 and over|New Mandy|      Oklahoma|Senegal|              NULL|                1379|                  605|                 Dormant|            NULL|                  NULL|       NULL|        NULL|      NULL|                    0.0|          0.0|           NULL|               NULL|                   NULL|                  NULL|                 1.0|                    0.0|                  0.0|               Mobile|                 Referral|                    NULL|                    NULL|                    NULL|             NULL|                   NULL|                  0.0|                 NULL|                     NULL|                  NULL|\n",
      "+-----------+-------------------+--------------+---------+------------------+------------+---------------------+----------------------+---------------------+--------------+------------------+---------------------+--------------------+-------------------+---------------------+---------------------+---------------+------+------------------+---------+--------------+-------+------------------+--------------------+---------------------+------------------------+----------------+----------------------+-----------+------------+----------+-----------------------+-------------+---------------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+---------------------+-------------------------+------------------------+------------------------+------------------------+-----------------+-----------------------+---------------------+---------------------+-------------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+----------+--------------+---------------+-------+\n",
      "|rfm_segment|churn_risk|customer_count|segment_revenue|avg_clv|\n",
      "+-----------+----------+--------------+---------------+-------+\n",
      "|       NULL|      NULL|           999|            0.0|    0.0|\n",
      "+-----------+----------+--------------+---------------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------+-------------------------+--------------+---------------+------------------------+\n",
      "|customer_segment_label|preferred_referrer_source|customer_count|segment_revenue|avg_revenue_per_customer|\n",
      "+----------------------+-------------------------+--------------+---------------+------------------------+\n",
      "|                  NULL|                     NULL|           650|            0.0|                     0.0|\n",
      "+----------------------+-------------------------+--------------+---------------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------+---------------------+--------------+---------------+------------------------+\n",
      "|customer_segment_label|preferred_device_type|customer_count|segment_revenue|avg_revenue_per_customer|\n",
      "+----------------------+---------------------+--------------+---------------+------------------------+\n",
      "|                  NULL|               MOBILE|             3|            0.0|                     0.0|\n",
      "+----------------------+---------------------+--------------+---------------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------------+--------------+------------------+-------------+------------------------+--------------------+\n",
      "|preferred_referrer_source|customer_count|           avg_clv|total_revenue|avg_revenue_per_customer|  avg_discount_share|\n",
      "+-------------------------+--------------+------------------+-------------+------------------------+--------------------+\n",
      "|                    Email|            43|211.97348837209304|      9114.86|      211.97348837209304|0.001708735375083...|\n",
      "+-------------------------+--------------+------------------+-------------+------------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------------+----------+--------------+-------+------------------+\n",
      "|preferred_referrer_source|churn_risk|customer_count|avg_clv|avg_discount_share|\n",
      "+-------------------------+----------+--------------+-------+------------------+\n",
      "|                     NULL|      NULL|           650|    0.0|               0.0|\n",
      "+-------------------------+----------+--------------+-------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------------+--------------+-------------+-------------------+----------------+-------+-----------------------+\n",
      "|customer_segment_label|customer_count|total_revenue| total_order_profit|total_net_profit|avg_clv|avg_profit_per_customer|\n",
      "+----------------------+--------------+-------------+-------------------+----------------+-------+-----------------------+\n",
      "|       Loyal Customers|             1|      9114.86|-21346.649999999998|       -21386.85|9114.86|    -21346.649999999998|\n",
      "+----------------------+--------------+-------------+-------------------+----------------+-------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+------+------------------+-----------------------+----------------+\n",
      "|rfm_segment|orders|     total_revenue|avg_order_value_segment|unique_customers|\n",
      "+-----------+------+------------------+-----------------------+----------------+\n",
      "|        134|    13|26097.969999999994|     2007.5361538461534|              13|\n",
      "+-----------+------+------------------+-----------------------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------------------+\n",
      "|total_inventory_carrying_cost|\n",
      "+-----------------------------+\n",
      "|            3409.883999999998|\n",
      "+-----------------------------+\n",
      "\n",
      "+--------------------+------------------------+--------------------+------------------------+------------------------+\n",
      "|total_wishlist_items|customers_using_wishlist|products_in_wishlist|wishlist_purchased_items|wishlist_conversion_rate|\n",
      "+--------------------+------------------------+--------------------+------------------------+------------------------+\n",
      "|                1963|                     851|                 847|                     302|     0.15384615384615385|\n",
      "+--------------------+------------------------+--------------------+------------------------+------------------------+\n",
      "\n",
      "+----------+-------------+------------------+------------------------+\n",
      "|product_id|wishlist_adds|wishlist_purchases|wishlist_conversion_rate|\n",
      "+----------+-------------+------------------+------------------------+\n",
      "|      1159|            4|                 0|                     0.0|\n",
      "+----------+-------------+------------------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-------------+------------------+------------------------+\n",
      "|customer_id|wishlist_adds|wishlist_purchases|wishlist_conversion_rate|\n",
      "+-----------+-------------+------------------+------------------------+\n",
      "|       1159|            1|                 0|                     0.0|\n",
      "+-----------+-------------+------------------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+-------------------+-----------+--------+---------+\n",
      "|records|           avg_time|median_time|p90_time| max_time|\n",
      "+-------+-------------------+-----------+--------+---------+\n",
      "|    295|1.616324338983051E7|   10108800|39225600|265161600|\n",
      "+-------+-------------------+-----------+--------+---------+\n",
      "\n",
      "+-----------+-----+------------------+\n",
      "|time_bucket|count|             share|\n",
      "+-----------+-----+------------------+\n",
      "|       > 30|  272|0.9220338983050848|\n",
      "+-----------+-----+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----------+----------+----------+--------------+------------+\n",
      "|wishlist_id|customer_id|product_id|added_date|purchased_date|removed_date|\n",
      "+-----------+-----------+----------+----------+--------------+------------+\n",
      "|        471|       1998|      1166|2024-03-02|          NULL|        NULL|\n",
      "+-----------+-----------+----------+----------+--------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+------------------------+\n",
      "|customer_id|abandoned_wishlist_count|\n",
      "+-----------+------------------------+\n",
      "|      99999|                      22|\n",
      "+-----------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------------------+\n",
      "|product_id|abandoned_wishlist_count|\n",
      "+----------+------------------------+\n",
      "|      9999|                      27|\n",
      "+----------+------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------+\n",
      "|year_month|wishlist_adds|\n",
      "+----------+-------------+\n",
      "|   2016-04|            1|\n",
      "+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+----------------+\n",
      "|total_carts|total_cart_lines|\n",
      "+-----------+----------------+\n",
      "|       1776|            1776|\n",
      "+-----------+----------------+\n",
      "\n",
      "+-----------+-----------+----------------+\n",
      "|cart_status|carts_count|cart_lines_count|\n",
      "+-----------+-----------+----------------+\n",
      "|       NULL|       1644|            1644|\n",
      "+-----------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------+---------------+---------------+----------------+-------------+\n",
      "|total_carts_tracked|abandoned_carts|converted_carts|abandonment_rate|purchase_rate|\n",
      "+-------------------+---------------+---------------+----------------+-------------+\n",
      "|               1776|              0|              0|             0.0|          0.0|\n",
      "+-------------------+---------------+---------------+----------------+-------------+\n",
      "\n",
      "+-----------+-----------+------------------+--------------+---------------------+----------------------------+\n",
      "|cart_status|carts_count|    avg_cart_value|avg_cart_items|avg_time_in_cart_days|avg_recovery_potential_score|\n",
      "+-----------+-----------+------------------+--------------+---------------------+----------------------------+\n",
      "|       NULL|       1644|2330.7745878162996|           1.0|   1221.0243309002433|           42.42700729927007|\n",
      "+-----------+-----------+------------------+--------------+---------------------+----------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+-----------+----------------+----------------+-----------------+------------------------+----------------------+\n",
      "|cart_id|customer_id|cart_total_value|cart_items_count|time_in_cart_days|recovery_potential_score|abandonment_risk_score|\n",
      "+-------+-----------+----------------+----------------+-----------------+------------------------+----------------------+\n",
      "+-------+-----------+----------------+----------------+-----------------+------------------------+----------------------+\n",
      "\n",
      "+---------------+---------------------+----------------------+-----------------------------+\n",
      "|completed_carts|avg_time_in_cart_days|avg_time_in_cart_hours|time_in_cart_days_percentiles|\n",
      "+---------------+---------------------+----------------------+-----------------------------+\n",
      "|              0|                 NULL|                  NULL|                         NULL|\n",
      "+---------------+---------------------+----------------------+-----------------------------+\n",
      "\n",
      "+---------------+------------------+---------------+---------------------+----------------------+--------------------+\n",
      "|cart_value_tier|cart_size_category|completed_carts|avg_time_in_cart_days|avg_time_in_cart_hours|avg_cart_items_count|\n",
      "+---------------+------------------+---------------+---------------------+----------------------+--------------------+\n",
      "+---------------+------------------+---------------+---------------------+----------------------+--------------------+\n",
      "\n",
      "+-----------------------+---------------+---------------------+\n",
      "|time_to_purchase_bucket|completed_carts|avg_time_in_cart_days|\n",
      "+-----------------------+---------------+---------------------+\n",
      "+-----------------------+---------------+---------------------+\n",
      "\n",
      "+-----------+--------------------+-------------+---------------+----------------+-------+------------+---------------------+----------------+---------------------+-----------------+---------------+------------------+-------------------------+--------------+-------------+\n",
      "|campaign_id|       campaign_name|campaign_type|campaign_status|performance_tier| budget|spent_amount|effective_impressions|effective_clicks|effective_conversions|revenue_generated|avg_order_value|     ctr_effective|conversion_rate_effective|roas_effective|roi_effective|\n",
      "+-----------+--------------------+-------------+---------------+----------------+-------+------------+---------------------+----------------+---------------------+-----------------+---------------+------------------+-------------------------+--------------+-------------+\n",
      "|          2|New Year Deal 202...|         NULL|           NULL|            NULL|8113.36|     2465.64|               104130|            3792|                  154|              0.0|           NULL|3.6416018438490347|        4.061181434599156|           0.0|         -1.0|\n",
      "+-----------+--------------------+-------------+---------------+----------------+-------+------------+---------------------+----------------+---------------------+-----------------+---------------+------------------+-------------------------+--------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----+---------------+---------------+---------------+-----------------+\n",
      "|device_used|carts|converted_carts|abandoned_carts|conversion_rate| abandonment_rate|\n",
      "+-----------+-----+---------------+---------------+---------------+-----------------+\n",
      "|    unknown| 1776|              0|           1649|            0.0|0.928490990990991|\n",
      "+-----------+-----+---------------+---------------+---------------+-----------------+\n",
      "\n",
      "+-------+--------------+-------------+-----------+\n",
      "|country|payment_method|payment_count|order_count|\n",
      "+-------+--------------+-------------+-----------+\n",
      "|unknown|     Gift Card|          226|        209|\n",
      "+-------+--------------+-------------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+--------------+-------------+-----------+\n",
      "|country|state_province|payment_method|payment_count|order_count|\n",
      "+-------+--------------+--------------+-------------+-----------+\n",
      "|unknown|       unknown|     Gift Card|          226|        209|\n",
      "+-------+--------------+--------------+-------------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+--------------+------------------+---------------+------------+\n",
      "|payment_method|total_payments|completed_payments|distinct_orders|success_rate|\n",
      "+--------------+--------------+------------------+---------------+------------+\n",
      "|  CREDIT_CARD#|             2|                 2|              2|         1.0|\n",
      "+--------------+--------------+------------------+---------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+--------------+------------------+---------------+------------+\n",
      "|country|payment_method|total_payments|completed_payments|distinct_orders|success_rate|\n",
      "+-------+--------------+--------------+------------------+---------------+------------+\n",
      "|unknown|Bank Transfer%|             1|                 1|              1|         1.0|\n",
      "+-------+--------------+--------------+------------------+---------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+-------------+-----------+-------------+----------------------+\n",
      "|payment_method|payment_count|order_count|total_revenue|avg_order_value_method|\n",
      "+--------------+-------------+-----------+-------------+----------------------+\n",
      "|      PayPall#|            1|          1|      1793.84|               1793.84|\n",
      "+--------------+-------------+-----------+-------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+--------------+-------------------+--------------------+--------------------+----------------------+\n",
      "|payment_method|total_payments|total_refund_amount|payments_with_refund|refund_rate_payments|avg_refund_per_payment|\n",
      "+--------------+--------------+-------------------+--------------------+--------------------+----------------------+\n",
      "|   Debit Card%|             1|             568.63|                   1|                 1.0|                568.63|\n",
      "+--------------+--------------+-------------------+--------------------+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+--------------+-----------+------------------+------------------+-------------------+------------------+\n",
      "|product_id|  product_name|   category|orders_for_product|orders_with_refund|total_refund_amount|refund_rate_orders|\n",
      "+----------+--------------+-----------+------------------+------------------+-------------------+------------------+\n",
      "|      1939|Samsung Tablet|Electronics|                 1|                 2|              758.0|               2.0|\n",
      "+----------+--------------+-----------+------------------+------------------+-------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+------------------+------+------------------+-------------------+------------------+-------------------+-------------------+\n",
      "|order_placed_year|order_placed_month|orders|orders_with_refund|total_refund_amount|total_order_amount| refund_rate_orders| refund_rate_amount|\n",
      "+-----------------+------------------+------+------------------+-------------------+------------------+-------------------+-------------------+\n",
      "|             1900|                 1|    86|                10|            8413.75|       89514.41972|0.11627906976744186|0.09399323624415044|\n",
      "+-----------------+------------------+------+------------------+-------------------+------------------+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+-----------------+-------------------+------------------+--------------------------+------------------+------------------+\n",
      "|payment_method|refunded_payments|total_refund_amount|avg_days_to_refund|days_to_refund_percentiles|min_days_to_refund|max_days_to_refund|\n",
      "+--------------+-----------------+-------------------+------------------+--------------------------+------------------+------------------+\n",
      "|    debit card|                1|           1729.605|              -9.0|        [-9.0, -9.0, -9.0]|                -9|                -9|\n",
      "+--------------+-----------------+-------------------+------------------+--------------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+-------------+----------------+\n",
      "|product_id|review_date|daily_reviews|avg_rating_daily|\n",
      "+----------+-----------+-------------+----------------+\n",
      "|      1000| 1900-01-01|            1|             5.0|\n",
      "+----------+-----------+-------------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+-----------+-----------+--------------+-----------------+\n",
      "|product_id| year_week|review_year|review_week|weekly_reviews|avg_rating_weekly|\n",
      "+----------+----------+-----------+-----------+--------------+-----------------+\n",
      "|      1000|1900-01-01|       1900|          1|             1|              5.0|\n",
      "+----------+----------+-----------+-----------+--------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+----------+-----------+------------+---------------+------------------+\n",
      "|product_id|year_month|review_year|review_month|monthly_reviews|avg_rating_monthly|\n",
      "+----------+----------+-----------+------------+---------------+------------------+\n",
      "|      1000|   1900-01|       1900|           1|              1|               5.0|\n",
      "+----------+----------+-----------+------------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+-------------+----------+----------------+----------------+---------------+-------------------+--------------+--------------+\n",
      "| category|total_reviews|avg_rating|positive_reviews|negative_reviews|neutral_reviews|avg_sentiment_score|positive_share|negative_share|\n",
      "+---------+-------------+----------+----------------+----------------+---------------+-------------------+--------------+--------------+\n",
      "|Shoes 584|            3|       5.0|               3|               0|              0|                1.0|           1.0|           0.0|\n",
      "+---------+-------------+----------+----------------+----------------+---------------+-------------------+--------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------+----------------+-------------+----------+-----------+------------+---------------+----------------+\n",
      "|product_id|product_name|category|total_units_sold|total_revenue|year_month|review_year|review_month|monthly_reviews|avg_rating_month|\n",
      "+----------+------------+--------+----------------+-------------+----------+-----------+------------+---------------+----------------+\n",
      "|      1000|        NULL|    NULL|            NULL|         NULL|   1900-01|       1900|           1|              1|             5.0|\n",
      "+----------+------------+--------+----------------+-------------+----------+-----------+------------+---------------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+----------------+----------------+-----------------+----------+-----------+------------+---------------+----------------+\n",
      "|product_id|       product_name|        category|total_units_sold|    total_revenue|year_month|review_year|review_month|monthly_reviews|avg_rating_month|\n",
      "+----------+-------------------+----------------+----------------+-----------------+----------+-----------+------------+---------------+----------------+\n",
      "|      1001|Dell Backpack Ultra|Women'S Clothing|              79|48851.23000000001|   2025-12|       2025|          12|             20|             3.4|\n",
      "+----------+-------------------+----------------+----------------+-----------------+----------+-----------+------------+---------------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+---------+-------------+------------------+----------------+------------------+-----------+\n",
      "|product_id|       product_name| category|total_reviews|avg_rating_product|total_units_sold|     total_revenue|rating_tier|\n",
      "+----------+-------------------+---------+-------------+------------------+----------------+------------------+-----------+\n",
      "|      1008|Puma Training Shoes|Furniture|           50|              3.88|            1560|3124695.6000000006|  3.5 - 4.0|\n",
      "+----------+-------------------+---------+-------------+------------------+----------------+------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+----------------+---------------------+------------------+--------------------------+-----------------------+------------------+-----------------------+\n",
      "|rating_tier|products_in_tier|total_units_sold_tier|total_revenue_tier|avg_units_sold_per_product|avg_revenue_per_product|avg_rating_in_tier|avg_reviews_per_product|\n",
      "+-----------+----------------+---------------------+------------------+--------------------------+-----------------------+------------------+-----------------------+\n",
      "|  4.5 - 5.0|               3|                  227|          18236.49|         75.66666666666667|      6078.830000000001|4.5888888888888895|      5.666666666666667|\n",
      "+-----------+----------------+---------------------+------------------+--------------------------+-----------------------+------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+------+-----------+-------------------+-----------------+--------------------------+----------------------+-------------------+\n",
      "|         category|orders|total_units|avg_processing_days|avg_delivery_days|avg_total_fulfillment_days|median_processing_days|max_processing_days|\n",
      "+-----------------+------+-----------+-------------------+-----------------+--------------------------+----------------------+-------------------+\n",
      "|Books & Media 282|     1|         47|            46029.0|              0.0|                   46029.0|                 46029|              46029|\n",
      "+-----------------+------+-----------+-------------------+-----------------+--------------------------+----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------+------------+------+-------------------+-----------------+----------------------+\n",
      "|         category|sub_category|orders|avg_processing_days|avg_delivery_days|median_processing_days|\n",
      "+-----------------+------------+------+-------------------+-----------------+----------------------+\n",
      "|Books & Media 282|      Budget|     1|            46029.0|              0.0|                 46029|\n",
      "+-----------------+------------+------+-------------------+-----------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------+-------------------+-----------------+-------------+\n",
      "|order_hour|orders|avg_processing_days|avg_delivery_days|total_revenue|\n",
      "+----------+------+-------------------+-----------------+-------------+\n",
      "|         0|   130|  32717.35714285714|9.636363636363637| 132189.22327|\n",
      "+----------+------+-------------------+-----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+--------------+------+-------------------+------------------+-------------+\n",
      "|order_dow|order_dow_name|orders|avg_processing_days| avg_delivery_days|total_revenue|\n",
      "+---------+--------------+------+-------------------+------------------+-------------+\n",
      "|        1|           Sun|   150|  7.680851063829787|4.7846153846153845| 140907.66022|\n",
      "+---------+--------------+------+-------------------+------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+------+-------------------+-----------------+-----------------+\n",
      "|day_type|orders|avg_processing_days|avg_delivery_days|    total_revenue|\n",
      "+--------+------+-------------------+-----------------+-----------------+\n",
      "| Weekday|   858|   5163.81425891182|8.676470588235293|869070.4587839997|\n",
      "+--------+------+-------------------+-----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+----------------+-----------------+--------------------+-----------------+\n",
      "|country|delivered_orders|avg_delivery_days|median_delivery_days|max_delivery_days|\n",
      "+-------+----------------+-----------------+--------------------+-----------------+\n",
      "|Ecuador|               1|              9.0|                   9|                9|\n",
      "+-------+----------------+-----------------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+----------------+-----------------+--------------------+-----------------+\n",
      "|country|state_province|delivered_orders|avg_delivery_days|median_delivery_days|max_delivery_days|\n",
      "+-------+--------------+----------------+-----------------+--------------------+-----------------+\n",
      "|Ecuador|    Queensland|               1|              9.0|                   9|                9|\n",
      "+-------+--------------+----------------+-----------------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+-----------+----------------+-----------------+--------------------+-----------------+\n",
      "|country|state_province|       city|delivered_orders|avg_delivery_days|median_delivery_days|max_delivery_days|\n",
      "+-------+--------------+-----------+----------------+-----------------+--------------------+-----------------+\n",
      "|Ecuador|    Queensland|Wintersport|               1|              9.0|                   9|                9|\n",
      "+-------+--------------+-----------+----------------+-----------------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+----------------+--------------+------------+-----------------+\n",
      "|country|delivered_orders|on_time_orders|on_time_rate|avg_delivery_days|\n",
      "+-------+----------------+--------------+------------+-----------------+\n",
      "|Ecuador|               1|             0|         0.0|              9.0|\n",
      "+-------+----------------+--------------+------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+----------------+--------------+------------+-----------------+\n",
      "|country|state_province|delivered_orders|on_time_orders|on_time_rate|avg_delivery_days|\n",
      "+-------+--------------+----------------+--------------+------------+-----------------+\n",
      "|Ecuador|    Queensland|               1|             0|         0.0|              9.0|\n",
      "+-------+--------------+----------------+--------------+------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+-----------+----------------+--------------+------------+-----------------+\n",
      "|country|state_province|       city|delivered_orders|on_time_orders|on_time_rate|avg_delivery_days|\n",
      "+-------+--------------+-----------+----------------+--------------+------------+-----------------+\n",
      "|Ecuador|    Queensland|Wintersport|               1|             0|         0.0|              9.0|\n",
      "+-------+--------------+-----------+----------------+--------------+------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "|country|orders|total_shipping_cost|    total_subtotal|avg_shipping_pct_of_subtotal|median_shipping_pct_of_subtotal|shipping_pct_of_subtotal_overall|\n",
      "+-------+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "|   NULL|  1170| 13095.779000000017|1115254.8869999994|          198.48456033884628|           0.010259892812568826|            0.011742408979912432|\n",
      "+-------+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "|country|state_province|orders|total_shipping_cost|    total_subtotal|avg_shipping_pct_of_subtotal|median_shipping_pct_of_subtotal|shipping_pct_of_subtotal_overall|\n",
      "+-------+--------------+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "|   NULL|          NULL|  1170| 13095.779000000017|1115254.8869999994|          198.48456033884628|           0.010259892812568826|            0.011742408979912432|\n",
      "+-------+--------------+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------+--------------+----+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "|country|state_province|city|orders|total_shipping_cost|    total_subtotal|avg_shipping_pct_of_subtotal|median_shipping_pct_of_subtotal|shipping_pct_of_subtotal_overall|\n",
      "+-------+--------------+----+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "|   NULL|          NULL|NULL|  1170| 13095.779000000017|1115254.8869999994|          198.48456033884628|           0.010259892812568826|            0.011742408979912432|\n",
      "+-------+--------------+----+------+-------------------+------------------+----------------------------+-------------------------------+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+------+-------------------+----------------------+-------------------+\n",
      "|season|orders|avg_processing_days|median_processing_days|max_processing_days|\n",
      "+------+------+-------------------+----------------------+-------------------+\n",
      "|Winter|   355| 11311.728395061727|                     4|              46029|\n",
      "+------+------+-------------------+----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+------------+------+-------------------+----------------------+\n",
      "|season|order_status|orders|avg_processing_days|median_processing_days|\n",
      "+------+------------+------+-------------------+----------------------+\n",
      "|  Fall|     Shipped|    30| 29.066666666666666|                     3|\n",
      "+------+------------+------+-------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+-----------+-------+--------------+----+---------------+------------+--------+-------------+------------------------+-------------------+\n",
      "|order_id|customer_id|country|state_province|city|order_placed_at|order_status|subtotal|shipping_cost|shipping_pct_of_subtotal|shipping_pct_zscore|\n",
      "+--------+-----------+-------+--------------+----+---------------+------------+--------+-------------+------------------------+-------------------+\n",
      "+--------+-----------+-------+--------------+----+---------------+------------+--------+-------------+------------------------+-------------------+\n",
      "\n",
      "+-------------------+-------------------+----------------+----------------+--------------+\n",
      "|signup_cohort_month|months_since_signup|active_customers|cohort_customers|retention_rate|\n",
      "+-------------------+-------------------+----------------+----------------+--------------+\n",
      "|         2022-10-01|                 24|               1|               1|           1.0|\n",
      "+-------------------+-------------------+----------------+----------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'best_selling_products'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     analysis[keys]\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys \u001b[38;5;129;01min\u001b[39;00m product_analysis\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m----> 5\u001b[0m     \u001b[43manalysis\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys \u001b[38;5;129;01min\u001b[39;00m supplier_analysis\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      8\u001b[0m     analysis[keys]\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'best_selling_products'"
     ]
    }
   ],
   "source": [
    "for keys in analysis.keys():\n",
    "    analysis[keys].show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1d008956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+------------+-----+----------------+------------+-------------+\n",
      "|product_id|product_name|category|sub_category|brand|total_units_sold|total_orders|total_revenue|\n",
      "+----------+------------+--------+------------+-----+----------------+------------+-------------+\n",
      "|      1006|        NULL| Kitchen|    Standard| NULL|            3698|          48|     66231.18|\n",
      "+----------+------------+--------+------------+-----+----------------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------+------------+-----+----------+-----------+----------+------------+\n",
      "|product_id|product_name|category|sub_category|brand|grain_year|grain_month|units_sold|orders_count|\n",
      "+----------+------------+--------+------------+-----+----------+-----------+----------+------------+\n",
      "|      1000|        NULL|    NULL|        NULL| NULL|      2019|          3|        58|           1|\n",
      "+----------+------------+--------+------------+-----+----------+-----------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+----------+-----------+----------+------------+\n",
      "|category|grain_year|grain_month|units_sold|orders_count|\n",
      "+--------+----------+-----------+----------+------------+\n",
      "|    NULL|      1900|          1|      1490|          39|\n",
      "+--------+----------+-----------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------+------------+-----+--------------+----------+------------+\n",
      "|product_id|product_name|category|sub_category|brand|calendar_month|units_sold|orders_count|\n",
      "+----------+------------+--------+------------+-----+--------------+----------+------------+\n",
      "|      1000|        NULL|    NULL|        NULL| NULL|             1|        80|           5|\n",
      "+----------+------------+--------+------------+-----+--------------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------+----------+------------+\n",
      "|category|calendar_month|units_sold|orders_count|\n",
      "+--------+--------------+----------+------------+\n",
      "|    NULL|             1|      2556|          70|\n",
      "+--------+--------------+----------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------------+------------+---------+-------------------+----------------+------------+------------------+\n",
      "|product_id|product_name|      category|sub_category|    brand|current_stock_level|total_units_sold|total_orders|     total_revenue|\n",
      "+----------+------------+--------------+------------+---------+-------------------+----------------+------------+------------------+\n",
      "|      1102|         Pro|Mobile Devices|      Budget|Nike Inc.|                  0|            1216|           4|1461048.3199999998|\n",
      "+----------+------------+--------------+------------+---------+-------------------+----------------+------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------+----------+\n",
      "|product_id|total_reviews|avg_rating|\n",
      "+----------+-------------+----------+\n",
      "|      1484|            2|       5.5|\n",
      "+----------+-------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------------+----------------+------------+-------------------------+--------------------+-------------+\n",
      "|category|products_in_category|total_units_sold|total_orders|avg_view_to_purchase_rate|avg_revenue_per_view|total_revenue|\n",
      "+--------+--------------------+----------------+------------+-------------------------+--------------------+-------------+\n",
      "|    NULL|                  63|            8070|         252|       1.7170212765957444|    899.588263829787|   4228064.84|\n",
      "+--------+--------------------+----------------+------------+-------------------------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------+---------------------+----------------+----------------+------------+\n",
      "|product_id|product_name|category|view_to_purchase_rate|revenue_per_view|total_units_sold|total_orders|\n",
      "+----------+------------+--------+---------------------+----------------+----------------+------------+\n",
      "|      1006|        NULL| Kitchen|                36.98|        662.3118|            3698|          48|\n",
      "+----------+------------+--------+---------------------+----------------+----------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+---------+-------------+------+----------------+------------+------------------+----------+---------------------+-------------+-------------------+------------------+----------------------+----------------------------------+\n",
      "|product_id|       product_name| category| sub_category| brand|total_units_sold|total_orders|     total_revenue|avg_rating|view_to_purchase_rate|revenue_score|       volume_score|      rating_score|view_to_purchase_score|product_performance_score_computed|\n",
      "+----------+-------------------+---------+-------------+------+----------------+------------+------------------+----------+---------------------+-------------+-------------------+------------------+----------------------+----------------------------------+\n",
      "|      1008|Puma Training Shoes|Furniture|Special Offer|Lenovo|            1560|          51|3124695.6000000006|      3.88|                 15.6|          1.0|0.42184964845862627|0.7054545454545454|    0.4218496484586263|                0.7098307684743596|\n",
      "+----------+-------------------+---------+-------------+------+----------------+------------+------------------+----------+---------------------+-------------+-------------------+------------------+----------------------+----------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+----------------+--------------------+----------------+-------------------+-------------------+\n",
      "|category|category_revenue|products_in_category|total_units_sold|revenue_per_product|      revenue_share|\n",
      "+--------+----------------+--------------------+----------------+-------------------+-------------------+\n",
      "|    NULL|      4228064.84|                  63|            8070|  67112.14031746032|0.11541251199027666|\n",
      "+--------+----------------+--------------------+----------------+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------------+----------------+------------+-------------+-------------------------+--------------------+-----------------+-------------+\n",
      "|category|products_in_category|total_units_sold|total_orders|total_revenue|avg_view_to_purchase_rate|avg_revenue_per_view|avg_profit_margin|revenue_share|\n",
      "+--------+--------------------+----------------+------------+-------------+-------------------------+--------------------+-----------------+-------------+\n",
      "|  Sports|                   1|               0|           0|          0.0|                      0.0|                 0.0|              0.0|          0.0|\n",
      "+--------+--------------------+----------------+------------+-------------+-------------------------+--------------------+-----------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------------+----------------+------------+-------------+-------------------+---------------+-----------+------------+------------------+--------------+----------+-------------------------+\n",
      "|category|products_in_category|total_units_sold|total_orders|total_revenue|total_wishlist_adds|total_cart_adds|units_score|orders_score|     revenue_score|wishlist_score|cart_score|category_popularity_score|\n",
      "+--------+--------------------+----------------+------------+-------------+-------------------+---------------+-----------+------------+------------------+--------------+----------+-------------------------+\n",
      "|    NULL|                  63|            8070|         252|   4228064.84|                121|            123|        1.0|         1.0|0.9999999999999996|           1.0|       1.0|       0.9999999999999998|\n",
      "+--------+--------------------+----------------+------------+-------------+-------------------+---------------+-----------+------------+------------------+--------------+----------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------+--------------+----------+------------+-------------------+\n",
      "|category|calendar_month|units_sold|orders_count|total_revenue_month|\n",
      "+--------+--------------+----------+------------+-------------------+\n",
      "|    NULL|             1|      2556|          70| 508523.98999999993|\n",
      "+--------+--------------+----------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------+-----------------+----------+------------+-------------------+\n",
      "|       category|peak_season_month|units_sold|orders_count|peak_season_revenue|\n",
      "+---------------+-----------------+----------+------------+-------------------+\n",
      "|Accessories 227|               11|        84|           2|            6776.28|\n",
      "+---------------+-----------------+----------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+----------------+------------+-------+-----------+-----------------+----------------+------------+-----------------+---------------------+------------------+-------------------+-------------+------------------+-----------------------+-------------------+\n",
      "|product_id|       product_name|        category|sub_category|  brand|launch_date|days_since_launch|total_units_sold|total_orders|    total_revenue|view_to_purchase_rate|        avg_rating|current_stock_level|current_stock|    days_of_supply|inventory_turnover_rate|    lifecycle_stage|\n",
      "+----------+-------------------+----------------+------------+-------+-----------+-----------------+----------------+------------+-----------------+---------------------+------------------+-------------------+-------------+------------------+-----------------------+-------------------+\n",
      "|      1001|Dell Backpack Ultra|Women'S Clothing|   Wholesale|Samsung| 2022-03-31|             1380|              79|          52|48851.23000000001|                 0.79|3.1904761904761907|                 41|           41|11.936708860759493|     1.9268292682926829|Mature (>=365 days)|\n",
      "+----------+-------------------+----------------+------------+-------+-----------+-----------------+----------------+------------+-----------------+---------------------+------------------+-------------------+-------------+------------------+-----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+--------------+-------------------------+-----------------+-----------------+-----------------+---------------------------+\n",
      "|     lifecycle_stage|products_count|avg_view_to_purchase_rate|       avg_rating|   avg_units_sold|      avg_revenue|avg_inventory_turnover_rate|\n",
      "+--------------------+--------------+-------------------------+-----------------+-----------------+-----------------+---------------------------+\n",
      "|Growth (90-365 days)|           141|       0.9740425531914891|3.176704213600334|97.40425531914893|61658.97375886529|          2.517393542176103|\n",
      "+--------------------+--------------+-------------------------+-----------------+-----------------+-----------------+---------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------+------------+-----+-------------------+-----------------+----------------+------------+-------------+--------------+------------+------------------+\n",
      "|product_id|product_name|category|sub_category|brand|current_stock_level|   days_of_supply|total_units_sold|total_orders|total_revenue|is_high_seller|is_low_stock|stockout_risk_flag|\n",
      "+----------+------------+--------+------------+-----+-------------------+-----------------+----------------+------------+-------------+--------------+------------+------------------+\n",
      "|      1006|        NULL| Kitchen|    Standard| NULL|                 48|0.337479718766901|            3698|          48|     66231.18|          true|        true|                 1|\n",
      "+----------+------------+--------+------------+-----+-------------------+-----------------+----------------+------------+-------------+--------------+------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+---------+-------------+------+--------------------+-------------+----------------+------------+------------------+----------------------------+\n",
      "|product_id|       product_name| category| sub_category| brand|stockout_occurrences|stockout_days|total_units_sold|total_orders|     total_revenue|replenishment_priority_score|\n",
      "+----------+-------------------+---------+-------------+------+--------------------+-------------+----------------+------------+------------------+----------------------------+\n",
      "|      1008|Puma Training Shoes|Furniture|Special Offer|Lenovo|                   0|            0|            1560|          51|3124695.6000000006|          3124.6956000000005|\n",
      "+----------+-------------------+---------+-------------+------+--------------------+-------------+----------------+------------+------------------+----------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+--------------+-----------+------------+-----+-------------------+----------------+------------+-------------+-----------------------+--------------+----------------+\n",
      "|product_id|  product_name|   category|sub_category|brand|current_stock_level|total_units_sold|total_orders|total_revenue|inventory_turnover_rate|days_of_supply|dead_stock_score|\n",
      "+----------+--------------+-----------+------------+-----+-------------------+----------------+------------+-------------+-----------------------+--------------+----------------+\n",
      "|      1982|Adidas T-Shirt|Fiction 361|    Consumer| NULL|                995|               0|           1|          0.0|                    0.0|           0.0|           995.0|\n",
      "+----------+--------------+-----------+------------+-----+-------------------+----------------+------------+-------------+-----------------------+--------------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------+---------------+--------------------------+------------------+------------------+-----------------+------------------------+---------------+------------------+--------------------+-----------+------------+------+----------------+------------+-------------+\n",
      "|product_id|current_stock|available_stock|reorder_point_breach_count|stockout_frequency|avg_stock_quantity|   days_of_supply|inventory_turnover_ratio|reorder_urgency|stock_health_score|        product_name|   category|sub_category| brand|total_units_sold|total_orders|total_revenue|\n",
      "+----------+-------------+---------------+--------------------------+------------------+------------------+-----------------+------------------------+---------------+------------------+--------------------+-----------+------------+------+----------------+------------+-------------+\n",
      "|      1591|          487|            487|                         0|                 0|             487.0|39.91803278688525|       7.690246406570843|            Low|                75|Apple Headphones Max|Electronics|    Consumer|Reebok|              61|           9|     26764.97|\n",
      "+----------+-------------+---------------+--------------------------+------------------+------------------+-----------------+------------------------+---------------+------------------+--------------------+-----------+------------+------+----------------+------------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------+---------------+--------------------------+------------------+------------------+------------------+------------------------+---------------+------------------+-------------------+---------+-------------+------+----------------+------------+------------------+---------------------+-----------------+\n",
      "|product_id|current_stock|available_stock|reorder_point_breach_count|stockout_frequency|avg_stock_quantity|    days_of_supply|inventory_turnover_ratio|reorder_urgency|stock_health_score|       product_name| category| sub_category| brand|total_units_sold|total_orders|     total_revenue|is_low_days_of_supply|criticality_score|\n",
      "+----------+-------------+---------------+--------------------------+------------------+------------------+------------------+------------------------+---------------+------------------+-------------------+---------+-------------+------+----------------+------------+------------------+---------------------+-----------------+\n",
      "|      1008|          123|             87|                         1|                 0|             123.0|1.8923076923076922|       2860.073739837398|           High|                50|Puma Training Shoes|Furniture|Special Offer|Lenovo|            1560|          51|3124695.6000000006|                 true|             NULL|\n",
      "+----------+-------------+---------------+--------------------------+------------------+------------------+------------------+------------------------+---------------+------------------+-------------------+---------+-------------+------+----------------+------------+------------------+---------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+----------------+------------+-------+-----------+----------------+------------+-----------------+-----------------+-------------+---------------+------------+-----------+---------------------------+--------------------+--------------------------+-----------------+-------------------+---------------------------+---------------------+----------------------+--------------------------+--------------------------+----------------------+-----------------+--------------------------+------------------+-----------------------+-------------------------+--------------------------+-------------------+----------------------+-------------------------+-----------------+--------------------------+\n",
      "|product_id|       product_name|        category|sub_category|  brand|supplier_id|total_units_sold|total_orders|    total_revenue|     total_profit|profit_margin|supplier_status|is_preferred|is_verified|sup_total_products_supplied|sup_total_units_sold|sup_total_orders_fulfilled|sup_total_reviews|sup_total_stockouts|sup_total_revenue_generated|sup_avg_profit_margin|sup_avg_product_rating|supplier_performance_score|supplier_reliability_score|stock_efficiency_ratio|sup_stockout_rate|sup_inventory_health_score|inv_total_products|inv_total_current_stock|inv_total_available_stock|inv_total_reorder_breaches|inv_total_stockouts|inv_total_storage_cost|inv_avg_stock_per_product|inv_stockout_rate|inv_inventory_health_score|\n",
      "+----------+-------------------+----------------+------------+-------+-----------+----------------+------------+-----------------+-----------------+-------------+---------------+------------+-----------+---------------------------+--------------------+--------------------------+-----------------+-------------------+---------------------------+---------------------+----------------------+--------------------------+--------------------------+----------------------+-----------------+--------------------------+------------------+-----------------------+-------------------------+--------------------------+-------------------+----------------------+-------------------------+-----------------+--------------------------+\n",
      "|      1001|Dell Backpack Ultra|Women'S Clothing|   Wholesale|Samsung|      10094|              79|          52|48851.23000000001|22390.17999999999|          0.0|           NULL|        NULL|       NULL|                       NULL|                NULL|                      NULL|             NULL|               NULL|                       NULL|                 NULL|                  NULL|                      NULL|                      NULL|                  NULL|             NULL|                      NULL|                 2|                    284|                      280|                         1|                  0|    5.6899999999999995|                    142.0|              0.0|                      75.0|\n",
      "+----------+-------------------+----------------+------------+-------+-----------+----------------+------------+-----------------+-----------------+-------------+---------------+------------+-----------+---------------------------+--------------------+--------------------------+-----------------+-------------------+---------------------------+---------------------+----------------------+--------------------------+--------------------------+----------------------+-----------------+--------------------------+------------------+-----------------------+-------------------------+--------------------------+-------------------+----------------------+-------------------------+-----------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+------------------+--------------------------+-------------+---------------+--------------+------------+------------+--------+\n",
      "|product_id|supplier_id|stockout_frequency|reorder_point_breach_count|current_stock|available_stock|days_of_supply|stock_status|product_name|category|\n",
      "+----------+-----------+------------------+--------------------------+-------------+---------------+--------------+------------+------------+--------+\n",
      "|      2498|        999|                 2|                         1|            0|              0|          NULL|Out of Stock|        NULL|    NULL|\n",
      "+----------+-----------+------------------+--------------------------+-------------+---------------+--------------+------------+------------+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+---------------+------------+-----------+-----------------------+----------------+----------------------+---------------+-----------------------+------------------+-------------+-------------------------------+--------------------------+--------------------------+----------------------+\n",
      "|supplier_id|supplier_status|is_preferred|is_verified|total_products_supplied|total_units_sold|total_orders_fulfilled|total_stockouts|total_revenue_generated| avg_profit_margin|stockout_rate|supplier_inventory_health_score|supplier_performance_score|supplier_reliability_score|stock_efficiency_ratio|\n",
      "+-----------+---------------+------------+-----------+-----------------------+----------------+----------------------+---------------+-----------------------+------------------+-------------+-------------------------------+--------------------------+--------------------------+----------------------+\n",
      "|      10069|         Active|       false|      false|                      2|            1591|                    52|              0|             3151153.17|1256.3362068965528|          0.0|                           75.0|        1000.9271274705883|                      4.08|     478.1741225199187|\n",
      "+-----------+---------------+------------+-----------+-----------------------+----------------+----------------------+---------------+-----------------------+------------------+-------------+-------------------------------+--------------------------+--------------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-------------------+--------+------------+---------+-----------+---------------+------------+-----------+------------------+----------------+------------+-------------+-------------------+-----------------+--------------------------+--------------------------+--------------------------+\n",
      "|product_id|       product_name|category|sub_category|    brand|supplier_id|supplier_status|is_preferred|is_verified|     total_revenue|total_units_sold|total_orders|profit_margin|sup_total_stockouts|sup_stockout_rate|sup_inventory_health_score|supplier_performance_score|supplier_reliability_score|\n",
      "+----------+-------------------+--------+------------+---------+-----------+---------------+------------+-----------+------------------+----------------+------------+-------------+-------------------+-----------------+--------------------------+--------------------------+--------------------------+\n",
      "|      1047|Google Monitor Plus| Fiction|   Wholesale|Microsoft|      10694|        Unknown|       false|       true|108828.64000000001|             184|           2|          0.0|                  2|             50.0|                      62.5|         92.27271999999999|        2.7940000000000005|\n",
      "+----------+-------------------+--------+------------+---------+-----------+---------------+------------+-----------+------------------+----------------+------------+-------------+-------------------+-----------------+--------------------------+--------------------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------------+------------------+----------+--------------------+---------------------------+-------------------+\n",
      "|product_a_category|product_b_category|pair_count|total_co_occurrences|avg_lift_between_categories|        avg_support|\n",
      "+------------------+------------------+----------+--------------------+---------------------------+-------------------+\n",
      "|        Gaming 590|            Garden|         1|                   3|                    18.3125|0.25597269624573377|\n",
      "+------------------+------------------+----------+--------------------+---------------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+-----------------+----------+--------------------+--------+-------------------+\n",
      "|base_category|affinity_category|pair_count|total_co_occurrences|avg_lift|        avg_support|\n",
      "+-------------+-----------------+----------+--------------------+--------+-------------------+\n",
      "|   Gaming 590|           Garden|         1|                   3| 18.3125|0.25597269624573377|\n",
      "+-------------+-----------------+----------+--------------------+--------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+----------------+------------------+------------+--------------+------------------+-------------------+-------------------+-----------------+-----------------+-----------+-----------+--------+-----------------+-----------------+\n",
      "|product_a_id|  product_a_name|product_a_category|product_b_id|product_b_name|product_b_category|co_occurrence_count|            support|confidence_a_to_b|confidence_b_to_a|lift_a_to_b|lift_b_to_a|avg_lift|affinity_strength|   affinity_score|\n",
      "+------------+----------------+------------------+------------+--------------+------------------+-------------------+-------------------+-----------------+-----------------+-----------+-----------+--------+-----------------+-----------------+\n",
      "|        1019|LG Running Shoes|        Gaming 590|        1261|  Apple Tablet|            Garden|                  3|0.25597269624573377|           4.6875|            100.0|    18.3125|    18.3125| 18.3125|      Very Strong|91.69929180887372|\n",
      "+------------+----------------+------------------+------------+--------------+------------------+-------------------+-------------------+-----------------+-----------------+-----------+-----------+--------+-----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+----------------+------------------+----------------------+------------------------+----------------------------+-------------------+-------------------+-----------------+-----------+--------+-----------------+-----------------+\n",
      "|product_a_id|  product_a_name|product_a_category|recommended_product_id|recommended_product_name|recommended_product_category|co_occurrence_count|            support|confidence_a_to_b|lift_a_to_b|avg_lift|affinity_strength|   affinity_score|\n",
      "+------------+----------------+------------------+----------------------+------------------------+----------------------------+-------------------+-------------------+-----------------+-----------+--------+-----------------+-----------------+\n",
      "|        1019|LG Running Shoes|        Gaming 590|                  1261|            Apple Tablet|                      Garden|                  3|0.25597269624573377|           4.6875|    18.3125| 18.3125|      Very Strong|91.69929180887372|\n",
      "+------------+----------------+------------------+----------------------+------------------------+----------------------------+-------------------+-------------------+-----------------+-----------+--------+-----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+--------------+------------------+------------+----------------+------------------+-------------------+-------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|product_a_id|product_a_name|product_a_category|product_b_id|  product_b_name|product_b_category|co_occurrence_count|            support|confidence_a_to_b|confidence_b_to_a|       lift_a_to_b|       lift_b_to_a|          avg_lift|affinity_strength|   affinity_score|\n",
      "+------------+--------------+------------------+------------+----------------+------------------+-------------------+-------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|        1000|          NULL|              NULL|        1009|Canon Headphones|           Kitchen|                  5|0.42662116040955633|7.246376811594203| 8.47457627118644|1.4394497666421027|1.4394497666421024|1.4394497666421024|             Weak|7.425235181333379|\n",
      "+------------+--------------+------------------+------------+----------------+------------------+-------------------+-------------------+-----------------+-----------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------------+--------------------+------------------+-------------------+\n",
      "|product_a_id|recommended_products|avg_affinity_score|has_recommendations|\n",
      "+------------+--------------------+------------------+-------------------+\n",
      "|        1000|{\"Amazon Monitor ...| 25.62536931652816|                  1|\n",
      "+------------+--------------------+------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------+-----------------------------+------------------+\n",
      "|total_products|products_with_recommendations|     coverage_rate|\n",
      "+--------------+-----------------------------+------------------+\n",
      "|            18|                           17|0.9444444444444444|\n",
      "+--------------+-----------------------------+------------------+\n",
      "\n",
      "+----------+-----------+---------------+-------------+-------------------+------------------+---------------------+\n",
      "|product_id|supplier_id|available_stock|current_stock|minimum_stock_level|    days_of_supply|stock_status_computed|\n",
      "+----------+-----------+---------------+-------------+-------------------+------------------+---------------------+\n",
      "|      1265|      10007|            -68|          194|                 77|2.3095238095238093|             critical|\n",
      "+----------+-----------+---------------+-------------+-------------------+------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "|product_id|supplier_id|available_stock|avg_daily_sales|days_of_supply|\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "|      1206|      10784|              0|           22.0|           0.0|\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+---------------+-------------+-------------------+--------------------------+--------------+---------------------+--------------------+\n",
      "|product_id|available_stock|current_stock|minimum_stock_level|reorder_point_breach_count|days_of_supply|reorder_urgency_score|reorder_urgency_tier|\n",
      "+----------+---------------+-------------+-------------------+--------------------------+--------------+---------------------+--------------------+\n",
      "|      1896|             55|           55|                 95|                         1|        3.4375|                  180|              urgent|\n",
      "+----------+---------------+-------------+-------------------+--------------------------+--------------+---------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+--------------------------+------------------+---------------+-------------+---------------+--------------+------------+------------+--------+----------------+-------------+\n",
      "|product_id|supplier_id|reorder_point_breach_count|stock_health_score|reorder_urgency|current_stock|available_stock|days_of_supply|stock_status|product_name|category|total_units_sold|total_revenue|\n",
      "+----------+-----------+--------------------------+------------------+---------------+-------------+---------------+--------------+------------+------------+--------+----------------+-------------+\n",
      "+----------+-----------+--------------------------+------------------+---------------+-------------+---------------+--------------+------------+------------+--------+----------------+-------------+\n",
      "\n",
      "+----------+-----------+------------------+------------+---------------------+-------------+---------------+--------------+------------+------------+--------+----------------+-------------+\n",
      "|product_id|supplier_id|stock_health_score|storage_cost|storage_cost_per_unit|current_stock|available_stock|days_of_supply|stock_status|product_name|category|total_units_sold|total_revenue|\n",
      "+----------+-----------+------------------+------------+---------------------+-------------+---------------+--------------+------------+------------+--------+----------------+-------------+\n",
      "+----------+-----------+------------------+------------+---------------------+-------------+---------------+--------------+------------+------------+--------+----------------+-------------+\n",
      "\n",
      "+----------+------------+--------+-----------+------------+---------------+-------------+---------------------+\n",
      "|product_id|product_name|category|supplier_id|storage_cost|available_stock|current_stock|storage_cost_per_unit|\n",
      "+----------+------------+--------+-----------+------------+---------------+-------------+---------------------+\n",
      "|      1006|        NULL| Kitchen|      10940|        4.97|             34|           48|  0.10354166666666666|\n",
      "+----------+------------+--------+-----------+------------+---------------+-------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+--------------+-------------+-----------+------------+---------------------+---------------+-------------+------------------+----------------+-------------+-----------------------+-----------------------+------------------------------+\n",
      "|product_id|  product_name|     category|supplier_id|storage_cost|storage_cost_per_unit|available_stock|current_stock|stock_health_score|total_units_sold|total_revenue|avg_order_value_product|storage_cost_to_revenue|storage_cost_per_unit_to_price|\n",
      "+----------+--------------+-------------+-----------+------------+---------------------+---------------+-------------+------------------+----------------+-------------+-----------------------+-----------------------+------------------------------+\n",
      "|      1348|Samsung Laptop|Winter Sports|      10743|        2.48|  0.00915129151291513|            271|          271|                75|              48|         0.48|                   0.16|      5.166666666666667|           0.05719557195571956|\n",
      "+----------+--------------+-------------+-----------+------------+---------------------+---------------+-------------+------------------+----------------+-------------+-----------------------+-----------------------+------------------------------+\n",
      "\n",
      "+----------+-----------+---------------+-----------------+-----------+--------------+\n",
      "|product_id|supplier_id|available_stock|reserved_quantity|total_stock|reserved_share|\n",
      "+----------+-----------+---------------+-----------------+-----------+--------------+\n",
      "|      1385|      10606|            -29|               69|         40|         1.725|\n",
      "+----------+-----------+---------------+-----------------+-----------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+------------+--------+-----------+---------------+-------------+-------------------------+------------------------+----------------+-------------+-----------------+------------------+---------------+\n",
      "|product_id|product_name|category|supplier_id|available_stock|current_stock|avg_daily_sales_effective|days_of_supply_effective|total_units_sold|total_revenue|days_since_launch|stock_health_score|reorder_urgency|\n",
      "+----------+------------+--------+-----------+---------------+-------------+-------------------------+------------------------+----------------+-------------+-----------------+------------------+---------------+\n",
      "+----------+------------+--------+-----------+---------------+-------------+-------------------------+------------------------+----------------+-------------+-----------------+------------------+---------------+\n",
      "\n",
      "+----------+------------+--------+-----------+---------------+-------------+-------------------------+------------------------+----------------------------+----------------+-------------+-----------------+------------------+---------------+\n",
      "|product_id|product_name|category|supplier_id|available_stock|current_stock|avg_daily_sales_effective|days_of_supply_effective|days_since_restock_effective|total_units_sold|total_revenue|days_since_launch|stock_health_score|reorder_urgency|\n",
      "+----------+------------+--------+-----------+---------------+-------------+-------------------------+------------------------+----------------------------+----------------+-------------+-----------------+------------------+---------------+\n",
      "|      2022|        NULL|    NULL|        999|            212|          222|                      0.0|                    NULL|                       46029|            NULL|         NULL|             NULL|                75|            Low|\n",
      "+----------+------------+--------+-----------+---------------+-------------+-------------------------+------------------------+----------------------------+----------------+-------------+-----------------+------------------+---------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------------+-------------+--------------------------+---------------------------------+\n",
      "|cart_abandonment_reason|dropoff_count|avg_abandonment_risk_score|conversion_after_abandonment_rate|\n",
      "+-----------------------+-------------+--------------------------+---------------------------------+\n",
      "|   Long Time - Lost ...|          795|         1737.187933317902|                              0.0|\n",
      "+-----------------------+-------------+--------------------------+---------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------------+-------------+--------------------------+\n",
      "|  dropoff_bucket|dropoff_count|avg_abandonment_risk_score|\n",
      "+----------------+-------------+--------------------------+\n",
      "|other_or_unknown|         1649|        1696.1352705302836|\n",
      "+----------------+-------------+--------------------------+\n",
      "\n",
      "+-----------+----------------+-------------+--------------------------+----------------------------+\n",
      "|device_type|  dropoff_bucket|dropoff_count|avg_abandonment_risk_score|avg_session_engagement_score|\n",
      "+-----------+----------------+-------------+--------------------------+----------------------------+\n",
      "|       NULL|other_or_unknown|         1649|        1696.1352705302786|                        NULL|\n",
      "+-----------+----------------+-------------+--------------------------+----------------------------+\n",
      "\n",
      "+-----------+---------------+---------------+----------------------+-------------------+-----------------+------------------+-----------------------+-------------------------+--------------------------+-------------------------------+\n",
      "|supplier_id|supplier_status|total_stockouts|supplier_stockout_rate|inv_total_stockouts|inv_stockout_rate|inv_total_products|inv_total_current_stock|inv_total_available_stock|supplier_reliability_score|supplier_inventory_health_score|\n",
      "+-----------+---------------+---------------+----------------------+-------------------+-----------------+------------------+-----------------------+-------------------------+--------------------------+-------------------------------+\n",
      "|      10479|         Active|              2|                 100.0|                  2|            100.0|                 2|                      0|                        0|                     5.412|                            0.0|\n",
      "+-----------+---------------+---------------+----------------------+-------------------+-----------------+------------------+-----------------------+-------------------------+--------------------------+-------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+------------------+---------------------+-------------------+-------------------------+-------------------------------+-----------------------+\n",
      "|supplier_id|total_storage_cost|total_available_stock|total_current_stock|avg_storage_cost_per_unit|supplier_inventory_health_score|storage_cost_efficiency|\n",
      "+-----------+------------------+---------------------+-------------------+-------------------------+-------------------------------+-----------------------+\n",
      "|      10980|              3.02|                    1|                  1|                     3.02|                           50.0|                   3.02|\n",
      "+-----------+------------------+---------------------+-------------------+-------------------------+-------------------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+------------------+---------------------+\n",
      "|supplier_id|total_storage_cost|total_available_stock|\n",
      "+-----------+------------------+---------------------+\n",
      "|        999|103.66000000000003|                 4596|\n",
      "+-----------+------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+---------------+------------------------------------+--------------------------+----------------------+\n",
      "|supplier_id|supplier_status|supplier_reliability_score_effective|supplier_performance_score|stock_efficiency_ratio|\n",
      "+-----------+---------------+------------------------------------+--------------------------+----------------------+\n",
      "|      10627|           NULL|                                6.61|                      NULL|     60.15465608628343|\n",
      "+-----------+---------------+------------------------------------+--------------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+---------------+----------------------+-------------------+-----------------+------------------+-----------------------+-------------------------+\n",
      "|supplier_id|total_stockouts|supplier_stockout_rate|inv_total_stockouts|inv_stockout_rate|inv_total_products|inv_total_current_stock|inv_total_available_stock|\n",
      "+-----------+---------------+----------------------+-------------------+-----------------+------------------+-----------------------+-------------------------+\n",
      "|      10479|              2|                 100.0|                  2|            100.0|                 2|                      0|                        0|\n",
      "+-----------+---------------+----------------------+-------------------+-----------------+------------------+-----------------------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+---------------+----------------------+----------------+---------------------+--------------------------+----------------------+------------------------------------+\n",
      "|supplier_id|supplier_status|total_orders_fulfilled|total_units_sold|avg_restock_lead_time|supplier_performance_score|stock_efficiency_ratio|supplier_reliability_score_effective|\n",
      "+-----------+---------------+----------------------+----------------+---------------------+--------------------------+----------------------+------------------------------------+\n",
      "|      10069|         Active|                    52|            1591|                  0.0|        1000.9271274705883|     478.1741225199187|                                4.08|\n",
      "+-----------+---------------+----------------------+----------------+---------------------+--------------------------+----------------------+------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----------------------+----------------------+-----------------+--------------------------+\n",
      "|supplier_id|total_revenue_generated|total_orders_fulfilled|  avg_order_value|revenue_contribution_share|\n",
      "+-----------+-----------------------+----------------------+-----------------+--------------------------+\n",
      "|      10069|             3151153.17|                    52|60599.09942307692|       0.09202665075457295|\n",
      "+-----------+-----------------------+----------------------+-----------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+------------------+-----------------------+----------------------+\n",
      "|supplier_id| avg_profit_margin|total_revenue_generated|total_orders_fulfilled|\n",
      "+-----------+------------------+-----------------------+----------------------+\n",
      "|      10648|1532.3200000000002|               174301.4|                     4|\n",
      "+-----------+------------------+-----------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+-----------------------+------------------+-----------------------+-------------------------+\n",
      "|supplier_id|days_since_last_restock|inv_total_products|inv_total_current_stock|inv_total_available_stock|\n",
      "+-----------+-----------------------+------------------+-----------------------+-------------------------+\n",
      "|      10280|                  46029|                 1|                     41|                       37|\n",
      "+-----------+-----------------------+------------------+-----------------------+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------+---------------+-------------------+-----------------+--------------------------+--------------------+------------------------------------+--------------------------+\n",
      "|supplier_id|supplier_status|contract_start_date|contract_end_date|days_until_contract_expiry|contract_status_flag|supplier_reliability_score_effective|supplier_performance_score|\n",
      "+-----------+---------------+-------------------+-----------------+--------------------------+--------------------+------------------------------------+--------------------------+\n",
      "|      10546|           NULL|         2023-06-20|       1900-01-01|                    -46029|             Expired|                               5.556|                 56.596007|\n",
      "+-----------+---------------+-------------------+-----------------+--------------------------+--------------------+------------------------------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for keys in product_analysis.keys():\n",
    "    product_analysis[keys].show(1)\n",
    "    \n",
    "for keys in supplier_analysis.keys():\n",
    "    supplier_analysis[keys].show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ed88f28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis['business_health_daily'] has no NULL values (1069 rows)\n",
      "analysis['business_health_weekly'] has no NULL values (125 rows)\n",
      "analysis['business_health_monthly'] has no NULL values (43 rows)\n",
      "analysis['low_margin_categories'] has some NULL values in columns: ['category']\n",
      "analysis['customer_account_status_distribution_daily'] has some NULL values in columns: ['account_status']\n",
      "analysis['customer_account_status_distribution_weekly'] has some NULL values in columns: ['account_status']\n",
      "analysis['customer_account_status_distribution_monthly'] has some NULL values in columns: ['account_status']\n",
      "analysis['new_customers_daily'] has no NULL values (764 rows)\n",
      "analysis['new_customers_weekly'] has no NULL values (256 rows)\n",
      "analysis['new_customers_monthly'] has no NULL values (62 rows)\n",
      "analysis['cumulative_customers_daily'] has no NULL values (764 rows)\n",
      "analysis['cumulative_customers_weekly'] has no NULL values (256 rows)\n",
      "analysis['cumulative_customers_monthly'] has no NULL values (62 rows)\n",
      "analysis['new_customers_geo_acquisition_daily'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['new_customers_geo_acquisition_monthly'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['customer_age_group_distribution'] has no NULL values (6 rows)\n",
      "analysis['customer_city_distribution'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['customer_state_distribution'] has some NULL values in columns: ['country', 'state_province']\n",
      "analysis['customer_country_distribution'] has some NULL values in columns: ['country']\n",
      "analysis['customer_age_group_spending'] has no NULL values (6 rows)\n",
      "analysis['gender_category_preference'] has no NULL values (4 rows)\n",
      "analysis['gender_product_preference'] has no NULL values (6 rows)\n",
      "analysis['new_vs_returning_customer_country'] has some NULL values in columns: ['country']\n",
      "analysis['new_vs_returning_customer_city'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['customer_engagement'] has some NULL values in columns: ['total_pages_viewed', 'total_products_viewed']\n",
      "analysis['customer_engagement_summary'] has no NULL values (1 rows)\n",
      "analysis['session_to_order_analysis'] has no NULL values (1 rows)\n",
      "analysis['top_customers_by_revenue'] has some NULL values in columns: ['customer_segment', 'rfm_segment']\n",
      "analysis['top_customers_by_profit'] has some NULL values in columns: ['total_order_profit', 'total_net_profit', 'customer_segment', 'rfm_segment', 'total_revenue', 'customer_lifetime_value']\n",
      "analysis['session_conversion_distribution'] has no NULL values (2 rows)\n",
      "analysis['cart_abandonment_distribution'] has no NULL values (3 rows)\n",
      "analysis['clv_summary'] has no NULL values (1 rows)\n",
      "analysis['rev_by_country_city'] has some NULL values in columns: ['country', 'city']\n",
      "analysis['rev_by_customer_segment'] has some NULL values in columns: ['customer_segment']\n",
      "analysis['rev_by_rfm_segment'] has some NULL values in columns: ['rfm_segment']\n",
      "analysis['rev_by_segment_label'] has some NULL values in columns: ['customer_segment_label']\n",
      "analysis['rev_by_referrer'] has some NULL values in columns: ['preferred_referrer_source']\n",
      "analysis['rev_by_device'] has some NULL values in columns: ['preferred_device_type']\n",
      "analysis['aov_trend_daily'] has no NULL values (573 rows)\n",
      "analysis['aov_trend_weekly'] has no NULL values (125 rows)\n",
      "analysis['aov_trend_monthly'] has no NULL values (43 rows)\n",
      "analysis['discount_customers'] has some NULL values in columns: ['account_status', 'total_cancelled_orders', 'total_pages_viewed', 'total_products_viewed', 'total_purchased_carts', 'order_frequency', 'gender', 'city', 'state_province', 'country', 'order_recency_days', 'order_total_spent', 'customer_segment', 'avg_order_value', 'avg_items_per_order', 'first_order_date', 'last_order_date', 'avg_days_between_orders', 'avg_review_rating', 'avg_session_duration', 'session_conversion_rate', 'cart_abandonment_rate', 'preferred_device_type', 'preferred_referrer_source', 'wishlist_conversion_rate', 'preferred_payment_method', 'days_since_last_purchase', 'cancellation_rate', 'customer_activity_score', 'avg_time_in_cart_days', 'customer_abandonment_rate', 'customer_purchase_rate', 'recency_score', 'frequency_score', 'monetary_score', 'rfm_segment', 'customer_segment_label', 'rfm_overall_score', 'rfm_category', 'churn_risk']\n",
      "analysis['discount_customers_summary'] has no NULL values (1 rows)\n",
      "analysis['correlation_discount_vs_clv'] has no NULL values (1 rows)\n",
      "analysis['high_discount_customers'] has 0 rows\n",
      "analysis['churn_risk_summary'] has some NULL values in columns: ['churn_risk', 'avg_recency_days']\n",
      "analysis['high_clv_at_risk'] has 0 rows\n",
      "analysis['rfm_segment_summary'] has some NULL values in columns: ['rfm_segment', 'avg_recency_days', 'avg_aov']\n",
      "analysis['high_intent_non_buyers'] has some NULL values in columns: ['total_purchased_carts']\n",
      "analysis['customers_cohorts'] has no NULL values (1 rows)\n",
      "analysis['signup_cohort_summary'] has no NULL values (1 rows)\n",
      "analysis['customer_overall_health_summary'] has some NULL values in columns: ['account_status', 'total_cancelled_orders', 'total_pages_viewed', 'total_products_viewed', 'total_purchased_carts', 'order_frequency', 'gender', 'city', 'state_province', 'country', 'order_recency_days', 'customer_segment', 'customer_segment_label', 'rfm_segment', 'rfm_category', 'churn_risk', 'avg_order_value', 'avg_items_per_order', 'total_discount_received', 'avg_discount_per_order', 'avg_session_duration', 'session_conversion_rate', 'cart_abandonment_rate', 'preferred_device_type', 'preferred_referrer_source', 'preferred_payment_method', 'wishlist_conversion_rate', 'days_since_last_purchase', 'cancellation_rate', 'customer_activity_score', 'avg_time_in_cart_days', 'customer_abandonment_rate', 'customer_purchase_rate']\n",
      "analysis['rfm_churn_crosstab'] has some NULL values in columns: ['rfm_segment', 'churn_risk']\n",
      "analysis['seg_referrer_crosstab'] has some NULL values in columns: ['customer_segment_label', 'preferred_referrer_source']\n",
      "analysis['seg_device_crosstab'] has some NULL values in columns: ['customer_segment_label', 'preferred_device_type']\n",
      "analysis['referrer_source_summary'] has some NULL values in columns: ['preferred_referrer_source']\n",
      "analysis['referrer_churn_summary'] has some NULL values in columns: ['preferred_referrer_source', 'churn_risk']\n",
      "analysis['customer_profit_per_segment'] has some NULL values in columns: ['customer_segment_label', 'total_order_profit', 'total_net_profit', 'avg_profit_per_customer']\n",
      "analysis['segment_aov_by_rfm'] has no NULL values (45 rows)\n",
      "analysis['inventory_carrying_cost_overall'] has no NULL values (1 rows)\n",
      "analysis['wishlist_overall_summary'] has no NULL values (1 rows)\n",
      "analysis['wishlist_by_product'] has no NULL values (847 rows)\n",
      "analysis['wishlist_by_customer'] has no NULL values (851 rows)\n",
      "analysis['wishlist_time_to_purchase_stats'] has no NULL values (1 rows)\n",
      "analysis['wishlist_time_to_purchase_distribution'] has no NULL values (2 rows)\n",
      "analysis['abandoned_wishlist_items'] has some NULL values in columns: ['added_date', 'purchased_date', 'removed_date']\n",
      "analysis['abandoned_wishlist_by_customer'] has no NULL values (773 rows)\n",
      "analysis['abandoned_wishlist_by_product'] has no NULL values (753 rows)\n",
      "analysis['wishlist_adds_by_month'] has no NULL values (49 rows)\n",
      "analysis['cart_overall_stats'] has no NULL values (1 rows)\n",
      "analysis['cart_status_distribution'] has some NULL values in columns: ['cart_status']\n",
      "analysis['cart_abandon_summary'] has no NULL values (1 rows)\n",
      "analysis['cart_value_stats'] has some NULL values in columns: ['cart_status']\n",
      "analysis['high_value_abandoned_carts'] has 0 rows\n",
      "analysis['time_to_purchase_overall'] has some NULL values in columns: ['avg_time_in_cart_days', 'avg_time_in_cart_hours', 'time_in_cart_days_percentiles']\n",
      "analysis['time_to_purchase_by_tier'] has 0 rows\n",
      "analysis['time_to_purchase_buckets'] has 0 rows\n",
      "analysis['campaign_performance'] has some NULL values in columns: ['campaign_type', 'campaign_status', 'performance_tier', 'avg_order_value', 'roas_effective', 'roi_effective']\n",
      "analysis['device_conversion_rates'] has no NULL values (1 rows)\n",
      "analysis['payment_counts_by_country_method'] has no NULL values (63 rows)\n",
      "analysis['payment_counts_by_state_method'] has no NULL values (63 rows)\n",
      "analysis['payment_method_success_rates'] has no NULL values (75 rows)\n",
      "analysis['payment_method_success_rates_by_country'] has no NULL values (75 rows)\n",
      "analysis['payment_method_aov'] has some NULL values in columns: ['total_revenue', 'avg_order_value_method']\n",
      "analysis['refund_rate_by_payment_method'] has no NULL values (75 rows)\n",
      "analysis['refund_rate_by_product'] has some NULL values in columns: ['product_name', 'category']\n",
      "analysis['refund_rate_by_month'] has no NULL values (43 rows)\n",
      "analysis['time_to_refund_by_payment_method'] has no NULL values (36 rows)\n",
      "analysis['review_velocity_daily'] has no NULL values (2384 rows)\n",
      "analysis['review_velocity_weekly'] has no NULL values (2384 rows)\n",
      "analysis['review_velocity_monthly'] has no NULL values (1635 rows)\n",
      "analysis['sentiment_by_category'] has some NULL values in columns: ['category']\n",
      "analysis['product_monthly_rating_trends'] has some NULL values in columns: ['product_name', 'category', 'total_units_sold', 'total_revenue']\n",
      "analysis['low_rated_product_monthly_trends_rating_only'] has some NULL values in columns: ['product_name', 'category', 'total_units_sold', 'total_revenue']\n",
      "analysis['rating_tier_per_product'] has some NULL values in columns: ['product_name', 'category']\n",
      "analysis['rating_tier_sales_velocity'] has no NULL values (4 rows)\n",
      "analysis['processing_by_category'] has some NULL values in columns: ['category', 'avg_delivery_days', 'avg_total_fulfillment_days']\n",
      "analysis['processing_by_subcategory'] has some NULL values in columns: ['category', 'avg_processing_days', 'avg_delivery_days', 'median_processing_days']\n",
      "analysis['processing_by_hour'] has no NULL values (24 rows)\n",
      "analysis['processing_by_day_of_week'] has no NULL values (7 rows)\n",
      "analysis['weekend_vs_weekday'] has no NULL values (2 rows)\n",
      "analysis['delivery_days_by_country'] has some NULL values in columns: ['country']\n",
      "analysis['delivery_days_by_state'] has some NULL values in columns: ['country', 'state_province']\n",
      "analysis['delivery_days_by_city'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['ontime_delivery_by_country'] has some NULL values in columns: ['country']\n",
      "analysis['ontime_delivery_by_state'] has some NULL values in columns: ['country', 'state_province']\n",
      "analysis['ontime_delivery_by_city'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['shipping_efficiency_by_country'] has some NULL values in columns: ['country']\n",
      "analysis['shipping_efficiency_by_state'] has some NULL values in columns: ['country', 'state_province']\n",
      "analysis['shipping_efficiency_by_city'] has some NULL values in columns: ['country', 'state_province', 'city']\n",
      "analysis['processing_by_season'] has no NULL values (4 rows)\n",
      "analysis['processing_by_season_and_status'] has some NULL values in columns: ['order_status', 'avg_processing_days', 'median_processing_days']\n",
      "analysis['shipping_cost_outliers'] has 0 rows\n",
      "analysis['customer_cohort_retention'] has no NULL values (2 rows)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "❌ Empty/None DataFrames (6):\n",
      "   - high_discount_customers\n",
      "   - high_clv_at_risk\n",
      "   - high_value_abandoned_carts\n",
      "   - time_to_purchase_by_tier\n",
      "   - time_to_purchase_buckets\n",
      "   - shipping_cost_outliers\n",
      "\n",
      "⚠️  All-NULL DataFrames (0):\n",
      "\n",
      "⚡ DataFrames with Some NULL values (54):\n",
      "   - low_margin_categories: ['category']\n",
      "   - customer_account_status_distribution_daily: ['account_status']\n",
      "   - customer_account_status_distribution_weekly: ['account_status']\n",
      "   - customer_account_status_distribution_monthly: ['account_status']\n",
      "   - new_customers_geo_acquisition_daily: ['country', 'state_province', 'city']\n",
      "   - new_customers_geo_acquisition_monthly: ['country', 'state_province', 'city']\n",
      "   - customer_city_distribution: ['country', 'state_province', 'city']\n",
      "   - customer_state_distribution: ['country', 'state_province']\n",
      "   - customer_country_distribution: ['country']\n",
      "   - new_vs_returning_customer_country: ['country']\n",
      "   - new_vs_returning_customer_city: ['country', 'state_province', 'city']\n",
      "   - customer_engagement: ['total_pages_viewed', 'total_products_viewed']\n",
      "   - top_customers_by_revenue: ['customer_segment', 'rfm_segment']\n",
      "   - top_customers_by_profit: ['total_order_profit', 'total_net_profit', 'customer_segment', 'rfm_segment', 'total_revenue', 'customer_lifetime_value']\n",
      "   - rev_by_country_city: ['country', 'city']\n",
      "   - rev_by_customer_segment: ['customer_segment']\n",
      "   - rev_by_rfm_segment: ['rfm_segment']\n",
      "   - rev_by_segment_label: ['customer_segment_label']\n",
      "   - rev_by_referrer: ['preferred_referrer_source']\n",
      "   - rev_by_device: ['preferred_device_type']\n",
      "   - discount_customers: ['account_status', 'total_cancelled_orders', 'total_pages_viewed', 'total_products_viewed', 'total_purchased_carts', 'order_frequency', 'gender', 'city', 'state_province', 'country', 'order_recency_days', 'order_total_spent', 'customer_segment', 'avg_order_value', 'avg_items_per_order', 'first_order_date', 'last_order_date', 'avg_days_between_orders', 'avg_review_rating', 'avg_session_duration', 'session_conversion_rate', 'cart_abandonment_rate', 'preferred_device_type', 'preferred_referrer_source', 'wishlist_conversion_rate', 'preferred_payment_method', 'days_since_last_purchase', 'cancellation_rate', 'customer_activity_score', 'avg_time_in_cart_days', 'customer_abandonment_rate', 'customer_purchase_rate', 'recency_score', 'frequency_score', 'monetary_score', 'rfm_segment', 'customer_segment_label', 'rfm_overall_score', 'rfm_category', 'churn_risk']\n",
      "   - churn_risk_summary: ['churn_risk', 'avg_recency_days']\n",
      "   - rfm_segment_summary: ['rfm_segment', 'avg_recency_days', 'avg_aov']\n",
      "   - high_intent_non_buyers: ['total_purchased_carts']\n",
      "   - customer_overall_health_summary: ['account_status', 'total_cancelled_orders', 'total_pages_viewed', 'total_products_viewed', 'total_purchased_carts', 'order_frequency', 'gender', 'city', 'state_province', 'country', 'order_recency_days', 'customer_segment', 'customer_segment_label', 'rfm_segment', 'rfm_category', 'churn_risk', 'avg_order_value', 'avg_items_per_order', 'total_discount_received', 'avg_discount_per_order', 'avg_session_duration', 'session_conversion_rate', 'cart_abandonment_rate', 'preferred_device_type', 'preferred_referrer_source', 'preferred_payment_method', 'wishlist_conversion_rate', 'days_since_last_purchase', 'cancellation_rate', 'customer_activity_score', 'avg_time_in_cart_days', 'customer_abandonment_rate', 'customer_purchase_rate']\n",
      "   - rfm_churn_crosstab: ['rfm_segment', 'churn_risk']\n",
      "   - seg_referrer_crosstab: ['customer_segment_label', 'preferred_referrer_source']\n",
      "   - seg_device_crosstab: ['customer_segment_label', 'preferred_device_type']\n",
      "   - referrer_source_summary: ['preferred_referrer_source']\n",
      "   - referrer_churn_summary: ['preferred_referrer_source', 'churn_risk']\n",
      "   - customer_profit_per_segment: ['customer_segment_label', 'total_order_profit', 'total_net_profit', 'avg_profit_per_customer']\n",
      "   - abandoned_wishlist_items: ['added_date', 'purchased_date', 'removed_date']\n",
      "   - cart_status_distribution: ['cart_status']\n",
      "   - cart_value_stats: ['cart_status']\n",
      "   - time_to_purchase_overall: ['avg_time_in_cart_days', 'avg_time_in_cart_hours', 'time_in_cart_days_percentiles']\n",
      "   - campaign_performance: ['campaign_type', 'campaign_status', 'performance_tier', 'avg_order_value', 'roas_effective', 'roi_effective']\n",
      "   - payment_method_aov: ['total_revenue', 'avg_order_value_method']\n",
      "   - refund_rate_by_product: ['product_name', 'category']\n",
      "   - sentiment_by_category: ['category']\n",
      "   - product_monthly_rating_trends: ['product_name', 'category', 'total_units_sold', 'total_revenue']\n",
      "   - low_rated_product_monthly_trends_rating_only: ['product_name', 'category', 'total_units_sold', 'total_revenue']\n",
      "   - rating_tier_per_product: ['product_name', 'category']\n",
      "   - processing_by_category: ['category', 'avg_delivery_days', 'avg_total_fulfillment_days']\n",
      "   - processing_by_subcategory: ['category', 'avg_processing_days', 'avg_delivery_days', 'median_processing_days']\n",
      "   - delivery_days_by_country: ['country']\n",
      "   - delivery_days_by_state: ['country', 'state_province']\n",
      "   - delivery_days_by_city: ['country', 'state_province', 'city']\n",
      "   - ontime_delivery_by_country: ['country']\n",
      "   - ontime_delivery_by_state: ['country', 'state_province']\n",
      "   - ontime_delivery_by_city: ['country', 'state_province', 'city']\n",
      "   - shipping_efficiency_by_country: ['country']\n",
      "   - shipping_efficiency_by_state: ['country', 'state_province']\n",
      "   - shipping_efficiency_by_city: ['country', 'state_province', 'city']\n",
      "   - processing_by_season_and_status: ['order_status', 'avg_processing_days', 'median_processing_days']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize tracking lists\n",
    "empty_dataframes = []\n",
    "all_null_dataframes = []\n",
    "has_null_values = []\n",
    "\n",
    "for key, df in analysis.items():\n",
    "    if df is None:\n",
    "        empty_dataframes.append(key)\n",
    "        print(f\"analysis['{key}'] is None\")\n",
    "    else:\n",
    "        row_count = df.count()\n",
    "        \n",
    "        if row_count == 0:\n",
    "            empty_dataframes.append(key)\n",
    "            print(f\"analysis['{key}'] has 0 rows\")\n",
    "        else:\n",
    "            null_count = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "            \n",
    "            # Check if all columns are completely null\n",
    "            if all(null_count[c] == row_count for c in df.columns):\n",
    "                all_null_dataframes.append(key)\n",
    "                print(f\"analysis['{key}'] has all NULL values in every column for all {row_count} rows\")\n",
    "            # Check if some columns have nulls\n",
    "            elif any(null_count[c] > 0 for c in df.columns):\n",
    "                null_cols = [c for c in df.columns if null_count[c] > 0]\n",
    "                has_null_values.append((key, null_cols))\n",
    "                print(f\"analysis['{key}'] has some NULL values in columns: {null_cols}\")\n",
    "            else:\n",
    "                print(f\"analysis['{key}'] has no NULL values ({row_count} rows)\")\n",
    "\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n❌ Empty/None DataFrames ({len(empty_dataframes)}):\")\n",
    "for key in empty_dataframes:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(f\"\\n⚠️  All-NULL DataFrames ({len(all_null_dataframes)}):\")\n",
    "for key in all_null_dataframes:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(f\"\\n⚡ DataFrames with Some NULL values ({len(has_null_values)}):\")\n",
    "for key, cols in has_null_values:\n",
    "    print(f\"   - {key}: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3c5f82d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_analysis['best_selling_products'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['product_monthly_trends'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['category_monthly_trends'] has some NULL values in columns: ['category']\n",
      "product_analysis['product_calendar_month_seasonality'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['category_calendar_month_seasonality'] has some NULL values in columns: ['category']\n",
      "product_analysis['out_of_stock_products'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['product_rating_summary'] has no NULL values (820 rows)\n",
      "product_analysis['category_view_patterns'] has some NULL values in columns: ['category']\n",
      "product_analysis['top_view_to_purchase_products'] has some NULL values in columns: ['product_name', 'category']\n",
      "product_analysis['product_performance_score'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['category_revenue_share'] has some NULL values in columns: ['category']\n",
      "product_analysis['low_performing_categories'] has some NULL values in columns: ['category']\n",
      "product_analysis['category_popularity_score'] has some NULL values in columns: ['category']\n",
      "product_analysis['category_monthly_seasonality'] has some NULL values in columns: ['category']\n",
      "product_analysis['category_peak_season'] has no NULL values (95 rows)\n",
      "product_analysis['product_lifecycle_segments'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['product_lifecycle_summary'] has no NULL values (3 rows)\n",
      "product_analysis['product_stockout_risk'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['product_stockout_replenishment'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['product_dead_stock'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand']\n",
      "product_analysis['product_inventory_health'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand', 'total_units_sold', 'total_orders', 'total_revenue']\n",
      "product_analysis['product_inventory_critical'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand', 'total_units_sold', 'total_orders', 'criticality_score']\n",
      "product_analysis['supplier_product_performance'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand', 'supplier_status', 'is_preferred', 'is_verified', 'sup_total_products_supplied', 'sup_total_units_sold', 'sup_total_orders_fulfilled', 'sup_total_reviews', 'sup_total_stockouts', 'sup_total_revenue_generated', 'sup_avg_profit_margin', 'sup_avg_product_rating', 'supplier_performance_score', 'supplier_reliability_score', 'stock_efficiency_ratio', 'sup_stockout_rate', 'sup_inventory_health_score', 'inv_total_products', 'inv_total_current_stock', 'inv_total_available_stock', 'inv_total_reorder_breaches', 'inv_total_stockouts', 'inv_total_storage_cost', 'inv_avg_stock_per_product', 'inv_stockout_rate', 'inv_inventory_health_score']\n",
      "product_analysis['stockout_rate_by_product'] has some NULL values in columns: ['days_of_supply', 'product_name', 'category']\n",
      "product_analysis['supplier_ranking_core'] has some NULL values in columns: ['supplier_status']\n",
      "product_analysis['supplier_stockout_impact_on_products'] has some NULL values in columns: ['product_name', 'category', 'sub_category', 'brand', 'supplier_status', 'is_preferred', 'is_verified', 'supplier_performance_score', 'supplier_reliability_score']\n",
      "product_analysis['category_affinity_pairs'] has no NULL values (36 rows)\n",
      "product_analysis['category_affinity_top_per_category'] has no NULL values (9 rows)\n",
      "product_analysis['product_affinity_pairs'] has some NULL values in columns: ['product_a_name', 'product_a_category', 'product_b_name', 'product_b_category']\n",
      "product_analysis['product_affinity_top_per_product'] has some NULL values in columns: ['product_a_name', 'product_a_category', 'recommended_product_name', 'recommended_product_category']\n",
      "product_analysis['product_affinity_top5_candidates'] has some NULL values in columns: ['product_a_name', 'product_a_category', 'product_b_name', 'product_b_category']\n",
      "product_analysis['precomputed_product_recommendations'] has no NULL values (18 rows)\n",
      "product_analysis['precomputed_reco_coverage'] has no NULL values (1 rows)\n",
      "product_analysis['inventory_stock_status'] has some NULL values in columns: ['days_of_supply']\n",
      "product_analysis['days_of_supply'] has some NULL values in columns: ['days_of_supply']\n",
      "product_analysis['sku_reorder_urgency'] has some NULL values in columns: ['days_of_supply']\n",
      "product_analysis['reorder_point_breach_frequency'] has 0 rows\n",
      "product_analysis['overstock_analysis'] has 0 rows\n",
      "product_analysis['inventory_carrying_cost_by_product'] has some NULL values in columns: ['product_name', 'category']\n",
      "product_analysis['margin_erosion_risk'] has no NULL values (1 rows)\n",
      "product_analysis['reserved_vs_available'] has no NULL values (1365 rows)\n",
      "product_analysis['excess_inventory_not_selling'] has 0 rows\n",
      "product_analysis['aging_inventory_slow_movers'] has some NULL values in columns: ['product_name', 'category', 'days_of_supply_effective', 'total_units_sold', 'total_revenue', 'days_since_launch']\n",
      "product_analysis['checkout_dropoff_reasons'] has no NULL values (4 rows)\n",
      "product_analysis['checkout_dropoff_buckets'] has no NULL values (1 rows)\n",
      "product_analysis['checkout_dropoff_by_device_and_reason'] has some NULL values in columns: ['device_type', 'avg_session_engagement_score']\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "❌ Empty/None DataFrames (3):\n",
      "   - reorder_point_breach_frequency\n",
      "   - overstock_analysis\n",
      "   - excess_inventory_not_selling\n",
      "\n",
      "⚠️  All-NULL DataFrames (0):\n",
      "\n",
      "⚡ DataFrames with Some NULL values (32):\n",
      "   - best_selling_products: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - product_monthly_trends: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - category_monthly_trends: ['category']\n",
      "   - product_calendar_month_seasonality: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - category_calendar_month_seasonality: ['category']\n",
      "   - out_of_stock_products: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - category_view_patterns: ['category']\n",
      "   - top_view_to_purchase_products: ['product_name', 'category']\n",
      "   - product_performance_score: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - category_revenue_share: ['category']\n",
      "   - low_performing_categories: ['category']\n",
      "   - category_popularity_score: ['category']\n",
      "   - category_monthly_seasonality: ['category']\n",
      "   - product_lifecycle_segments: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - product_stockout_risk: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - product_stockout_replenishment: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - product_dead_stock: ['product_name', 'category', 'sub_category', 'brand']\n",
      "   - product_inventory_health: ['product_name', 'category', 'sub_category', 'brand', 'total_units_sold', 'total_orders', 'total_revenue']\n",
      "   - product_inventory_critical: ['product_name', 'category', 'sub_category', 'brand', 'total_units_sold', 'total_orders', 'criticality_score']\n",
      "   - supplier_product_performance: ['product_name', 'category', 'sub_category', 'brand', 'supplier_status', 'is_preferred', 'is_verified', 'sup_total_products_supplied', 'sup_total_units_sold', 'sup_total_orders_fulfilled', 'sup_total_reviews', 'sup_total_stockouts', 'sup_total_revenue_generated', 'sup_avg_profit_margin', 'sup_avg_product_rating', 'supplier_performance_score', 'supplier_reliability_score', 'stock_efficiency_ratio', 'sup_stockout_rate', 'sup_inventory_health_score', 'inv_total_products', 'inv_total_current_stock', 'inv_total_available_stock', 'inv_total_reorder_breaches', 'inv_total_stockouts', 'inv_total_storage_cost', 'inv_avg_stock_per_product', 'inv_stockout_rate', 'inv_inventory_health_score']\n",
      "   - stockout_rate_by_product: ['days_of_supply', 'product_name', 'category']\n",
      "   - supplier_ranking_core: ['supplier_status']\n",
      "   - supplier_stockout_impact_on_products: ['product_name', 'category', 'sub_category', 'brand', 'supplier_status', 'is_preferred', 'is_verified', 'supplier_performance_score', 'supplier_reliability_score']\n",
      "   - product_affinity_pairs: ['product_a_name', 'product_a_category', 'product_b_name', 'product_b_category']\n",
      "   - product_affinity_top_per_product: ['product_a_name', 'product_a_category', 'recommended_product_name', 'recommended_product_category']\n",
      "   - product_affinity_top5_candidates: ['product_a_name', 'product_a_category', 'product_b_name', 'product_b_category']\n",
      "   - inventory_stock_status: ['days_of_supply']\n",
      "   - days_of_supply: ['days_of_supply']\n",
      "   - sku_reorder_urgency: ['days_of_supply']\n",
      "   - inventory_carrying_cost_by_product: ['product_name', 'category']\n",
      "   - aging_inventory_slow_movers: ['product_name', 'category', 'days_of_supply_effective', 'total_units_sold', 'total_revenue', 'days_since_launch']\n",
      "   - checkout_dropoff_by_device_and_reason: ['device_type', 'avg_session_engagement_score']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize tracking lists\n",
    "empty_dataframes = []\n",
    "all_null_dataframes = []\n",
    "has_null_values = []\n",
    "\n",
    "for key, df in product_analysis.items():\n",
    "    if df is None:\n",
    "        empty_dataframes.append(key)\n",
    "        print(f\"product_analysis['{key}'] is None\")\n",
    "    else:\n",
    "        row_count = df.count()\n",
    "        \n",
    "        if row_count == 0:\n",
    "            empty_dataframes.append(key)\n",
    "            print(f\"product_analysis['{key}'] has 0 rows\")\n",
    "        else:\n",
    "            null_count = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "            \n",
    "            # Check if all columns are completely null\n",
    "            if all(null_count[c] == row_count for c in df.columns):\n",
    "                all_null_dataframes.append(key)\n",
    "                print(f\"product_analysis['{key}'] has all NULL values in every column for all {row_count} rows\")\n",
    "            # Check if some columns have nulls\n",
    "            elif any(null_count[c] > 0 for c in df.columns):\n",
    "                null_cols = [c for c in df.columns if null_count[c] > 0]\n",
    "                has_null_values.append((key, null_cols))\n",
    "                print(f\"product_analysis['{key}'] has some NULL values in columns: {null_cols}\")\n",
    "            else:\n",
    "                print(f\"product_analysis['{key}'] has no NULL values ({row_count} rows)\")\n",
    "\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n❌ Empty/None DataFrames ({len(empty_dataframes)}):\")\n",
    "for key in empty_dataframes:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(f\"\\n⚠️  All-NULL DataFrames ({len(all_null_dataframes)}):\")\n",
    "for key in all_null_dataframes:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(f\"\\n⚡ DataFrames with Some NULL values ({len(has_null_values)}):\")\n",
    "for key, cols in has_null_values:\n",
    "    print(f\"   - {key}: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3c77f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supplier_analysis['stockout_rate_by_supplier'] has no NULL values (941 rows)\n",
      "supplier_analysis['storage_cost_efficiency_by_supplier'] has no NULL values (740 rows)\n",
      "supplier_analysis['inventory_carrying_cost_by_supplier'] has no NULL values (740 rows)\n",
      "supplier_analysis['supplier_reliability'] has some NULL values in columns: ['supplier_status', 'supplier_performance_score', 'stock_efficiency_ratio']\n",
      "supplier_analysis['supplier_stockouts'] has no NULL values (941 rows)\n",
      "supplier_analysis['supplier_fulfillment_performance'] has some NULL values in columns: ['supplier_status']\n",
      "supplier_analysis['supplier_revenue_contribution'] has some NULL values in columns: ['avg_order_value']\n",
      "supplier_analysis['supplier_profit_margin'] has some NULL values in columns: ['avg_profit_margin']\n",
      "supplier_analysis['supplier_days_since_last_restock'] has some NULL values in columns: ['days_since_last_restock']\n",
      "supplier_analysis['supplier_contract_expiry'] has some NULL values in columns: ['supplier_status']\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "❌ Empty/None DataFrames (0):\n",
      "\n",
      "⚠️  All-NULL DataFrames (0):\n",
      "\n",
      "⚡ DataFrames with Some NULL values (6):\n",
      "   - supplier_reliability: ['supplier_status', 'supplier_performance_score', 'stock_efficiency_ratio']\n",
      "   - supplier_fulfillment_performance: ['supplier_status']\n",
      "   - supplier_revenue_contribution: ['avg_order_value']\n",
      "   - supplier_profit_margin: ['avg_profit_margin']\n",
      "   - supplier_days_since_last_restock: ['days_since_last_restock']\n",
      "   - supplier_contract_expiry: ['supplier_status']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize tracking lists\n",
    "empty_dataframes = []\n",
    "all_null_dataframes = []\n",
    "has_null_values = []\n",
    "\n",
    "for key, df in supplier_analysis.items():\n",
    "    if df is None:\n",
    "        empty_dataframes.append(key)\n",
    "        print(f\"supplier_analysis['{key}'] is None\")\n",
    "    else:\n",
    "        row_count = df.count()\n",
    "        \n",
    "        if row_count == 0:\n",
    "            empty_dataframes.append(key)\n",
    "            print(f\"supplier_analysis['{key}'] has 0 rows\")\n",
    "        else:\n",
    "            null_count = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0]\n",
    "            \n",
    "            # Check if all columns are completely null\n",
    "            if all(null_count[c] == row_count for c in df.columns):\n",
    "                all_null_dataframes.append(key)\n",
    "                print(f\"supplier_analysis['{key}'] has all NULL values in every column for all {row_count} rows\")\n",
    "            # Check if some columns have nulls\n",
    "            elif any(null_count[c] > 0 for c in df.columns):\n",
    "                null_cols = [c for c in df.columns if null_count[c] > 0]\n",
    "                has_null_values.append((key, null_cols))\n",
    "                print(f\"supplier_analysis['{key}'] has some NULL values in columns: {null_cols}\")\n",
    "            else:\n",
    "                print(f\"supplier_analysis['{key}'] has no NULL values ({row_count} rows)\")\n",
    "\n",
    "# Summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n❌ Empty/None DataFrames ({len(empty_dataframes)}):\")\n",
    "for key in empty_dataframes:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(f\"\\n⚠️  All-NULL DataFrames ({len(all_null_dataframes)}):\")\n",
    "for key in all_null_dataframes:\n",
    "    print(f\"   - {key}\")\n",
    "\n",
    "print(f\"\\n⚡ DataFrames with Some NULL values ({len(has_null_values)}):\")\n",
    "for key, cols in has_null_values:\n",
    "    print(f\"   - {key}: {cols}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
